{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# NB01: Data Extraction — Ecotype Species Screening\n",
    "\n",
    "**Run on**: BERDL JupyterHub (Spark available via `get_spark_session()`)\n",
    "\n",
    "## Goal\n",
    "\n",
    "Extract and summarize three data dimensions for all 338 species that have phylogenetic tree data:\n",
    "\n",
    "1. **Phylogenetic substructure** — per-species branch distance statistics from `phylogenetic_tree_distance_pairs`\n",
    "2. **Environmental diversity** — per-species environmental category counts and entropy from `nmdc_ncbi_biosamples.env_triads_flattened`\n",
    "3. **Pangenome openness** — core/accessory/singleton counts from `pangenome`\n",
    "\n",
    "All heavy aggregations run on Spark. Only small summary tables (~338 rows each) are brought to the driver via `.toPandas()`.\n",
    "\n",
    "## Outputs\n",
    "\n",
    "| File | Description | Rows |\n",
    "|------|-------------|------|\n",
    "| `species_tree_list.csv` | All tree species with phylogenetic_tree_id | ~338 |\n",
    "| `species_phylo_stats.csv` | Branch distance statistics per species | ~338 |\n",
    "| `species_pangenome_stats.csv` | Pangenome openness metrics per species | ~338 |\n",
    "| `species_env_stats.csv` | Environmental diversity metrics per species | ~338 |\n",
    "| `genome_biosample_map.csv` | genome_id → biosample_accession for tree species | ~90K |\n",
    "\n",
    "## Key Pitfalls\n",
    "\n",
    "- **Genome ID format in `phylogenetic_tree_distance_pairs`**: IDs may be bare accessions (no `RS_`/`GB_` prefix). Always verify before joining to `genome` table.\n",
    "- **Cross-database joins**: Use `nmdc_ncbi_biosamples.env_triads_flattened` with full `database.table` notation.\n",
    "- **`--` in species IDs**: Avoid placing these in SQL `IN()` clauses. Use Spark DataFrame joins instead.\n",
    "- **Never `.toPandas()` on large tables**: Aggregate on Spark, collect only summary rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# On JupyterHub: spark is available via get_spark_session() with no import\n",
    "spark = get_spark_session()\n",
    "\n",
    "OUTPUT_PATH = \"../data\"\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Output path: {os.path.abspath(OUTPUT_PATH)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2-md",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 1: Load Tree Species Universe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load all species with phylogenetic trees + pangenome stats\n",
    "\n",
    "tree_species_df = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        pt.gtdb_species_clade_id,\n",
    "        pt.phylogenetic_tree_id,\n",
    "        sc.GTDB_species,\n",
    "        p.no_genomes,\n",
    "        p.no_core,\n",
    "        p.no_aux_genome,\n",
    "        p.no_singleton_gene_clusters,\n",
    "        p.no_gene_clusters,\n",
    "        sc.mean_intra_species_ANI,\n",
    "        sc.min_intra_species_ANI\n",
    "    FROM kbase_ke_pangenome.phylogenetic_tree pt\n",
    "    JOIN kbase_ke_pangenome.gtdb_species_clade sc\n",
    "        ON pt.gtdb_species_clade_id = sc.gtdb_species_clade_id\n",
    "    JOIN kbase_ke_pangenome.pangenome p\n",
    "        ON pt.gtdb_species_clade_id = p.gtdb_species_clade_id\n",
    "\"\"\")\n",
    "\n",
    "tree_species_pd = tree_species_df.toPandas()\n",
    "\n",
    "# Derived pangenome metrics\n",
    "tree_species_pd['singleton_fraction'] = (\n",
    "    tree_species_pd['no_singleton_gene_clusters'] / tree_species_pd['no_gene_clusters']\n",
    ")\n",
    "tree_species_pd['core_fraction'] = (\n",
    "    tree_species_pd['no_core'] / tree_species_pd['no_gene_clusters']\n",
    ")\n",
    "tree_species_pd['aux_fraction'] = (\n",
    "    tree_species_pd['no_aux_genome'] / tree_species_pd['no_gene_clusters']\n",
    ")\n",
    "\n",
    "print(f\"Tree species loaded: {len(tree_species_pd)}\")\n",
    "print(f\"Genome count range: {tree_species_pd['no_genomes'].min()} – {tree_species_pd['no_genomes'].max()}\")\n",
    "print(f\"Genome count median: {tree_species_pd['no_genomes'].median():.0f}\")\n",
    "print(f\"\\nTop 10 by genome count:\")\n",
    "print(tree_species_pd.nlargest(10, 'no_genomes')[['GTDB_species', 'no_genomes', 'singleton_fraction']].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Save species tree list and pangenome stats\n",
    "\n",
    "tree_species_pd.to_csv(f\"{OUTPUT_PATH}/species_tree_list.csv\", index=False)\n",
    "print(f\"Saved species_tree_list.csv ({len(tree_species_pd)} species)\")\n",
    "\n",
    "# Also save pangenome stats separately for downstream notebooks\n",
    "pangenome_cols = [\n",
    "    'gtdb_species_clade_id', 'GTDB_species', 'no_genomes', 'no_core',\n",
    "    'no_aux_genome', 'no_singleton_gene_clusters', 'no_gene_clusters',\n",
    "    'singleton_fraction', 'core_fraction', 'aux_fraction',\n",
    "    'mean_intra_species_ANI', 'min_intra_species_ANI'\n",
    "]\n",
    "tree_species_pd[pangenome_cols].to_csv(f\"{OUTPUT_PATH}/species_pangenome_stats.csv\", index=False)\n",
    "print(f\"Saved species_pangenome_stats.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4-md",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 2: Phylogenetic Substructure Statistics\n",
    "\n",
    "Compute branch distance statistics entirely on Spark.\n",
    "The full table has 22.6M rows — we never `.toPandas()` it directly.\n",
    "We aggregate to ~338 rows before collecting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Check genome ID format in phylogenetic_tree_distance_pairs\n",
    "# IMPORTANT: IDs here may be bare accessions (no RS_/GB_ prefix)\n",
    "\n",
    "sample_pairs = spark.sql(\"\"\"\n",
    "    SELECT phylogenetic_tree_id, genome1_id, genome2_id, branch_distance\n",
    "    FROM kbase_ke_pangenome.phylogenetic_tree_distance_pairs\n",
    "    LIMIT 5\n",
    "\"\"\").toPandas()\n",
    "\n",
    "print(\"Sample rows from phylogenetic_tree_distance_pairs:\")\n",
    "print(sample_pairs.to_string())\n",
    "\n",
    "# Check prefix: do IDs start with RS_ or GB_?\n",
    "sample_genome_id = sample_pairs['genome1_id'].iloc[0]\n",
    "print(f\"\\nSample genome1_id: {sample_genome_id}\")\n",
    "print(f\"Starts with RS_ or GB_: {sample_genome_id.startswith('RS_') or sample_genome_id.startswith('GB_')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Determine correct join key for genome IDs in distance table\n",
    "# If IDs lack prefix, we need to strip RS_/GB_ from genome.genome_id before joining\n",
    "\n",
    "# Check alignment between phylogenetic_tree_distance_pairs genome IDs\n",
    "# and kbase_ke_pangenome.genome genome_ids\n",
    "\n",
    "# Get one species to test\n",
    "test_tree_id = tree_species_pd['phylogenetic_tree_id'].iloc[0]\n",
    "test_species_id = tree_species_pd['gtdb_species_clade_id'].iloc[0]\n",
    "\n",
    "# Get genome IDs from distance table for this species\n",
    "dist_genome_ids = spark.sql(f\"\"\"\n",
    "    SELECT DISTINCT genome1_id\n",
    "    FROM kbase_ke_pangenome.phylogenetic_tree_distance_pairs\n",
    "    WHERE phylogenetic_tree_id = '{test_tree_id}'\n",
    "    LIMIT 5\n",
    "\"\"\").toPandas()\n",
    "\n",
    "# Get genome IDs from genome table for same species\n",
    "genome_table_ids = spark.sql(f\"\"\"\n",
    "    SELECT genome_id\n",
    "    FROM kbase_ke_pangenome.genome\n",
    "    WHERE gtdb_species_clade_id = '{test_species_id}'\n",
    "    LIMIT 5\n",
    "\"\"\").toPandas()\n",
    "\n",
    "print(f\"Test species: {test_species_id[:60]}\")\n",
    "print(f\"\\ngenome1_id values from distance table: {dist_genome_ids['genome1_id'].tolist()}\")\n",
    "print(f\"genome_id values from genome table:    {genome_table_ids['genome_id'].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Compute per-species branch distance statistics on Spark\n",
    "#\n",
    "# Metrics computed per species:\n",
    "#   - n_genomes_in_tree: count of distinct genomes in tree\n",
    "#   - n_pairs: total pairwise comparisons\n",
    "#   - mean, std, min, max, median (p50), IQR (p75-p25) of branch_distance\n",
    "#   - cv (coefficient of variation = std / mean): high CV -> substructure\n",
    "#   - max_median_ratio: another substructure indicator\n",
    "\n",
    "phylo_stats_df = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        phylogenetic_tree_id,\n",
    "        COUNT(DISTINCT genome1_id)                                        AS n_genomes_in_tree,\n",
    "        COUNT(*)                                                           AS n_pairs,\n",
    "        AVG(branch_distance)                                              AS mean_branch_dist,\n",
    "        STDDEV(branch_distance)                                           AS std_branch_dist,\n",
    "        MIN(branch_distance)                                              AS min_branch_dist,\n",
    "        MAX(branch_distance)                                              AS max_branch_dist,\n",
    "        PERCENTILE(branch_distance, 0.25)                                 AS q25_branch_dist,\n",
    "        PERCENTILE(branch_distance, 0.50)                                 AS median_branch_dist,\n",
    "        PERCENTILE(branch_distance, 0.75)                                 AS q75_branch_dist,\n",
    "        PERCENTILE(branch_distance, 0.90)                                 AS q90_branch_dist,\n",
    "        PERCENTILE(branch_distance, 0.75) - PERCENTILE(branch_distance, 0.25) AS iqr_branch_dist,\n",
    "        STDDEV(branch_distance) / NULLIF(AVG(branch_distance), 0)        AS cv_branch_dist,\n",
    "        MAX(branch_distance) / NULLIF(PERCENTILE(branch_distance, 0.50), 0) AS max_median_ratio\n",
    "    FROM kbase_ke_pangenome.phylogenetic_tree_distance_pairs\n",
    "    GROUP BY phylogenetic_tree_id\n",
    "\"\"\")\n",
    "\n",
    "phylo_stats_pd = phylo_stats_df.toPandas()\n",
    "\n",
    "print(f\"Phylo stats computed for {len(phylo_stats_pd)} species\")\n",
    "print(f\"\\nCV summary (coefficient of variation):\")\n",
    "print(phylo_stats_pd['cv_branch_dist'].describe())\n",
    "\n",
    "print(f\"\\nSpecies with highest CV (most substructure):\")\n",
    "print(phylo_stats_pd.nlargest(10, 'cv_branch_dist')[\n",
    "    ['phylogenetic_tree_id', 'n_genomes_in_tree', 'cv_branch_dist', 'max_median_ratio']\n",
    "].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Join phylo stats back to species IDs and save\n",
    "\n",
    "phylo_stats_pd = phylo_stats_pd.merge(\n",
    "    tree_species_pd[['gtdb_species_clade_id', 'GTDB_species', 'phylogenetic_tree_id']],\n",
    "    on='phylogenetic_tree_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# How many tree IDs matched to a species?\n",
    "n_matched = phylo_stats_pd['gtdb_species_clade_id'].notna().sum()\n",
    "print(f\"Phylo stats with matched species ID: {n_matched}/{len(phylo_stats_pd)}\")\n",
    "if n_matched < len(phylo_stats_pd):\n",
    "    print(\"Unmatched tree IDs (investigate):\")\n",
    "    print(phylo_stats_pd[phylo_stats_pd['gtdb_species_clade_id'].isna()]['phylogenetic_tree_id'].tolist())\n",
    "\n",
    "phylo_stats_pd.to_csv(f\"{OUTPUT_PATH}/species_phylo_stats.csv\", index=False)\n",
    "print(f\"\\nSaved species_phylo_stats.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8-md",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 3: Environmental Diversity via NMDC BioSamples\n",
    "\n",
    "Link: `kbase_ke_pangenome.sample` → `nmdc_ncbi_biosamples.env_triads_flattened`\n",
    "\n",
    "We compute, per species:\n",
    "- Number of genomes with any environmental annotation\n",
    "- Number of distinct `env_broad_scale` categories\n",
    "- Shannon entropy of `env_broad_scale` distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Explore nmdc_ncbi_biosamples schema\n",
    "\n",
    "# Check what tables are available\n",
    "spark.sql(\"SHOW TABLES IN nmdc_ncbi_biosamples\").show(20, truncate=False)\n",
    "\n",
    "# Check env_triads_flattened schema\n",
    "print(\"\\nenv_triads_flattened schema:\")\n",
    "spark.sql(\"DESCRIBE nmdc_ncbi_biosamples.env_triads_flattened\").show(30, truncate=False)\n",
    "\n",
    "print(\"\\nSample rows from env_triads_flattened:\")\n",
    "spark.sql(\"SELECT * FROM nmdc_ncbi_biosamples.env_triads_flattened LIMIT 3\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Check the biosamples_flattened table for fallback env fields\n",
    "# (if env_triads_flattened has sparse coverage)\n",
    "\n",
    "print(\"biosamples_flattened schema (first 20 cols):\")\n",
    "spark.sql(\"DESCRIBE nmdc_ncbi_biosamples.biosamples_flattened\").show(20, truncate=False)\n",
    "\n",
    "# Check a sample row for env-related columns\n",
    "print(\"\\nSample row (env-related columns only):\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT accession, isolation_source, env_broad_scale, env_local_scale, env_medium,\n",
    "           geo_loc_name, lat_lon\n",
    "    FROM nmdc_ncbi_biosamples.biosamples_flattened\n",
    "    WHERE env_broad_scale IS NOT NULL\n",
    "    LIMIT 5\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Get genome → biosample accession map for all tree species\n",
    "#\n",
    "# Join genome table to sample table, filtered to tree species only.\n",
    "# Use a Spark join (not isin() on a Python list) to avoid serialization issues\n",
    "# with large numbers of species IDs.\n",
    "\n",
    "# Create a Spark DataFrame of tree species IDs to use as a filter\n",
    "tree_species_spark = spark.createDataFrame(\n",
    "    tree_species_pd[['gtdb_species_clade_id']]\n",
    ")\n",
    "\n",
    "# Get genome → biosample accession for tree species\n",
    "genome_biosample_df = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        g.genome_id,\n",
    "        g.gtdb_species_clade_id,\n",
    "        s.ncbi_biosample_accession_id\n",
    "    FROM kbase_ke_pangenome.genome g\n",
    "    JOIN kbase_ke_pangenome.sample s\n",
    "        ON g.genome_id = s.genome_id\n",
    "\"\"\").join(tree_species_spark, on='gtdb_species_clade_id', how='inner')\n",
    "\n",
    "genome_biosample_df.cache()\n",
    "\n",
    "n_genomes = genome_biosample_df.count()\n",
    "n_with_biosample = genome_biosample_df.filter(\n",
    "    F.col('ncbi_biosample_accession_id').isNotNull()\n",
    ").count()\n",
    "\n",
    "print(f\"Genomes in tree species: {n_genomes:,}\")\n",
    "print(f\"Genomes with biosample accession: {n_with_biosample:,} ({100*n_with_biosample/n_genomes:.1f}%)\")\n",
    "\n",
    "# Save genome-biosample map\n",
    "genome_biosample_pd = genome_biosample_df.toPandas()\n",
    "genome_biosample_pd.to_csv(f\"{OUTPUT_PATH}/genome_biosample_map.csv\", index=False)\n",
    "print(f\"\\nSaved genome_biosample_map.csv ({len(genome_biosample_pd):,} rows)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Join genomes to env_triads_flattened and compute env diversity per species\n",
    "#\n",
    "# Strategy: join genome → biosample → env_triads, then aggregate per species.\n",
    "# env_broad_scale is the primary diversity signal (marine, soil, freshwater, host, etc.)\n",
    "# Compute on Spark; collect only the ~338-row summary.\n",
    "\n",
    "env_diversity_df = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        g.gtdb_species_clade_id,\n",
    "        COUNT(DISTINCT g.genome_id)                                           AS n_genomes_total,\n",
    "        COUNT(DISTINCT CASE WHEN e.env_broad_scale IS NOT NULL\n",
    "                            THEN g.genome_id END)                             AS n_genomes_with_env,\n",
    "        COUNT(DISTINCT e.env_broad_scale)                                     AS n_distinct_env_broad,\n",
    "        COUNT(DISTINCT e.env_local_scale)                                     AS n_distinct_env_local,\n",
    "        COUNT(DISTINCT e.env_medium)                                          AS n_distinct_env_medium\n",
    "    FROM kbase_ke_pangenome.genome g\n",
    "    JOIN kbase_ke_pangenome.sample s\n",
    "        ON g.genome_id = s.genome_id\n",
    "    LEFT JOIN nmdc_ncbi_biosamples.env_triads_flattened e\n",
    "        ON s.ncbi_biosample_accession_id = e.accession\n",
    "    GROUP BY g.gtdb_species_clade_id\n",
    "\"\"\").join(tree_species_spark, on='gtdb_species_clade_id', how='inner')\n",
    "\n",
    "env_diversity_pd = env_diversity_df.toPandas()\n",
    "\n",
    "env_diversity_pd['env_coverage_fraction'] = (\n",
    "    env_diversity_pd['n_genomes_with_env'] / env_diversity_pd['n_genomes_total']\n",
    ")\n",
    "\n",
    "print(f\"Species with env diversity computed: {len(env_diversity_pd)}\")\n",
    "print(f\"\\nEnv coverage fraction summary:\")\n",
    "print(env_diversity_pd['env_coverage_fraction'].describe())\n",
    "print(f\"\\nSpecies with zero env coverage: {(env_diversity_pd['env_coverage_fraction'] == 0).sum()}\")\n",
    "print(f\"\\nn_distinct_env_broad summary:\")\n",
    "print(env_diversity_pd['n_distinct_env_broad'].describe())\n",
    "print(f\"\\nTop 10 by env_broad diversity:\")\n",
    "top_env = env_diversity_pd.merge(\n",
    "    tree_species_pd[['gtdb_species_clade_id', 'GTDB_species']],\n",
    "    on='gtdb_species_clade_id', how='left'\n",
    ").nlargest(10, 'n_distinct_env_broad')\n",
    "print(top_env[['GTDB_species', 'n_genomes_with_env', 'n_distinct_env_broad', 'env_coverage_fraction']].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Compute Shannon entropy of env_broad_scale distribution per species\n",
    "#\n",
    "# Shannon entropy H = -sum(p * log(p)) where p = fraction of genomes in each env category.\n",
    "# Higher entropy = more evenly spread across environments.\n",
    "\n",
    "# Get per-species per-env counts\n",
    "env_counts_df = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        g.gtdb_species_clade_id,\n",
    "        e.env_broad_scale,\n",
    "        COUNT(DISTINCT g.genome_id) AS n_genomes_in_env\n",
    "    FROM kbase_ke_pangenome.genome g\n",
    "    JOIN kbase_ke_pangenome.sample s\n",
    "        ON g.genome_id = s.genome_id\n",
    "    JOIN nmdc_ncbi_biosamples.env_triads_flattened e\n",
    "        ON s.ncbi_biosample_accession_id = e.accession\n",
    "    WHERE e.env_broad_scale IS NOT NULL\n",
    "    GROUP BY g.gtdb_species_clade_id, e.env_broad_scale\n",
    "\"\"\").join(tree_species_spark, on='gtdb_species_clade_id', how='inner')\n",
    "\n",
    "env_counts_pd = env_counts_df.toPandas()\n",
    "\n",
    "# Compute Shannon entropy per species\n",
    "def shannon_entropy(counts):\n",
    "    total = counts.sum()\n",
    "    if total == 0:\n",
    "        return 0.0\n",
    "    probs = counts / total\n",
    "    return float(-np.sum(probs * np.log2(probs + 1e-12)))\n",
    "\n",
    "entropy_per_species = (\n",
    "    env_counts_pd\n",
    "    .groupby('gtdb_species_clade_id')['n_genomes_in_env']\n",
    "    .apply(shannon_entropy)\n",
    "    .reset_index()\n",
    "    .rename(columns={'n_genomes_in_env': 'env_broad_entropy'})\n",
    ")\n",
    "\n",
    "print(f\"Entropy computed for {len(entropy_per_species)} species with env annotations\")\n",
    "print(entropy_per_species['env_broad_entropy'].describe())\n",
    "\n",
    "# Save per-environment breakdown for downstream analysis\n",
    "env_counts_pd.to_csv(f\"{OUTPUT_PATH}/species_env_category_counts.csv\", index=False)\n",
    "print(f\"\\nSaved species_env_category_counts.csv ({len(env_counts_pd):,} rows)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Merge env stats and save\n",
    "\n",
    "species_env_stats = env_diversity_pd.merge(\n",
    "    entropy_per_species, on='gtdb_species_clade_id', how='left'\n",
    ").merge(\n",
    "    tree_species_pd[['gtdb_species_clade_id', 'GTDB_species']],\n",
    "    on='gtdb_species_clade_id', how='left'\n",
    ")\n",
    "\n",
    "# Fill entropy = 0 for species with no env annotations\n",
    "species_env_stats['env_broad_entropy'] = species_env_stats['env_broad_entropy'].fillna(0.0)\n",
    "\n",
    "species_env_stats.to_csv(f\"{OUTPUT_PATH}/species_env_stats.csv\", index=False)\n",
    "print(f\"Saved species_env_stats.csv ({len(species_env_stats)} species)\")\n",
    "\n",
    "print(f\"\\nSpecies with zero env coverage (will score 0 for env diversity): \"\n",
    "      f\"{(species_env_stats['n_genomes_with_env'] == 0).sum()}\")\n",
    "print(f\"Tip: species with 0 env coverage are not disqualified — they score lowest on env dimension only.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14-md",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 4: Sanity Checks and Handoff Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Verify all output files and print summary\n",
    "\n",
    "import os\n",
    "\n",
    "output_files = [\n",
    "    'species_tree_list.csv',\n",
    "    'species_pangenome_stats.csv',\n",
    "    'species_phylo_stats.csv',\n",
    "    'species_env_stats.csv',\n",
    "    'species_env_category_counts.csv',\n",
    "    'genome_biosample_map.csv',\n",
    "]\n",
    "\n",
    "print(\"=== Output File Inventory ===\")\n",
    "for f in output_files:\n",
    "    fpath = os.path.join(OUTPUT_PATH, f)\n",
    "    if os.path.exists(fpath):\n",
    "        df = pd.read_csv(fpath)\n",
    "        print(f\"  {f}: {len(df):,} rows, {df.shape[1]} cols\")\n",
    "    else:\n",
    "        print(f\"  MISSING: {f}\")\n",
    "\n",
    "print()\n",
    "print(\"=== Universe Summary ===\")\n",
    "print(f\"  Total tree species: {len(tree_species_pd)}\")\n",
    "print(f\"  Species with >=20 genomes in tree: {(tree_species_pd['no_genomes'] >= 20).sum()}\")\n",
    "print(f\"  Species with >=50 genomes in tree: {(tree_species_pd['no_genomes'] >= 50).sum()}\")\n",
    "print()\n",
    "print(\"=== Phylo Substructure Summary ===\")\n",
    "phylo_check = pd.read_csv(f\"{OUTPUT_PATH}/species_phylo_stats.csv\")\n",
    "print(f\"  Median CV: {phylo_check['cv_branch_dist'].median():.3f}\")\n",
    "print(f\"  Top quartile CV threshold (75th pct): {phylo_check['cv_branch_dist'].quantile(0.75):.3f}\")\n",
    "print()\n",
    "print(\"=== Environmental Diversity Summary ===\")\n",
    "env_check = pd.read_csv(f\"{OUTPUT_PATH}/species_env_stats.csv\")\n",
    "n_any_env = (env_check['n_genomes_with_env'] > 0).sum()\n",
    "print(f\"  Species with any env annotation: {n_any_env}/{len(env_check)} ({100*n_any_env/len(env_check):.0f}%)\")\n",
    "print(f\"  Median n_distinct_env_broad (among annotated): \"\n",
    "      f\"{env_check[env_check['n_genomes_with_env']>0]['n_distinct_env_broad'].median():.1f}\")\n",
    "print(f\"  Median env entropy (among annotated): \"\n",
    "      f\"{env_check[env_check['env_broad_entropy']>0]['env_broad_entropy'].median():.2f}\")\n",
    "\n",
    "print()\n",
    "print(\"All done. Download data/ directory and proceed with NB02 locally.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
