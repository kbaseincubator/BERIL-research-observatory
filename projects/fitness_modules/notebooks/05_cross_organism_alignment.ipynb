{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB 05: Cross-Organism Module Alignment\n",
    "\n",
    "Group modules from different organisms into conserved \"module families\"\n",
    "using ortholog fingerprints.\n",
    "\n",
    "Steps:\n",
    "1. Extract BBH pairs from `ortholog` table\n",
    "2. Build ortholog groups (OGs) via connected components\n",
    "3. Convert each module → OG fingerprint\n",
    "4. Cluster into module families by cosine similarity\n",
    "5. Consensus annotations for families\n",
    "\n",
    "**Part 1 (JupyterHub)**: Extract ortholog pairs.\n",
    "**Part 2 (local)**: Build OGs and align modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from pathlib import Path\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "DATA_DIR = Path('../data')\n",
    "MODULE_DIR = DATA_DIR / 'modules'\n",
    "ORTHO_DIR = DATA_DIR / 'orthologs'\n",
    "FAMILY_DIR = DATA_DIR / 'module_families'\n",
    "ORTHO_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FAMILY_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pilots = pd.read_csv(DATA_DIR / 'pilot_organisms.csv')\n",
    "pilot_ids = pilots['orgId'].tolist()\n",
    "print(f\"Pilot organisms: {pilot_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Extract BBH Ortholog Pairs (Spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbh_file = ORTHO_DIR / 'pilot_bbh_pairs.csv'\n",
    "\n",
    "try:\n",
    "    spark = get_spark_session()\n",
    "    HAS_SPARK = True\n",
    "    print(f\"Spark version: {spark.version}\")\n",
    "except Exception:\n",
    "    HAS_SPARK = False\n",
    "    print(\"No Spark — using cached data\")\n",
    "\n",
    "if HAS_SPARK and not (bbh_file.exists() and bbh_file.stat().st_size > 0):\n",
    "    # Build SQL filter for pilot organisms\n",
    "    org_list = \", \".join([f\"'{o}'\" for o in pilot_ids])\n",
    "    \n",
    "    bbh = spark.sql(f\"\"\"\n",
    "        SELECT orgId1, locusId1, orgId2, locusId2,\n",
    "               CAST(ratio AS FLOAT) as ratio\n",
    "        FROM kescience_fitnessbrowser.ortholog\n",
    "        WHERE orgId1 IN ({org_list})\n",
    "          AND orgId2 IN ({org_list})\n",
    "    \"\"\").toPandas()\n",
    "    bbh.to_csv(bbh_file, index=False)\n",
    "    print(f\"Extracted {len(bbh)} BBH pairs among pilot organisms\")\n",
    "else:\n",
    "    bbh = pd.read_csv(bbh_file)\n",
    "    print(f\"Loaded {len(bbh)} cached BBH pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Build Ortholog Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_file = ORTHO_DIR / 'ortholog_groups.csv'\n",
    "\n",
    "if og_file.exists() and og_file.stat().st_size > 0:\n",
    "    og_df = pd.read_csv(og_file)\n",
    "    print(f\"CACHED: {og_df['OG_id'].nunique()} ortholog groups\")\n",
    "else:\n",
    "    # Build graph: nodes = (orgId, locusId), edges = BBH pairs\n",
    "    G = nx.Graph()\n",
    "    for _, row in bbh.iterrows():\n",
    "        n1 = f\"{row['orgId1']}:{row['locusId1']}\"\n",
    "        n2 = f\"{row['orgId2']}:{row['locusId2']}\"\n",
    "        G.add_edge(n1, n2, weight=row['ratio'])\n",
    "    \n",
    "    print(f\"Graph: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
    "    \n",
    "    # Connected components = ortholog groups\n",
    "    components = list(nx.connected_components(G))\n",
    "    print(f\"Ortholog groups: {len(components)}\")\n",
    "    \n",
    "    og_records = []\n",
    "    for og_id, comp in enumerate(components):\n",
    "        for node in comp:\n",
    "            org, locus = node.split(':', 1)\n",
    "            og_records.append({'OG_id': f'OG{og_id:05d}', 'orgId': org, 'locusId': locus})\n",
    "    \n",
    "    og_df = pd.DataFrame(og_records)\n",
    "    og_df.to_csv(og_file, index=False)\n",
    "    print(f\"Saved {len(og_df)} gene-OG assignments ({og_df['OG_id'].nunique()} OGs)\")\n",
    "\n",
    "# Summary\n",
    "og_sizes = og_df.groupby('OG_id').size()\n",
    "og_org_count = og_df.groupby('OG_id')['orgId'].nunique()\n",
    "print(f\"\\nOG size: median={og_sizes.median():.0f}, max={og_sizes.max()}\")\n",
    "print(f\"OGs spanning 2+ organisms: {(og_org_count >= 2).sum()}\")\n",
    "print(f\"OGs spanning all {len(pilot_ids)} pilots: {(og_org_count >= len(pilot_ids)).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Module → OG Fingerprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all OG IDs for fingerprint vector\n",
    "all_ogs = sorted(og_df['OG_id'].unique())\n",
    "og_to_idx = {og: i for i, og in enumerate(all_ogs)}\n",
    "n_ogs = len(all_ogs)\n",
    "print(f\"Fingerprint dimensionality: {n_ogs} OGs\")\n",
    "\n",
    "# Build gene→OG lookup per organism\n",
    "gene_to_og = {}\n",
    "for _, row in og_df.iterrows():\n",
    "    key = (row['orgId'], str(row['locusId']))\n",
    "    gene_to_og[key] = row['OG_id']\n",
    "\n",
    "# Build fingerprints for all modules across all organisms\n",
    "module_fingerprints = []\n",
    "module_labels = []\n",
    "\n",
    "for org_id in pilot_ids:\n",
    "    weights_file = MODULE_DIR / f'{org_id}_gene_weights.csv'\n",
    "    member_file = MODULE_DIR / f'{org_id}_gene_membership.csv'\n",
    "    \n",
    "    if not member_file.exists():\n",
    "        print(f\"Skipping {org_id} — no modules\")\n",
    "        continue\n",
    "    \n",
    "    weights = pd.read_csv(weights_file, index_col=0)\n",
    "    membership = pd.read_csv(member_file, index_col=0)\n",
    "    \n",
    "    for mod in membership.columns:\n",
    "        mod_genes = membership.index[membership[mod] == 1].astype(str)\n",
    "        if len(mod_genes) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Build OG fingerprint weighted by |gene_weight|\n",
    "        fingerprint = np.zeros(n_ogs)\n",
    "        for gene in mod_genes:\n",
    "            og = gene_to_og.get((org_id, gene))\n",
    "            if og and og in og_to_idx:\n",
    "                w = abs(weights.loc[weights.index.astype(str) == gene, mod].values[0])\n",
    "                fingerprint[og_to_idx[og]] += w\n",
    "        \n",
    "        if fingerprint.sum() > 0:\n",
    "            module_fingerprints.append(fingerprint)\n",
    "            module_labels.append({'orgId': org_id, 'module': mod})\n",
    "\n",
    "fp_matrix = np.array(module_fingerprints)\n",
    "label_df = pd.DataFrame(module_labels)\n",
    "print(f\"\\nFingerprint matrix: {fp_matrix.shape[0]} modules × {fp_matrix.shape[1]} OGs\")\n",
    "print(f\"Non-zero entries: {(fp_matrix > 0).sum()} / {fp_matrix.size} ({(fp_matrix > 0).mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cluster into Module Families"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarity between modules from DIFFERENT organisms only\n",
    "cos_sim = cosine_similarity(fp_matrix)\n",
    "\n",
    "# Mask within-organism similarities (set to 0)\n",
    "for i in range(len(label_df)):\n",
    "    for j in range(len(label_df)):\n",
    "        if label_df.iloc[i]['orgId'] == label_df.iloc[j]['orgId']:\n",
    "            cos_sim[i, j] = 0 if i != j else 1\n",
    "\n",
    "# Hierarchical clustering on cosine distance\n",
    "cos_dist = 1 - np.abs(cos_sim)\n",
    "np.fill_diagonal(cos_dist, 0)\n",
    "cos_dist = np.clip(cos_dist, 0, 1)  # numerical safety\n",
    "\n",
    "condensed = squareform(cos_dist)\n",
    "Z = linkage(condensed, method='average')\n",
    "\n",
    "# Cut at distance threshold to define families\n",
    "family_labels = fcluster(Z, t=0.7, criterion='distance')\n",
    "label_df['familyId'] = [f'F{f:03d}' for f in family_labels]\n",
    "\n",
    "# Keep only families spanning 2+ organisms\n",
    "family_org_count = label_df.groupby('familyId')['orgId'].nunique()\n",
    "multi_org_families = family_org_count[family_org_count >= 2].index\n",
    "print(f\"Total families: {label_df['familyId'].nunique()}\")\n",
    "print(f\"Families spanning 2+ organisms: {len(multi_org_families)}\")\n",
    "\n",
    "label_df.to_csv(FAMILY_DIR / 'module_families.csv', index=False)\n",
    "print(f\"Saved: module_families.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Consensus Annotations for Families"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each multi-organism family, collect module annotations and find consensus\n",
    "family_annotations = []\n",
    "\n",
    "for fam_id in multi_org_families:\n",
    "    fam_modules = label_df[label_df['familyId'] == fam_id]\n",
    "    \n",
    "    # Collect all significant enrichments from member modules\n",
    "    all_terms = []\n",
    "    for _, row in fam_modules.iterrows():\n",
    "        ann_file = MODULE_DIR / f\"{row['orgId']}_module_annotations.csv\"\n",
    "        if not ann_file.exists():\n",
    "            continue\n",
    "        ann = pd.read_csv(ann_file)\n",
    "        mod_ann = ann[(ann['module'] == row['module']) & (ann['significant'] == True)]\n",
    "        for _, a in mod_ann.iterrows():\n",
    "            all_terms.append({\n",
    "                'term': a['term'],\n",
    "                'database': a['database'],\n",
    "                'orgId': row['orgId'],\n",
    "                'fdr': a['fdr']\n",
    "            })\n",
    "    \n",
    "    if not all_terms:\n",
    "        family_annotations.append({\n",
    "            'familyId': fam_id,\n",
    "            'n_organisms': len(fam_modules['orgId'].unique()),\n",
    "            'n_modules': len(fam_modules),\n",
    "            'consensus_term': 'unannotated',\n",
    "            'consensus_db': '',\n",
    "            'term_organisms': 0,\n",
    "            'mean_fdr': 1.0\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    terms_df = pd.DataFrame(all_terms)\n",
    "    \n",
    "    # Consensus: term appearing in most organisms with best mean FDR\n",
    "    term_stats = terms_df.groupby(['term', 'database']).agg(\n",
    "        term_organisms=('orgId', 'nunique'),\n",
    "        mean_fdr=('fdr', 'mean')\n",
    "    ).reset_index()\n",
    "    term_stats = term_stats.sort_values(['term_organisms', 'mean_fdr'],\n",
    "                                        ascending=[False, True])\n",
    "    best = term_stats.iloc[0]\n",
    "    \n",
    "    family_annotations.append({\n",
    "        'familyId': fam_id,\n",
    "        'n_organisms': len(fam_modules['orgId'].unique()),\n",
    "        'n_modules': len(fam_modules),\n",
    "        'consensus_term': best['term'],\n",
    "        'consensus_db': best['database'],\n",
    "        'term_organisms': int(best['term_organisms']),\n",
    "        'mean_fdr': best['mean_fdr']\n",
    "    })\n",
    "\n",
    "fam_ann_df = pd.DataFrame(family_annotations)\n",
    "fam_ann_df.to_csv(FAMILY_DIR / 'family_annotations.csv', index=False)\n",
    "print(f\"\\nFamily annotation summary:\")\n",
    "print(f\"  Annotated families: {(fam_ann_df['consensus_term'] != 'unannotated').sum()}\")\n",
    "print(f\"  Unannotated families: {(fam_ann_df['consensus_term'] == 'unannotated').sum()}\")\n",
    "fam_ann_df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
