{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB01: Data Extraction -- AlphaEarth Embeddings + Environment Labels\n",
    "\n",
    "**Requires BERDL JupyterHub** (`get_spark_session()` is only available there)\n",
    "\n",
    "This notebook extracts and joins three data layers:\n",
    "1. AlphaEarth 64-dim environmental embeddings (83K genomes)\n",
    "2. NCBI environment metadata (EAV format, pivoted)\n",
    "3. Coverage statistics\n",
    "\n",
    "**Outputs** (saved to `../data/`):\n",
    "- `alphaearth_with_env.csv` -- embeddings + pivoted env labels\n",
    "- `coverage_stats.csv` -- overlap statistics\n",
    "- `ncbi_env_attribute_counts.csv` -- which attributes exist and their population rates\n",
    "- `isolation_source_raw_counts.csv` -- raw value counts for harmonization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Spark session (available on BERDL JupyterHub)\n",
    "spark = get_spark_session()\n",
    "\n",
    "# Output directory\n",
    "DATA_DIR = '../data'\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "print('Spark session initialized')\n",
    "print(f'Output directory: {os.path.abspath(DATA_DIR)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extract AlphaEarth Embeddings\n",
    "\n",
    "The `alphaearth_embeddings_all_years` table has 83,287 rows -- small enough to collect entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract full AlphaEarth table\n",
    "ae_df = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM kbase_ke_pangenome.alphaearth_embeddings_all_years\n",
    "\"\"\").toPandas()\n",
    "\n",
    "print(f'AlphaEarth embeddings: {len(ae_df):,} genomes')\n",
    "print(f'Columns: {list(ae_df.columns)}')\n",
    "print(f'\\nEmbedding columns: A00 through A63 ({len([c for c in ae_df.columns if c.startswith(\"A\") and c[1:].isdigit()])} dimensions)')\n",
    "print(f'\\nLat/lon coverage:')\n",
    "print(f'  cleaned_lat non-null: {ae_df[\"cleaned_lat\"].notna().sum():,}')\n",
    "print(f'  cleaned_lon non-null: {ae_df[\"cleaned_lon\"].notna().sum():,}')\n",
    "print(f'\\nYear range: {ae_df[\"cleaned_year\"].min()} - {ae_df[\"cleaned_year\"].max()}')\n",
    "print(f'\\nTaxonomy coverage:')\n",
    "for col in ['domain', 'phylum', 'class', 'order', 'family', 'genus', 'species']:\n",
    "    if col in ae_df.columns:\n",
    "        print(f'  {col}: {ae_df[col].nunique():,} unique values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick look at the data\n",
    "ae_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inventory NCBI Environment Attributes\n",
    "\n",
    "Before pivoting, check which `harmonized_name` values exist and how many genomes have each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get attribute inventory across ALL genomes (not just AlphaEarth)\n",
    "attr_counts = spark.sql(\"\"\"\n",
    "    SELECT harmonized_name,\n",
    "           COUNT(*) as n_rows,\n",
    "           COUNT(DISTINCT accession) as n_genomes\n",
    "    FROM kbase_ke_pangenome.ncbi_env\n",
    "    GROUP BY harmonized_name\n",
    "    ORDER BY n_genomes DESC\n",
    "\"\"\").toPandas()\n",
    "\n",
    "print(f'NCBI env attributes: {len(attr_counts)} distinct harmonized_name values')\n",
    "print(f'\\nTop 30 attributes by genome count:')\n",
    "print(attr_counts.head(30).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save attribute inventory\n",
    "attr_counts.to_csv(os.path.join(DATA_DIR, 'ncbi_env_attribute_counts.csv'), index=False)\n",
    "print(f'Saved ncbi_env_attribute_counts.csv ({len(attr_counts)} attributes)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pivot NCBI Environment Labels for AlphaEarth Genomes\n",
    "\n",
    "Join `ncbi_env` to AlphaEarth genomes via `ncbi_biosample_accession_id` and pivot key attributes into columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the biosample IDs from AlphaEarth table\n",
    "biosample_ids = ae_df['ncbi_biosample_accession_id'].dropna().unique().tolist()\n",
    "print(f'AlphaEarth genomes with biosample IDs: {len(biosample_ids):,}')\n",
    "\n",
    "# Register as temp view for efficient Spark join\n",
    "biosample_sdf = spark.createDataFrame(\n",
    "    [(b,) for b in biosample_ids],\n",
    "    ['accession']\n",
    ")\n",
    "biosample_sdf.createOrReplaceTempView('ae_biosamples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot key environment attributes for AlphaEarth genomes\n",
    "# Target attributes based on common NCBI BioSample fields\n",
    "TARGET_ATTRS = [\n",
    "    'isolation_source',\n",
    "    'geo_loc_name',\n",
    "    'env_broad_scale',\n",
    "    'env_local_scale',\n",
    "    'env_medium',\n",
    "    'host',\n",
    "    'collection_date',\n",
    "    'lat_lon',\n",
    "    'depth',\n",
    "    'altitude',\n",
    "    'temp',\n",
    "]\n",
    "\n",
    "attr_in_clause = \"', '\".join(TARGET_ATTRS)\n",
    "\n",
    "env_pivot = spark.sql(f\"\"\"\n",
    "    SELECT ne.accession,\n",
    "           MAX(CASE WHEN ne.harmonized_name = 'isolation_source' THEN ne.content END) as isolation_source,\n",
    "           MAX(CASE WHEN ne.harmonized_name = 'geo_loc_name' THEN ne.content END) as geo_loc_name,\n",
    "           MAX(CASE WHEN ne.harmonized_name = 'env_broad_scale' THEN ne.content END) as env_broad_scale,\n",
    "           MAX(CASE WHEN ne.harmonized_name = 'env_local_scale' THEN ne.content END) as env_local_scale,\n",
    "           MAX(CASE WHEN ne.harmonized_name = 'env_medium' THEN ne.content END) as env_medium,\n",
    "           MAX(CASE WHEN ne.harmonized_name = 'host' THEN ne.content END) as host,\n",
    "           MAX(CASE WHEN ne.harmonized_name = 'collection_date' THEN ne.content END) as collection_date,\n",
    "           MAX(CASE WHEN ne.harmonized_name = 'lat_lon' THEN ne.content END) as lat_lon,\n",
    "           MAX(CASE WHEN ne.harmonized_name = 'depth' THEN ne.content END) as depth,\n",
    "           MAX(CASE WHEN ne.harmonized_name = 'altitude' THEN ne.content END) as altitude,\n",
    "           MAX(CASE WHEN ne.harmonized_name = 'temp' THEN ne.content END) as temp\n",
    "    FROM kbase_ke_pangenome.ncbi_env ne\n",
    "    JOIN ae_biosamples ab ON ne.accession = ab.accession\n",
    "    WHERE ne.harmonized_name IN ('{attr_in_clause}')\n",
    "    GROUP BY ne.accession\n",
    "\"\"\").toPandas()\n",
    "\n",
    "print(f'Environment labels pivoted: {len(env_pivot):,} genomes')\n",
    "print(f'\\nAttribute population rates (of {len(env_pivot):,} genomes with any env data):')\n",
    "for col in env_pivot.columns[1:]:\n",
    "    n = env_pivot[col].notna().sum()\n",
    "    pct = 100 * n / len(env_pivot) if len(env_pivot) > 0 else 0\n",
    "    print(f'  {col}: {n:,} ({pct:.1f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Join Embeddings with Environment Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge AlphaEarth embeddings with pivoted env labels\n",
    "merged = ae_df.merge(\n",
    "    env_pivot,\n",
    "    left_on='ncbi_biosample_accession_id',\n",
    "    right_on='accession',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f'Merged dataset: {len(merged):,} genomes')\n",
    "print(f'  With env labels: {merged[\"isolation_source\"].notna().sum():,}')\n",
    "print(f'  Without env labels: {merged[\"isolation_source\"].isna().sum():,}')\n",
    "\n",
    "# Drop duplicate accession column from merge\n",
    "if 'accession' in merged.columns:\n",
    "    merged = merged.drop(columns=['accession'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Coverage Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute coverage flags\n",
    "has_latlon = merged['cleaned_lat'].notna() & merged['cleaned_lon'].notna()\n",
    "has_isolation = merged['isolation_source'].notna()\n",
    "has_env_broad = merged['env_broad_scale'].notna()\n",
    "has_env_local = merged['env_local_scale'].notna()\n",
    "has_env_medium = merged['env_medium'].notna()\n",
    "has_host = merged['host'].notna()\n",
    "has_geo_loc = merged['geo_loc_name'].notna()\n",
    "\n",
    "coverage = pd.DataFrame({\n",
    "    'attribute': ['lat/lon', 'isolation_source', 'env_broad_scale', 'env_local_scale',\n",
    "                  'env_medium', 'host', 'geo_loc_name'],\n",
    "    'n_genomes': [has_latlon.sum(), has_isolation.sum(), has_env_broad.sum(),\n",
    "                  has_env_local.sum(), has_env_medium.sum(), has_host.sum(), has_geo_loc.sum()],\n",
    "    'pct_of_alphaearth': [100 * has_latlon.mean(), 100 * has_isolation.mean(),\n",
    "                          100 * has_env_broad.mean(), 100 * has_env_local.mean(),\n",
    "                          100 * has_env_medium.mean(), 100 * has_host.mean(),\n",
    "                          100 * has_geo_loc.mean()]\n",
    "})\n",
    "\n",
    "print('Coverage of AlphaEarth genomes (83K total):')\n",
    "print(coverage.to_string(index=False))\n",
    "\n",
    "# Intersection counts for UpSet plot\n",
    "# Store per-genome boolean flags\n",
    "merged['has_latlon'] = has_latlon\n",
    "merged['has_isolation_source'] = has_isolation\n",
    "merged['has_env_broad_scale'] = has_env_broad\n",
    "merged['has_env_local_scale'] = has_env_local\n",
    "merged['has_env_medium'] = has_env_medium\n",
    "merged['has_host'] = has_host\n",
    "merged['has_geo_loc_name'] = has_geo_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage.to_csv(os.path.join(DATA_DIR, 'coverage_stats.csv'), index=False)\n",
    "print('Saved coverage_stats.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Raw Isolation Source Value Counts\n",
    "\n",
    "Save the raw `isolation_source` values for harmonization in NB02."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso_counts = (\n",
    "    merged[merged['isolation_source'].notna()]['isolation_source']\n",
    "    .value_counts()\n",
    "    .reset_index()\n",
    ")\n",
    "iso_counts.columns = ['isolation_source', 'count']\n",
    "\n",
    "print(f'Unique isolation_source values: {len(iso_counts):,}')\n",
    "print(f'\\nTop 30:')\n",
    "print(iso_counts.head(30).to_string(index=False))\n",
    "\n",
    "iso_counts.to_csv(os.path.join(DATA_DIR, 'isolation_source_raw_counts.csv'), index=False)\n",
    "print(f'\\nSaved isolation_source_raw_counts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Merged Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the full merged dataset\n",
    "out_path = os.path.join(DATA_DIR, 'alphaearth_with_env.csv')\n",
    "merged.to_csv(out_path, index=False)\n",
    "\n",
    "print(f'Saved alphaearth_with_env.csv')\n",
    "print(f'  Rows: {len(merged):,}')\n",
    "print(f'  Columns: {len(merged.columns)}')\n",
    "print(f'  File size: {os.path.getsize(out_path) / 1e6:.1f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Quick Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding dimension stats\n",
    "emb_cols = [f'A{i:02d}' for i in range(64)]\n",
    "emb_stats = merged[emb_cols].describe().T\n",
    "print('Embedding dimension summary (A00-A63):')\n",
    "print(f'  Value range: [{emb_stats[\"min\"].min():.3f}, {emb_stats[\"max\"].max():.3f}]')\n",
    "print(f'  Mean of means: {emb_stats[\"mean\"].mean():.3f}')\n",
    "print(f'  Mean of stds: {emb_stats[\"std\"].mean():.3f}')\n",
    "print(f'  Any NaN in embeddings: {merged[emb_cols].isna().any().any()}')\n",
    "\n",
    "# Lat/lon range\n",
    "print(f'\\nLat range: [{merged[\"cleaned_lat\"].min():.2f}, {merged[\"cleaned_lat\"].max():.2f}]')\n",
    "print(f'Lon range: [{merged[\"cleaned_lon\"].min():.2f}, {merged[\"cleaned_lon\"].max():.2f}]')\n",
    "\n",
    "# Phylum distribution\n",
    "print(f'\\nTop 10 phyla:')\n",
    "print(merged['phylum'].value_counts().head(10).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n=== Data extraction complete ===')\n",
    "print(f'Output files in {os.path.abspath(DATA_DIR)}:')\n",
    "for f in sorted(os.listdir(DATA_DIR)):\n",
    "    if f.endswith('.csv'):\n",
    "        size = os.path.getsize(os.path.join(DATA_DIR, f)) / 1e6\n",
    "        print(f'  {f} ({size:.1f} MB)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
