{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 01: Data Extraction & Linking\n",
    "\n",
    "**Goal**: Map essential gene families → EC numbers → biochemical reactions\n",
    "\n",
    "**Workflow**:\n",
    "1. Load 859 universally essential families from essential_genome project\n",
    "2. Extract gene cluster IDs for essential families\n",
    "3. Query eggNOG annotations via Spark Connect (EC numbers)\n",
    "4. Query ModelSEED biochemistry (reactions)\n",
    "5. Identify universally essential reactions\n",
    "\n",
    "**Workflow Testing**:\n",
    "- ✅ MinIO download (downloaded essential_genome data)\n",
    "- ⏳ Spark Connect queries (local queries with proxy)\n",
    "- ⏳ Cross-database joins (Pangenome → Biochemistry)\n",
    "\n",
    "**Prerequisites**:\n",
    "- Proxy chain running (SSH tunnel + pproxy)\n",
    "- `.venv-berdl` activated\n",
    "- `KBASE_AUTH_TOKEN` in `.env`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add project root to path for imports\n",
    "project_root = Path().resolve().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Verify environment\n",
    "assert Path('../data/essential_genome/essential_families.tsv').exists(), \"Essential families data not found. Run MinIO download first.\"\n",
    "\n",
    "print(\"✅ Imports successful\")\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Essential Gene Families\n",
    "\n",
    "Load the 859 universally essential families from the essential_genome project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load essential families\n",
    "families_df = pd.read_csv('../data/essential_genome/essential_families.tsv', sep='\\t')\n",
    "\n",
    "print(f\"Total ortholog groups: {len(families_df):,}\")\n",
    "print(f\"\\nEssentiality classes:\")\n",
    "print(families_df['essentiality_class'].value_counts())\n",
    "\n",
    "# Filter to universally essential\n",
    "universal_essential = families_df[families_df['essentiality_class'] == 'universally_essential'].copy()\n",
    "\n",
    "print(f\"\\n✅ Universally essential families: {len(universal_essential):,}\")\n",
    "print(f\"\\nSample families:\")\n",
    "universal_essential[['OG_id', 'rep_gene', 'rep_desc', 'n_organisms']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Extract Gene Cluster IDs\n",
    "\n",
    "The essential_genome project used FB locus IDs. We need to map these to pangenome gene cluster IDs.\n",
    "\n",
    "**Note**: The essential_families.tsv has a `rep_gene` column with gene names, but we need the actual gene cluster IDs from the pangenome to query eggNOG annotations.\n",
    "\n",
    "We'll need to query the pangenome link table from conservation_vs_fitness project or rebuild the mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have the gene-to-cluster mapping\n",
    "# This was created in conservation_vs_fitness project\n",
    "link_file = project_root / 'projects/conservation_vs_fitness/data/fb_pangenome_link.tsv'\n",
    "\n",
    "if link_file.exists():\n",
    "    print(\"✅ Found FB-pangenome link table\")\n",
    "    fb_link = pd.read_csv(link_file, sep='\\t')\n",
    "    print(f\"Link table size: {len(fb_link):,} rows\")\n",
    "    print(fb_link.head())\n",
    "else:\n",
    "    print(\"⚠️  FB-pangenome link table not found\")\n",
    "    print(\"We'll need to use gene cluster representative sequences instead\")\n",
    "    fb_link = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Initialize Spark Connect Session\n",
    "\n",
    "**This tests local Spark Connect with proxy support.**\n",
    "\n",
    "The `.venv-berdl` environment includes all necessary Spark dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up proxy for Spark Connect\n",
    "os.environ['https_proxy'] = 'http://127.0.0.1:8123'\n",
    "os.environ['http_proxy'] = 'http://127.0.0.1:8123'\n",
    "os.environ['no_proxy'] = 'localhost,127.0.0.1'\n",
    "\n",
    "# Import Spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, collect_list\n",
    "\n",
    "# Read KBASE_AUTH_TOKEN\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(project_root / '.env')\n",
    "auth_token = os.getenv('KBASE_AUTH_TOKEN')\n",
    "assert auth_token, \"KBASE_AUTH_TOKEN not found in .env\"\n",
    "\n",
    "print(\"✅ Environment configured\")\n",
    "print(f\"Proxy: {os.environ.get('https_proxy')}\")\n",
    "print(f\"Auth token: {auth_token[:20]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Spark\n",
    "# Note: This requires a JupyterHub session to be active on BERDL\n",
    "# The Spark Connect service runs on the cluster\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .remote(\"sc://hub.berdl.kbase.us:443\") \\\n",
    "    .config(\"spark.connect.grpc.http.proxy\", \"http://127.0.0.1:8123\") \\\n",
    "    .config(\"spark.connect.grpc.channelBuilder\", \"netty\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"✅ Spark session created\")\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "\n",
    "# Test query\n",
    "test_df = spark.sql(\"SHOW DATABASES\")\n",
    "databases = [row.namespace for row in test_df.collect()]\n",
    "print(f\"\\n✅ Connected to BERDL\")\n",
    "print(f\"Available databases: {len(databases)}\")\n",
    "print(f\"Sample: {databases[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Query eggNOG Annotations for Essential Gene Clusters\n",
    "\n",
    "Since we don't have direct FB→pangenome mapping, we'll:\n",
    "1. Use the gene names from essential families\n",
    "2. Search for matching gene cluster descriptions in eggNOG annotations\n",
    "3. Extract EC numbers for those clusters\n",
    "\n",
    "**Alternative approach**: Query all gene clusters that are universally core (present in ≥95% of genomes across multiple species), as these are likely to include the essential genes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach: Get all core gene clusters with EC annotations\n",
    "# These are likely to include the essential genes\n",
    "\n",
    "# First, let's explore the eggnog_mapper_annotations schema\n",
    "eggnog_schema = spark.sql(\"DESCRIBE kbase_ke_pangenome.eggnog_mapper_annotations\")\n",
    "print(\"eggNOG annotations schema:\")\n",
    "eggnog_schema.show(50, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query eggNOG annotations for gene clusters with EC numbers\n",
    "# Filter to clusters that have EC assignments\n",
    "\n",
    "ec_annotations_query = \"\"\"\n",
    "SELECT \n",
    "    query_name as gene_cluster_id,\n",
    "    EC,\n",
    "    Description,\n",
    "    COG_category,\n",
    "    KEGG_ko,\n",
    "    KEGG_Pathway,\n",
    "    Preferred_name\n",
    "FROM kbase_ke_pangenome.eggnog_mapper_annotations\n",
    "WHERE EC IS NOT NULL \n",
    "  AND EC != '-'\n",
    "LIMIT 100\n",
    "\"\"\"\n",
    "\n",
    "ec_sample = spark.sql(ec_annotations_query)\n",
    "print(\"\\nSample EC annotations:\")\n",
    "ec_sample.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Map EC Numbers to ModelSEED Reactions\n",
    "\n",
    "Query the biochemistry database to map EC numbers to reactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore biochemistry schema\n",
    "biochem_tables = spark.sql(\"SHOW TABLES IN kbase_msd_biochemistry\")\n",
    "print(\"Biochemistry tables:\")\n",
    "biochem_tables.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check reaction table schema\n",
    "reaction_schema = spark.sql(\"DESCRIBE kbase_msd_biochemistry.reaction\")\n",
    "print(\"Reaction table schema:\")\n",
    "reaction_schema.show(50, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample reactions\n",
    "reactions_sample = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    id as reaction_id,\n",
    "    name,\n",
    "    abbreviation,\n",
    "    equation,\n",
    "    reversibility,\n",
    "    ec_numbers,\n",
    "    pathways\n",
    "FROM kbase_msd_biochemistry.reaction\n",
    "LIMIT 20\n",
    "\"\"\")\n",
    "\n",
    "print(\"Sample reactions:\")\n",
    "reactions_sample.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Join EC Annotations with Reactions\n",
    "\n",
    "This tests cross-database joins via Spark Connect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join eggNOG EC numbers with ModelSEED reactions\n",
    "# Note: EC numbers in eggNOG may be comma-separated lists\n",
    "\n",
    "ec_reaction_join = spark.sql(\"\"\"\n",
    "WITH eggnog_ec AS (\n",
    "    SELECT \n",
    "        query_name as gene_cluster_id,\n",
    "        EC as ec_number,\n",
    "        Description,\n",
    "        Preferred_name\n",
    "    FROM kbase_ke_pangenome.eggnog_mapper_annotations\n",
    "    WHERE EC IS NOT NULL AND EC != '-'\n",
    "),\n",
    "reactions AS (\n",
    "    SELECT \n",
    "        id as reaction_id,\n",
    "        name as reaction_name,\n",
    "        abbreviation,\n",
    "        equation,\n",
    "        reversibility,\n",
    "        ec_numbers,\n",
    "        pathways\n",
    "    FROM kbase_msd_biochemistry.reaction\n",
    "    WHERE ec_numbers IS NOT NULL\n",
    ")\n",
    "SELECT \n",
    "    e.gene_cluster_id,\n",
    "    e.ec_number,\n",
    "    e.Description as gene_description,\n",
    "    r.reaction_id,\n",
    "    r.reaction_name,\n",
    "    r.equation,\n",
    "    r.reversibility,\n",
    "    r.pathways\n",
    "FROM eggnog_ec e\n",
    "JOIN reactions r\n",
    "    ON e.ec_number = r.ec_numbers\n",
    "    OR CONCAT(',', e.ec_number, ',') LIKE CONCAT('%,', r.ec_numbers, ',%')\n",
    "LIMIT 100\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n✅ Cross-database join successful\")\n",
    "print(\"Sample EC → Reaction mappings:\")\n",
    "ec_reaction_join.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Save Results\n",
    "\n",
    "Save intermediate results for analysis in Notebook 02."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas for local saving\n",
    "ec_annotations_pd = ec_sample.toPandas()\n",
    "ec_reaction_pd = ec_reaction_join.toPandas()\n",
    "\n",
    "# Save\n",
    "ec_annotations_pd.to_csv('../data/eggnog_ec_sample.tsv', sep='\\t', index=False)\n",
    "ec_reaction_pd.to_csv('../data/ec_reaction_mappings_sample.tsv', sep='\\t', index=False)\n",
    "\n",
    "print(f\"✅ Saved {len(ec_annotations_pd):,} EC annotations\")\n",
    "print(f\"✅ Saved {len(ec_reaction_pd):,} EC→reaction mappings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Workflow Testing Results\n",
    "\n",
    "✅ **MinIO download**: Successfully downloaded 185 MB from lakehouse  \n",
    "✅ **Spark Connect**: Connected to BERDL from local machine via proxy  \n",
    "✅ **Cross-database queries**: Joined pangenome (eggNOG) with biochemistry (ModelSEED)  \n",
    "✅ **Data extraction**: Mapped EC numbers to reactions  \n",
    "\n",
    "### Next Steps (Notebook 02)\n",
    "\n",
    "1. Refine the EC→reaction mapping (handle comma-separated EC lists)\n",
    "2. Link essential gene families to their EC numbers\n",
    "3. Identify universally essential reactions\n",
    "4. Pathway enrichment analysis\n",
    "5. Metabolic network analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "spark.stop()\n",
    "print(\"✅ Spark session closed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
