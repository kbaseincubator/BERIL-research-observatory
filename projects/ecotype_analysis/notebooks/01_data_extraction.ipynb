{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Scale Up Ecotype Analysis to More Species\n\n## Goal\nExpand the environment-ecotype analysis to many more species by:\n1. Finding species with **good environmental embedding coverage** (\u226520 embeddings, \u226530% coverage)\n2. **Downsampling large species** to maximize phylogenetic diversity\n3. Extracting data for the expanded species set\n\n## Selection Criteria\n\n| Parameter | Value | Rationale |\n|-----------|-------|----------|\n| **Minimum genomes with embeddings** | 20 | Basic statistical power |\n| **Embedding coverage** | \u226530% | Representative sample |\n| **Maximum genomes per species** | 250 | Tractable pairwise computations |\n| **Downsampling method** | Maximize diversity using ANI distances | Preserve full genetic spread |"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 1: Find Species with Good Embedding Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 1: Compute Embedding Coverage per Species\n\nimport numpy as np\nimport pandas as pd\nimport os\nfrom pyspark.sql.functions import col, count, countDistinct\n\n# Initialize Spark\nspark = get_spark_session()\n\nOUTPUT_PATH = \"../data\"\nos.makedirs(OUTPUT_PATH, exist_ok=True)\n\n# Join genome table with embeddings to compute coverage per species\ncoverage_df = spark.sql(\"\"\"\n    SELECT\n        g.gtdb_species_clade_id,\n        COUNT(DISTINCT g.genome_id) as n_total,\n        COUNT(DISTINCT e.genome_id) as n_with_embeddings\n    FROM kbase_ke_pangenome.genome g\n    LEFT JOIN kbase_ke_pangenome.alphaearth_embeddings_all_years e\n        ON g.genome_id = e.genome_id\n    GROUP BY g.gtdb_species_clade_id\n\"\"\")\n\ncoverage_df.cache()\ncoverage_pd = coverage_df.toPandas()\ncoverage_pd['coverage'] = coverage_pd['n_with_embeddings'] / coverage_pd['n_total']\n\n# Save coverage data\ncoverage_pd.to_csv(f\"{OUTPUT_PATH}/species_embedding_coverage.csv\", index=False)\nprint(f\"Saved coverage data for {len(coverage_pd)} species\")\n\n# Show species with best coverage (>=20 embeddings, >=30% coverage)\ngood_coverage = coverage_pd[\n    (coverage_pd['n_with_embeddings'] >= 20) &\n    (coverage_pd['coverage'] >= 0.30)\n].sort_values('n_with_embeddings', ascending=False)\n\nprint(f\"\\nSpecies with >=20 embeddings AND >=30% coverage: {len(good_coverage)}\")\nprint(good_coverage.head(30))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 2: Select Target Species for Expanded Analysis\n\n# Use ALL species meeting criteria:\n# - \u226520 genomes with embeddings (for statistical power)\n# - \u226530% coverage (representative sample)\n\nTARGET_SPECIES = good_coverage['gtdb_species_clade_id'].tolist()\nprint(f\"Selected {len(TARGET_SPECIES)} species for expanded analysis\")\n\n# Save target species list\nwith open(f\"{OUTPUT_PATH}/target_species_expanded.txt\", 'w') as f:\n    for sp in TARGET_SPECIES:\n        f.write(sp + '\\n')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## STEP 2: Downsample Large Species (Maximize Phylogenetic Diversity)\n\nFor species with >250 genomes with embeddings, select representatives that **maximize total phylogenetic diversity** using ANI-based distances (distance = 100 - ANI).\n\n*Note: Using ANI as a proxy for phylogenetic distance until per-species tree data is available.*"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 3: Identify Species Needing Downsampling\n\nMAX_GENOMES = 250  # Maximum genomes per species after downsampling\n\n# All target species from good_coverage (already filtered to \u226520 embeddings, \u226530% coverage)\nprint(f\"Total target species: {len(TARGET_SPECIES)}\")\n\n# Show which need downsampling (more than MAX_GENOMES genomes with embeddings)\nneeds_downsampling = good_coverage[good_coverage['n_with_embeddings'] > MAX_GENOMES]\nprint(f\"\\nSpecies needing downsampling (>{MAX_GENOMES} with embeddings): {len(needs_downsampling)}\")\nif len(needs_downsampling) > 0:\n    print(needs_downsampling[['gtdb_species_clade_id', 'n_total', 'n_with_embeddings']].head(20))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 4: Diversity-Maximizing Downsampling Function\n\nimport numpy as np\n\ndef downsample_maximize_diversity(species_id, max_genomes=250):\n    \"\"\"\n    Downsample a species by selecting genomes that MAXIMIZE phylogenetic diversity.\n    \n    Only considers genomes WITH embeddings (required for the analysis).\n    Uses ANI-based distances (100 - ANI) as proxy for phylogenetic distance.\n\n    Algorithm:\n    1. Get genomes with embeddings for this species\n    2. Build distance matrix from genome_ani table (distance = 100 - ANI)\n    3. Use maximin selection to maximize phylogenetic spread\n    \"\"\"\n    # Get genomes WITH embeddings for this species\n    embed_genomes = spark.sql(f\"\"\"\n        SELECT g.genome_id\n        FROM kbase_ke_pangenome.genome g\n        JOIN kbase_ke_pangenome.alphaearth_embeddings_all_years e\n            ON g.genome_id = e.genome_id\n        WHERE g.gtdb_species_clade_id = '{species_id}'\n    \"\"\").collect()\n    genome_ids = [r.genome_id for r in embed_genomes]\n    \n    n_genomes = len(genome_ids)\n    short_name = species_id.split('__')[1].split('--')[0]\n    print(f\"{short_name}: {n_genomes} genomes with embeddings\")\n\n    # If small enough, return all genomes\n    if n_genomes <= max_genomes:\n        return genome_ids\n\n    # Need to downsample - get ANI matrix\n    print(f\"  Building ANI distance matrix for {n_genomes} genomes...\")\n    ani_df = spark.sql(f\"\"\"\n        SELECT genome1_id, genome2_id, ANI\n        FROM kbase_ke_pangenome.genome_ani\n        WHERE genome1_id IN ({','.join([f\"'{g}'\" for g in genome_ids])})\n          AND genome2_id IN ({','.join([f\"'{g}'\" for g in genome_ids])})\n    \"\"\").toPandas()\n\n    # Build distance matrix (distance = 100 - ANI)\n    genome_to_idx = {g: i for i, g in enumerate(genome_ids)}\n    dist_matrix = np.zeros((n_genomes, n_genomes))\n\n    for _, row in ani_df.iterrows():\n        i = genome_to_idx.get(row['genome1_id'])\n        j = genome_to_idx.get(row['genome2_id'])\n        if i is not None and j is not None:\n            dist = 100 - row['ANI']\n            dist_matrix[i, j] = dist\n            dist_matrix[j, i] = dist\n\n    # Greedy maximin selection\n    print(f\"  Selecting {max_genomes} representatives to maximize diversity...\")\n    selected_idx = []\n    remaining_idx = set(range(n_genomes))\n\n    # Start with genome that has max sum of distances (most divergent)\n    sum_dists = dist_matrix.sum(axis=1)\n    first = int(np.argmax(sum_dists))\n    selected_idx.append(first)\n    remaining_idx.remove(first)\n\n    # Iteratively add genome with max minimum distance to selected set\n    while len(selected_idx) < max_genomes and remaining_idx:\n        best_idx = None\n        best_min_dist = -1\n\n        for idx in remaining_idx:\n            min_dist = min(dist_matrix[idx, s] for s in selected_idx)\n            if min_dist > best_min_dist:\n                best_min_dist = min_dist\n                best_idx = idx\n\n        if best_idx is None:\n            break\n\n        selected_idx.append(best_idx)\n        remaining_idx.remove(best_idx)\n\n    # Summary\n    total_diversity = sum(dist_matrix[i, j] for i in selected_idx for j in selected_idx if i < j)\n    print(f\"  Selected {len(selected_idx)} genomes\")\n    print(f\"  Total pairwise ANI-distance: {total_diversity:.2f}\")\n\n    return [genome_ids[i] for i in selected_idx]\n\n# Test on one species (uncomment to test)\n# if len(needs_downsampling) > 0:\n#     test_species = needs_downsampling.iloc[0]['gtdb_species_clade_id']\n#     genomes = downsample_maximize_diversity(test_species)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 5: Build Final Genome List (All Target Species)\n\n# For each target species, get genomes (downsampled if needed)\n# All selected genomes will have embeddings\nall_target_genomes = []\nspecies_stats = []\n\nfor species_id in TARGET_SPECIES:\n    species_info = good_coverage[good_coverage['gtdb_species_clade_id'] == species_id].iloc[0]\n    n_with_embed = species_info['n_with_embeddings']\n    n_total = species_info['n_total']\n\n    if n_with_embed > MAX_GENOMES:\n        # Downsample large species using diversity-maximizing selection\n        genomes = downsample_maximize_diversity(species_id, max_genomes=MAX_GENOMES)\n    else:\n        # Use all genomes with embeddings\n        genomes_result = spark.sql(f\"\"\"\n            SELECT g.genome_id\n            FROM kbase_ke_pangenome.genome g\n            JOIN kbase_ke_pangenome.alphaearth_embeddings_all_years e\n                ON g.genome_id = e.genome_id\n            WHERE g.gtdb_species_clade_id = '{species_id}'\n        \"\"\").collect()\n        genomes = [r.genome_id for r in genomes_result]\n\n    for g in genomes:\n        all_target_genomes.append({\n            'genome_id': g, \n            'gtdb_species_clade_id': species_id\n        })\n\n    species_stats.append({\n        'species': species_id,\n        'n_total': n_total,\n        'n_with_embeddings': n_with_embed,\n        'n_selected': len(genomes)\n    })\n\ntarget_genomes_df = pd.DataFrame(all_target_genomes)\ntarget_genomes_df.to_csv(f\"{OUTPUT_PATH}/target_genomes_expanded.csv\", index=False)\n\nstats_df = pd.DataFrame(species_stats)\nstats_df.to_csv(f\"{OUTPUT_PATH}/species_selection_stats.csv\", index=False)\n\nprint(f\"\\n=== Summary ===\")\nprint(f\"Total species: {len(TARGET_SPECIES)}\")\nprint(f\"Total genomes selected: {len(target_genomes_df)}\")\nprint(f\"Genomes per species: {stats_df['n_selected'].mean():.0f} mean, {stats_df['n_selected'].min()}-{stats_df['n_selected'].max()} range\")"
  },
  {
   "cell_type": "code",
   "source": "# Cell 5b: Compute Embedding Diversity per Species\n\nfrom scipy.spatial.distance import pdist, squareform\nimport matplotlib.pyplot as plt\n\n# Embedding columns (A00-A63, 64 dimensions)\nEMBEDDING_COLS = [f\"A{i:02d}\" for i in range(64)]\n\n# Get embeddings for all target genomes\ntarget_genome_ids = target_genomes_df['genome_id'].tolist()\n\n# Query embeddings with all dimension columns\nembeddings_query = f\"\"\"\n    SELECT genome_id, {', '.join(EMBEDDING_COLS)}\n    FROM kbase_ke_pangenome.alphaearth_embeddings_all_years\n\"\"\"\nembeddings_df = spark.sql(embeddings_query).filter(\n    col(\"genome_id\").isin(target_genome_ids)\n).toPandas()\n\nprint(f\"Retrieved {len(embeddings_df)} embeddings\")\n\n# Convert embedding columns to numpy array\nembeddings_df['embedding_vec'] = embeddings_df[EMBEDDING_COLS].values.tolist()\nembeddings_df['embedding_vec'] = embeddings_df['embedding_vec'].apply(np.array)\n\n# Merge with species info\nembeddings_df = embeddings_df.merge(\n    target_genomes_df[['genome_id', 'gtdb_species_clade_id']], \n    on='genome_id'\n)\n\n# Compute embedding diversity per species\ndiversity_stats = []\n\nfor species_id in TARGET_SPECIES:\n    species_embeddings = embeddings_df[\n        embeddings_df['gtdb_species_clade_id'] == species_id\n    ]['embedding_vec'].values\n    \n    if len(species_embeddings) < 2:\n        continue\n    \n    # Stack into matrix\n    emb_matrix = np.vstack(species_embeddings)\n    \n    # Compute pairwise cosine distances\n    # cosine distance = 1 - cosine_similarity\n    cosine_dists = pdist(emb_matrix, metric='cosine')\n    \n    short_name = species_id.split('__')[1].split('--')[0]\n    diversity_stats.append({\n        'species': species_id,\n        'short_name': short_name,\n        'n_genomes': len(species_embeddings),\n        'mean_cosine_dist': np.mean(cosine_dists),\n        'std_cosine_dist': np.std(cosine_dists),\n        'min_cosine_dist': np.min(cosine_dists),\n        'max_cosine_dist': np.max(cosine_dists),\n        'median_cosine_dist': np.median(cosine_dists)\n    })\n\ndiversity_df = pd.DataFrame(diversity_stats)\ndiversity_df.to_csv(f\"{OUTPUT_PATH}/species_embedding_diversity.csv\", index=False)\n\nprint(f\"Computed embedding diversity for {len(diversity_df)} species\")\nprint(f\"\\nSummary statistics:\")\nprint(diversity_df[['mean_cosine_dist', 'std_cosine_dist']].describe())\n\n# Plot distribution\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\naxes[0].hist(diversity_df['mean_cosine_dist'], bins=30, edgecolor='black', alpha=0.7)\naxes[0].set_xlabel('Mean Pairwise Cosine Distance')\naxes[0].set_ylabel('Number of Species')\naxes[0].set_title('Distribution of Embedding Diversity Across Species')\n\naxes[1].scatter(diversity_df['n_genomes'], diversity_df['mean_cosine_dist'], alpha=0.5)\naxes[1].set_xlabel('Number of Genomes')\naxes[1].set_ylabel('Mean Pairwise Cosine Distance')\naxes[1].set_title('Embedding Diversity vs Sample Size')\n\nplt.tight_layout()\nplt.savefig(f\"{OUTPUT_PATH}/embedding_diversity_distribution.png\", dpi=150)\nplt.show()\n\n# Show species with lowest diversity (potential concern)\nprint(\"\\nSpecies with LOWEST embedding diversity (potential concern):\")\nprint(diversity_df.nsmallest(10, 'mean_cosine_dist')[['short_name', 'n_genomes', 'mean_cosine_dist']])",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 3: Extract Data for Expanded Species Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 6: Extract Embeddings for Target Genomes\n\ntarget_genome_ids = target_genomes_df['genome_id'].tolist()\n\nembeddings_df = spark.sql(\"\"\"\n    SELECT *\n    FROM kbase_ke_pangenome.alphaearth_embeddings_all_years\n\"\"\").filter(col(\"genome_id\").isin(target_genome_ids))\n\nembeddings_pd = embeddings_df.toPandas()\nembeddings_pd.to_csv(f\"{OUTPUT_PATH}/embeddings_expanded.csv\", index=False)\nprint(f\"Saved {len(embeddings_pd)} embeddings\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 7: Extract Within-Species ANI (Chunked)\n\nfrom pyspark.sql.functions import monotonically_increasing_id\nimport os\n\nANI_OUTPUT_PATH = f\"{OUTPUT_PATH}/ani_expanded\"\nos.makedirs(ANI_OUTPUT_PATH, exist_ok=True)\n\n# Get ANI for target genomes (within-species pairs only)\nani_df = spark.sql(\"\"\"\n    SELECT\n        a.genome1_id,\n        a.genome2_id,\n        a.ANI,\n        g.gtdb_species_clade_id\n    FROM kbase_ke_pangenome.genome_ani a\n    JOIN kbase_ke_pangenome.genome g ON a.genome1_id = g.genome_id\n\"\"\").filter(\n    col(\"genome1_id\").isin(target_genome_ids) &\n    col(\"genome2_id\").isin(target_genome_ids)\n)\n\nani_df.cache()\ntotal_count = ani_df.count()\nprint(f\"ANI pairs: {total_count}\")\n\n# Export in chunks\nCHUNK_SIZE = 1000000\nani_with_id = ani_df.withColumn(\"_id\", monotonically_increasing_id())\n\nn_chunks = (total_count // CHUNK_SIZE) + 1\nfor i in range(n_chunks):\n    start_id = i * CHUNK_SIZE\n    end_id = (i + 1) * CHUNK_SIZE\n    \n    chunk = ani_with_id.filter(\n        (col(\"_id\") >= start_id) & (col(\"_id\") < end_id)\n    ).drop(\"_id\")\n    \n    chunk_pd = chunk.toPandas()\n    chunk_pd.to_csv(f\"{ANI_OUTPUT_PATH}/ani_chunk_{i:03d}.csv\", index=False)\n    print(f\"  Saved chunk {i+1}/{n_chunks}: {len(chunk_pd)} rows\")\n\nprint(f\"\\nCompleted ANI export to {ANI_OUTPUT_PATH}\")"
  },
  {
   "cell_type": "code",
   "source": "# Cell 8: Extract Gene Clusters per Genome (Chunked)\n\nCLUSTERS_OUTPUT_PATH = f\"{OUTPUT_PATH}/gene_clusters_expanded\"\nos.makedirs(CLUSTERS_OUTPUT_PATH, exist_ok=True)\n\n# Get gene clusters for target genomes\ngenome_clusters_df = spark.sql(\"\"\"\n    SELECT\n        g.genome_id,\n        gg.gene_cluster_id,\n        gm.gtdb_species_clade_id\n    FROM kbase_ke_pangenome.gene g\n    JOIN kbase_ke_pangenome.gene_genecluster_junction gg\n        ON g.gene_id = gg.gene_id\n    JOIN kbase_ke_pangenome.genome gm\n        ON g.genome_id = gm.genome_id\n\"\"\").filter(col(\"genome_id\").isin(target_genome_ids))\n\ngenome_clusters_df.cache()\ntotal_count = genome_clusters_df.count()\nprint(f\"Gene-cluster associations: {total_count}\")\n\n# Export in chunks\nCHUNK_SIZE = 5000000\nclusters_with_id = genome_clusters_df.withColumn(\"_id\", monotonically_increasing_id())\n\nn_chunks = (total_count // CHUNK_SIZE) + 1\nfor i in range(n_chunks):\n    start_id = i * CHUNK_SIZE\n    end_id = (i + 1) * CHUNK_SIZE\n    \n    chunk = clusters_with_id.filter(\n        (col(\"_id\") >= start_id) & (col(\"_id\") < end_id)\n    ).drop(\"_id\")\n    \n    chunk_pd = chunk.toPandas()\n    chunk_pd.to_csv(f\"{CLUSTERS_OUTPUT_PATH}/clusters_chunk_{i:03d}.csv\", index=False)\n    print(f\"  Saved chunk {i+1}/{n_chunks}: {len(chunk_pd)} rows\")\n\nprint(f\"\\nCompleted gene clusters export to {CLUSTERS_OUTPUT_PATH}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Summary\n\nAfter running this notebook, download the following from the cluster:\n\n- `species_embedding_coverage.csv` - Coverage for all species\n- `target_genomes_expanded.csv` - Selected genomes (all have embeddings)\n- `species_selection_stats.csv` - Selection statistics per species\n- `species_embedding_diversity.csv` - Embedding diversity per species\n- `embedding_diversity_distribution.png` - Visualization of diversity\n- `embeddings_expanded.csv` - Environmental embeddings\n- `ani_expanded/` - Pairwise ANI chunks\n- `gene_clusters_expanded/` - Gene cluster chunks\n\nThen run local analysis to:\n1. Review embedding diversity distribution and decide on any flagging criteria\n2. Compute environment-gene content correlations for all species\n3. Identify which species show strongest environment-ecotype signal"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}