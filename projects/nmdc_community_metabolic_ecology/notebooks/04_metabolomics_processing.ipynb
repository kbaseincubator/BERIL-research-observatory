{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# NB04: Metabolomics Processing\n",
    "\n",
    "**Project**: Community Metabolic Ecology via NMDC × Pangenome Integration  \n",
    "**Requires**: BERDL JupyterHub (Spark — `get_spark_session()` injected into kernel)\n",
    "\n",
    "## Purpose\n",
    "\n",
    "Extract and process NMDC metabolomics data for the 220 overlap samples.\n",
    "Map detected compounds to amino acid categories matching GapMind `'aa'` pathways.\n",
    "Merge metabolomics + community pathway completeness + abiotic features into\n",
    "a single analysis-ready matrix for NB05.\n",
    "\n",
    "**Key decisions carried in from NB03**:\n",
    "- 220 overlap samples (centrifuge + metabolomics, passing bridge QC)\n",
    "- 18 GapMind amino acid pathways: `arg asn chorismate cys gln gly his ile leu lys met phe pro ser thr trp tyr val`\n",
    "- Each sample has 1–4 met files (avg 2.9); HILICZ_POS and C18_POS/NEG fractions\n",
    "\n",
    "## Inputs\n",
    "\n",
    "- `data/nmdc_sample_inventory.csv` — sample × clf_file × met_file mapping\n",
    "- `data/bridge_quality.csv` — per-file bridge QC flags\n",
    "- `data/community_pathway_matrix.csv` — NB03 output (220 samples × 80 pathways)\n",
    "- `nmdc_arkin.metabolomics_gold` — 3.1M rows of measured metabolite features\n",
    "- `nmdc_arkin.abiotic_features` — environmental measurements (pH, temp, etc.)\n",
    "\n",
    "## Outputs\n",
    "\n",
    "- `data/metabolomics_matrix.csv` — per-sample × per-compound normalized intensities\n",
    "- `data/amino_acid_metabolites.csv` — subset: amino acid compounds with pathway mapping\n",
    "- `data/analysis_ready_matrix.csv` — merged: pathway completeness + metabolomics + abiotic\n",
    "- `figures/metabolomics_distribution.png` — compound abundance distributions by ecosystem type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.connect.session.SparkSession at 0x77cd2bea16a0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On BERDL JupyterHub — get_spark_session() is injected into the kernel; no import needed\n",
    "spark = get_spark_session()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_DIR:    /home/cjneely/repos/BERIL-research-observatory/projects/nmdc_community_metabolic_ecology/data\n",
      "FIGURES_DIR: /home/cjneely/repos/BERIL-research-observatory/projects/nmdc_community_metabolic_ecology/figures\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "PROJECT_DIR = os.path.abspath(os.path.join(os.path.dirname('__file__'), '..'))\n",
    "DATA_DIR    = os.path.join(PROJECT_DIR, 'data')\n",
    "FIGURES_DIR = os.path.join(PROJECT_DIR, 'figures')\n",
    "BRIDGE_TBL  = 'nmdc_arkin.omics_files_table'\n",
    "\n",
    "print(f'DATA_DIR:    {DATA_DIR}')\n",
    "print(f'FIGURES_DIR: {FIGURES_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Load NB02/NB03 Outputs and Identify Met Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlap samples: 220\n",
      "Community matrix samples: 220\n",
      "\n",
      "Unique met_file_ids for overlap samples: 644\n",
      "Met files per sample: avg=2.9\n",
      "\n",
      "File name patterns:\n",
      "  HILICZ: 252\n",
      "  C18:    252\n",
      "  POS:    252\n",
      "  NEG:    252\n"
     ]
    }
   ],
   "source": [
    "inventory     = pd.read_csv(os.path.join(DATA_DIR, 'nmdc_sample_inventory.csv'))\n",
    "bridge_quality = pd.read_csv(os.path.join(DATA_DIR, 'bridge_quality.csv'))\n",
    "community_matrix = pd.read_csv(os.path.join(DATA_DIR, 'community_pathway_matrix.csv'))\n",
    "\n",
    "# Overlap samples = those whose clf_file passes bridge QC\n",
    "qc_pass_files   = set(bridge_quality[bridge_quality['passes_bridge_qc']]['file_id'])\n",
    "overlap_samples = set(\n",
    "    inventory[inventory['clf_file_id'].isin(qc_pass_files)]['sample_id']\n",
    ")\n",
    "print(f'Overlap samples: {len(overlap_samples)}')\n",
    "print(f'Community matrix samples: {community_matrix[\"sample_id\"].nunique()}')\n",
    "\n",
    "# Met file IDs for overlap samples (may be multiple per sample)\n",
    "overlap_inv = inventory[inventory['sample_id'].isin(overlap_samples)].copy()\n",
    "overlap_met_file_ids = overlap_inv['met_file_id'].unique().tolist()\n",
    "print(f'\\nUnique met_file_ids for overlap samples: {len(overlap_met_file_ids)}')\n",
    "print(f'Met files per sample: avg={len(overlap_met_file_ids)/len(overlap_samples):.1f}')\n",
    "\n",
    "# Ionization mode / column breakdown from file names\n",
    "fnames = overlap_inv['met_file_name']\n",
    "print(f'\\nFile name patterns:')\n",
    "print(f'  HILICZ: {fnames.str.contains(\"HILICZ\", na=False).sum()}')\n",
    "print(f'  C18:    {fnames.str.contains(\"C18\", na=False).sum()}')\n",
    "print(f'  POS:    {fnames.str.contains(\"_POS_\", na=False).sum()}')\n",
    "print(f'  NEG:    {fnames.str.contains(\"_NEG_\", na=False).sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Explore `metabolomics_gold` Schema\n",
    "\n",
    "Determine column names before building the extraction query.\n",
    "Key unknowns: compound name column, intensity/abundance column, any KEGG ID column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5d73751-ecb0-4f6e-8ed1-475ac2c7d844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+---------+\n",
      "|                name|   Intensity|Peak Area|\n",
      "+--------------------+------------+---------+\n",
      "|       Carvone, (-)-|  2504080.25|Peak Area|\n",
      "|           Lactulose|1.70560928E8|Peak Area|\n",
      "|              Maltol|   8972314.0|Peak Area|\n",
      "|13-keto-9Z,11E-oc...|   4.65551E7|Peak Area|\n",
      "|      Pinolenic acid|   8816448.0|Peak Area|\n",
      "+--------------------+------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select name, Intensity, \\\"Peak Area\\\" from nmdc_arkin.metabolomics_gold where chebi is not null limit 5;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== metabolomics_gold schema ===\n",
      "+--------------------------------------------+---------+-------+\n",
      "|col_name                                    |data_type|comment|\n",
      "+--------------------------------------------+---------+-------+\n",
      "|file_id                                     |string   |NULL   |\n",
      "|file_name                                   |string   |NULL   |\n",
      "|feature_id                                  |string   |NULL   |\n",
      "|Apex Scan Number                            |double   |NULL   |\n",
      "|Area                                        |double   |NULL   |\n",
      "|Associated Mass Features after Deconvolution|string   |NULL   |\n",
      "|Calculated m/z                              |double   |NULL   |\n",
      "|Confidence Score                            |double   |NULL   |\n",
      "|Dispersity Index                            |double   |NULL   |\n",
      "|Entropy Similarity                          |double   |NULL   |\n",
      "|Intensity                                   |double   |NULL   |\n",
      "|Ion Formula                                 |string   |NULL   |\n",
      "|Ion Type                                    |string   |NULL   |\n",
      "|Is Largest Ion after Deconvolution          |boolean  |NULL   |\n",
      "|Isotopologue Similarity                     |double   |NULL   |\n",
      "|Isotopologue Type                           |string   |NULL   |\n",
      "|Library mzs in Query (fraction)             |double   |NULL   |\n",
      "|MS2 Spectrum                                |string   |NULL   |\n",
      "|Mass Feature ID                             |bigint   |NULL   |\n",
      "|Molecular Formula                           |string   |NULL   |\n",
      "|Monoisotopic Mass Feature ID                |double   |NULL   |\n",
      "|Persistence                                 |double   |NULL   |\n",
      "|Polarity                                    |string   |NULL   |\n",
      "|Retention Time (min)                        |double   |NULL   |\n",
      "|Sample Name                                 |string   |NULL   |\n",
      "|Spectra with Annotation (n)                 |double   |NULL   |\n",
      "|Tailing Factor                              |double   |NULL   |\n",
      "|chebi                                       |double   |NULL   |\n",
      "|database_name                               |string   |NULL   |\n",
      "|final_scan                                  |bigint   |NULL   |\n",
      "|inchi                                       |string   |NULL   |\n",
      "|inchikey                                    |string   |NULL   |\n",
      "|kegg                                        |string   |NULL   |\n",
      "|m/z                                         |double   |NULL   |\n",
      "|m/z Error (ppm)                             |double   |NULL   |\n",
      "|m/z Error Score                             |double   |NULL   |\n",
      "|name                                        |string   |NULL   |\n",
      "|noise_score                                 |double   |NULL   |\n",
      "|noise_score_max                             |double   |NULL   |\n",
      "|noise_score_min                             |double   |NULL   |\n",
      "|normalized_dispersity_index                 |double   |NULL   |\n",
      "|ref_ms_id                                   |string   |NULL   |\n",
      "|smiles                                      |string   |NULL   |\n",
      "|start_scan                                  |bigint   |NULL   |\n",
      "|Peak Area                                   |double   |NULL   |\n",
      "|Traditional Name                            |string   |NULL   |\n",
      "|Spectral Similarity Score                   |string   |NULL   |\n",
      "|Similarity Score                            |string   |NULL   |\n",
      "|Retention Time Ref                          |string   |NULL   |\n",
      "|Retention Index Score                       |string   |NULL   |\n",
      "+--------------------------------------------+---------+-------+\n",
      "only showing top 50 rows\n"
     ]
    }
   ],
   "source": [
    "print('=== metabolomics_gold schema ===')\n",
    "spark.sql('DESCRIBE nmdc_arkin.metabolomics_gold').show(50, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sample rows from metabolomics_gold ===\n",
      "+---------------------+-----------------------------------------------------------------------------------------------------------------------------------+----------+----------------+------------------+---------------------------------------------------------------------------+------------------+------------------+------------------+------------------+------------+----------------+--------+----------------------------------+-----------------------+-----------------+-------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------+-----------------+----------------------------+------------+--------+--------------------+-------------------------------------------------------------------------------------------------------------------------------+---------------------------+------------------+-----+-------------+----------+-----+--------+----+------------------+-------------------+------------------+----+------------------+------------------+------------------+---------------------------+---------+------+----------+---------+----------------+-------------------------+----------------+------------------+---------------------+----------------+-------------------+---------------+-----------------------+-------------+----------+---------+--------------+-----------+--------------+--------+----------+-----------+\n",
      "|file_id              |file_name                                                                                                                          |feature_id|Apex Scan Number|Area              |Associated Mass Features after Deconvolution                               |Calculated m/z    |Confidence Score  |Dispersity Index  |Entropy Similarity|Intensity   |Ion Formula     |Ion Type|Is Largest Ion after Deconvolution|Isotopologue Similarity|Isotopologue Type|Library mzs in Query (fraction)|MS2 Spectrum                                                                                                                                                                                                                                                   |Mass Feature ID|Molecular Formula|Monoisotopic Mass Feature ID|Persistence |Polarity|Retention Time (min)|Sample Name                                                                                                                    |Spectra with Annotation (n)|Tailing Factor    |chebi|database_name|final_scan|inchi|inchikey|kegg|m/z               |m/z Error (ppm)    |m/z Error Score   |name|noise_score       |noise_score_max   |noise_score_min   |normalized_dispersity_index|ref_ms_id|smiles|start_scan|Peak Area|Traditional Name|Spectral Similarity Score|Similarity Score|Retention Time Ref|Retention Index Score|Kegg Compound ID|Retention index Ref|Retention index|Half Height Width (min)|Compound Name|IUPAC Name|Inchi Key|Retention Time|Peak Height|Derivatization|Chebi ID|Peak Index|Common Name|\n",
      "+---------------------+-----------------------------------------------------------------------------------------------------------------------------------+----------+----------------+------------------+---------------------------------------------------------------------------+------------------+------------------+------------------+------------------+------------+----------------+--------+----------------------------------+-----------------------+-----------------+-------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------+-----------------+----------------------------+------------+--------+--------------------+-------------------------------------------------------------------------------------------------------------------------------+---------------------------+------------------+-----+-------------+----------+-----+--------+----+------------------+-------------------+------------------+----+------------------+------------------+------------------+---------------------------+---------+------+----------+---------+----------------+-------------------------+----------------+------------------+---------------------+----------------+-------------------+---------------+-----------------------+-------------+----------+---------+--------------+-----------+--------------+--------+----------+-----------+\n",
      "|nmdc:dobj-12-c6668e59|20220119_JGI_MD_507130_BioS_final1_QE-139_HILICZ_USHXG01825_NEG_MSMS_84RM_BESC-360-Corv-RM_1_Rg70to1050-CE102040-root-S1_Run722.csv|1823      |5536.0          |207312.58836064488|104, 167, 491, 524, 726, 746, 833, 1607, 1658, 1823, 2135, 2174, 2277, 2334|505.16501065439905|0.4998728013228436|0.009356166646272 |NULL              |4624002.0   |C24 H29 O8 N2 S1|NULL    |false                             |0.7723194841954907     |NULL             |NULL                           |NULL                                                                                                                                                                                                                                                           |1823           |NULL             |NULL                        |4624002.0   |negative|15.37629222869873   |20220119_JGI_MD_507130_BioS_final1_QE-139_HILICZ_USHXG01825_NEG_MSMS_84RM_BESC-360-Corv-RM_1_Rg70to1050-CE102040-root-S1_Run722|NULL                       |0.909089972598224 |NULL |NULL         |5554      |NULL |NULL    |NULL|505.1663513183594 |2.6539129433839457 |0.3182416794077456|NULL|0.7646735703542076|0.9221820515613964|0.6071650891470185|0.0015614843839305         |NULL     |NULL  |5515      |NULL     |NULL            |NULL                     |NULL            |NULL              |NULL                 |NULL            |NULL               |NULL           |NULL                   |NULL         |NULL      |NULL     |NULL          |NULL       |NULL          |NULL    |NULL      |NULL       |\n",
      "|nmdc:dobj-12-c6668e59|20220119_JGI_MD_507130_BioS_final1_QE-139_HILICZ_USHXG01825_NEG_MSMS_84RM_BESC-360-Corv-RM_1_Rg70to1050-CE102040-root-S1_Run722.csv|194       |643.0           |1996123.4725409448|194, 828, 970, 1099, 2379                                                  |399.1846834708591 |0.8623966979911594|0.0038853883743286|NULL              |5.6286152E7 |C20 H31 O6 S1   |NULL    |true                              |0.9805461274215276     |NULL             |NULL                           |96.9602:0.09; 399.1851:1.0                                                                                                                                                                                                                                     |194            |NULL             |194.0                       |5.6286152E7 |negative|1.768058180809021   |20220119_JGI_MD_507130_BioS_final1_QE-139_HILICZ_USHXG01825_NEG_MSMS_84RM_BESC-360-Corv-RM_1_Rg70to1050-CE102040-root-S1_Run722|NULL                       |1.5783462748289745|NULL |NULL         |679       |NULL |NULL    |NULL|399.184814453125  |0.328124477993345  |0.7836304117042472|NULL|0.9270566902694404|0.952969921876794 |0.9011434586620864|8.17346627209E-4           |NULL     |NULL  |628       |NULL     |NULL            |NULL                     |NULL            |NULL              |NULL                 |NULL            |NULL               |NULL           |NULL                   |NULL         |NULL      |NULL     |NULL          |NULL       |NULL          |NULL    |NULL      |NULL       |\n",
      "|nmdc:dobj-12-c6668e59|20220119_JGI_MD_507130_BioS_final1_QE-139_HILICZ_USHXG01825_NEG_MSMS_84RM_BESC-360-Corv-RM_1_Rg70to1050-CE102040-root-S1_Run722.csv|45        |271.0           |6817671.240330413 |45, 97, 188, 272, 735, 1069                                                |445.3323335085391 |0.6158845203521177|0.0038833320140838|NULL              |1.92782656E8|C28 H45 O4      |NULL    |true                              |0.990726622176506      |NULL             |NULL                           |133.0296:0.11; 177.0193:0.05; 430.3088:0.32; 445.3322:1.0                                                                                                                                                                                                      |45             |NULL             |45.0                        |1.92782656E8|negative|0.7712243795394897  |20220119_JGI_MD_507130_BioS_final1_QE-139_HILICZ_USHXG01825_NEG_MSMS_84RM_BESC-360-Corv-RM_1_Rg70to1050-CE102040-root-S1_Run722|NULL                       |0.8804817595393114|NULL |NULL         |283       |NULL |NULL    |NULL|445.33209228515625|-0.5416704889335812|0.3659897858025256|NULL|0.9869020785675428|1.0               |0.9738041571350856|0.0010336373063757         |NULL     |NULL  |253       |NULL     |NULL            |NULL                     |NULL            |NULL              |NULL                 |NULL            |NULL               |NULL           |NULL                   |NULL         |NULL      |NULL     |NULL          |NULL       |NULL          |NULL    |NULL      |NULL       |\n",
      "|nmdc:dobj-12-c6668e59|20220119_JGI_MD_507130_BioS_final1_QE-139_HILICZ_USHXG01825_NEG_MSMS_84RM_BESC-360-Corv-RM_1_Rg70to1050-CE102040-root-S1_Run722.csv|534       |3184.0          |618261.4868396148 |62, 534, 708, 803, 1286, 1934, 2459                                        |NULL              |NULL              |0.0               |NULL              |2.0292116E7 |NULL            |NULL    |false                             |NULL                   |NULL             |NULL                           |57.3553:0.01; 64.43:0.01; 79.9569:0.14; 80.9647:0.02; 95.9522:0.04; 96.96:1.0; 133.904:0.01; 172.0124:0.95; 172.5139:0.11; 172.8308:0.02; 181.0176:0.4; 182.0209:0.02; 247.0642:0.14; 248.0664:0.02; 273.2598:0.01; 296.2651:0.01; 311.0364:0.01; 319.4683:0.01|534            |NULL             |NULL                        |2.0292116E7 |negative|8.802865982055664   |20220119_JGI_MD_507130_BioS_final1_QE-139_HILICZ_USHXG01825_NEG_MSMS_84RM_BESC-360-Corv-RM_1_Rg70to1050-CE102040-root-S1_Run722|NULL                       |1.118663847780127 |NULL |NULL         |3208      |NULL |NULL    |NULL|172.0122528076172 |NULL               |NULL              |NULL|1.0               |1.0               |1.0               |0.0                        |NULL     |NULL  |3166      |NULL     |NULL            |NULL                     |NULL            |NULL              |NULL                 |NULL            |NULL               |NULL           |NULL                   |NULL         |NULL      |NULL     |NULL          |NULL       |NULL          |NULL    |NULL      |NULL       |\n",
      "|nmdc:dobj-12-c6668e59|20220119_JGI_MD_507130_BioS_final1_QE-139_HILICZ_USHXG01825_NEG_MSMS_84RM_BESC-360-Corv-RM_1_Rg70to1050-CE102040-root-S1_Run722.csv|750       |1609.0          |1236735.067583379 |750, 945, 1581                                                             |249.0434527681991 |0.6133640927005115|0.0210686804710109|NULL              |1.3480962E7 |C11 H10 O3 N2 P1|NULL    |true                              |0.8936766034327009     |NULL             |NULL                           |96.96:1.0; 249.0435:0.1; 249.08:0.02                                                                                                                                                                                                                           |750            |NULL             |NULL                        |1.3480962E7 |negative|4.419151306152344   |20220119_JGI_MD_507130_BioS_final1_QE-139_HILICZ_USHXG01825_NEG_MSMS_84RM_BESC-360-Corv-RM_1_Rg70to1050-CE102040-root-S1_Run722|NULL                       |0.9243409989181778|NULL |NULL         |1708      |NULL |NULL    |NULL|249.0436859130859 |0.9361614780199936 |0.4264890855457184|NULL|0.8925020802252861|0.9855993560493296|0.7994048044012427|0.0035168828868922         |NULL     |NULL  |1546      |NULL     |NULL            |NULL                     |NULL            |NULL              |NULL                 |NULL            |NULL               |NULL           |NULL                   |NULL         |NULL      |NULL     |NULL          |NULL       |NULL          |NULL    |NULL      |NULL       |\n",
      "+---------------------+-----------------------------------------------------------------------------------------------------------------------------------+----------+----------------+------------------+---------------------------------------------------------------------------+------------------+------------------+------------------+------------------+------------+----------------+--------+----------------------------------+-----------------------+-----------------+-------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------+-----------------+----------------------------+------------+--------+--------------------+-------------------------------------------------------------------------------------------------------------------------------+---------------------------+------------------+-----+-------------+----------+-----+--------+----+------------------+-------------------+------------------+----+------------------+------------------+------------------+---------------------------+---------+------+----------+---------+----------------+-------------------------+----------------+------------------+---------------------+----------------+-------------------+---------------+-----------------------+-------------+----------+---------+--------------+-----------+--------------+--------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample 5 rows to understand data layout\n",
    "# Use LIMIT — metabolomics_gold has 3.1M rows\n",
    "print('=== Sample rows from metabolomics_gold ===')\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM nmdc_arkin.metabolomics_gold\n",
    "    LIMIT 5\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== file_id format in metabolomics_gold ===\n",
      "+---------------------+-----+\n",
      "|file_id              |n    |\n",
      "+---------------------+-----+\n",
      "|nmdc:dobj-12-tsrn1q55|12281|\n",
      "|nmdc:dobj-12-e7vpe098|11461|\n",
      "|nmdc:dobj-12-j7tq4n27|11355|\n",
      "|nmdc:dobj-12-kk96n084|11162|\n",
      "|nmdc:dobj-12-0zjp6543|10871|\n",
      "|nmdc:dobj-12-v98y9q72|10706|\n",
      "|nmdc:dobj-12-3j09wy18|10576|\n",
      "|nmdc:dobj-12-h0c0js15|10535|\n",
      "|nmdc:dobj-12-4ks2cq70|9184 |\n",
      "|nmdc:dobj-12-9241q096|8766 |\n",
      "+---------------------+-----+\n",
      "\n",
      "\n",
      "Row count for one sample met file (nmdc:dobj-12-716d9s20):\n",
      "+---+\n",
      "|  n|\n",
      "+---+\n",
      "|810|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Confirm the file_id format for metabolomics_gold (expect nmdc:dobj-12-* prefix)\n",
    "print('=== file_id format in metabolomics_gold ===')\n",
    "spark.sql(\"\"\"\n",
    "    SELECT file_id, COUNT(*) as n\n",
    "    FROM nmdc_arkin.metabolomics_gold\n",
    "    GROUP BY file_id\n",
    "    ORDER BY n DESC\n",
    "    LIMIT 10\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "# Check total rows for overlap met_file_ids\n",
    "# Build a small validation using one sample met_file_id\n",
    "sample_met_id = overlap_met_file_ids[0]\n",
    "print(f'\\nRow count for one sample met file ({sample_met_id}):')\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT COUNT(*) as n\n",
    "    FROM nmdc_arkin.metabolomics_gold\n",
    "    WHERE file_id = '{sample_met_id}'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Extract Metabolomics Data for Overlap Samples\n",
    "\n",
    "Use the INTERSECT subquery pattern to filter to overlap samples\n",
    "(avoids pandas→Spark roundtrip / ChunkedArray issues).\n",
    "\n",
    "**Column names below use placeholders** — update after schema inspection in Part 2:\n",
    "- `COMPOUND_COL`: the compound name column  \n",
    "- `INTENSITY_COL`: the abundance/intensity column  \n",
    "- Add `KEGG_COL` if a KEGG compound ID column exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using compound column:  name\n",
      "Using intensity column: Intensity\n",
      "KEGG ID column:         kegg\n"
     ]
    }
   ],
   "source": [
    "# ── Set column names based on schema inspection above ──────────────────────\n",
    "# Update these after running Part 2.\n",
    "# Common NMDC metabolomics_gold column names (verify from DESCRIBE output):\n",
    "#   compound name: 'compound_name', 'metabolite_name', 'feature_name', 'name'\n",
    "#   intensity:     'abundance', 'intensity', 'peak_area', 'normalized_abundance'\n",
    "#   KEGG ID:       'kegg_id', 'compound_id', 'annotation_id' (may not exist)\n",
    "# ---------------------------------------------------------------------------\n",
    "COMPOUND_COL  = 'name'     # UPDATE if different\n",
    "INTENSITY_COL = 'Intensity'         # UPDATE if different\n",
    "KEGG_COL      = 'kegg'                # Set to column name if KEGG IDs exist\n",
    "\n",
    "print(f'Using compound column:  {COMPOUND_COL}')\n",
    "print(f'Using intensity column: {INTENSITY_COL}')\n",
    "print(f'KEGG ID column:         {KEGG_COL}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting metabolomics data (may take a few minutes)...\n",
      "\n",
      "Metabolomics rows: 33,726\n",
      "Unique samples:   175\n",
      "Unique files:     553\n",
      "Unique compounds: 1,944\n",
      "Intensity range:  [7.41e+04, 4.54e+09]\n",
      "              sample_id                file_id           compound_name    intensity kegg_id\n",
      "0  nmdc:bsm-11-dz264p50  nmdc:dobj-12-d1v5ag89   3-Hydroxybenzaldehyde   9118199.00  C03067\n",
      "1  nmdc:bsm-11-tcxb3w32  nmdc:dobj-12-d7nkac20      Phthalic anhydride  42594000.00     NaN\n",
      "2  nmdc:bsm-11-cv5vxb38  nmdc:dobj-12-dj8n6c53                 Choline  57018780.00  C00114\n",
      "3  nmdc:bsm-11-vp7n5531  nmdc:dobj-12-dn3mvn79                D-Ribose   3060144.25     NaN\n",
      "4  nmdc:bsm-11-ck98jg55  nmdc:dobj-12-edfh5e04  4-Hydroxycinnamic acid  19322698.00     NaN\n"
     ]
    }
   ],
   "source": [
    "# Build SELECT clause (include KEGG_COL only if it exists)\n",
    "kegg_select = f', m.{KEGG_COL} AS kegg_id' if KEGG_COL else ''\n",
    "\n",
    "# Extract metabolomics data for overlap samples via Spark.\n",
    "# Filter: INTERSECT of samples with centrifuge + metabolomics files\n",
    "# Cast intensity to DOUBLE to avoid decimal.Decimal arithmetic errors\n",
    "met_spark = spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        b.sample_id,\n",
    "        m.file_id,\n",
    "        m.{COMPOUND_COL} AS compound_name,\n",
    "        CAST(m.{INTENSITY_COL} AS DOUBLE) AS intensity\n",
    "        {kegg_select}\n",
    "    FROM nmdc_arkin.metabolomics_gold m\n",
    "    JOIN {BRIDGE_TBL} b ON m.file_id = b.file_id\n",
    "    WHERE b.sample_id IN (\n",
    "        SELECT b2.sample_id FROM {BRIDGE_TBL} b2\n",
    "        JOIN (SELECT DISTINCT file_id FROM nmdc_arkin.centrifuge_gold) c2\n",
    "          ON b2.file_id = c2.file_id\n",
    "        INTERSECT\n",
    "        SELECT b3.sample_id FROM {BRIDGE_TBL} b3\n",
    "        JOIN (SELECT DISTINCT file_id FROM nmdc_arkin.metabolomics_gold) m3\n",
    "          ON b3.file_id = m3.file_id\n",
    "    )\n",
    "    AND m.{COMPOUND_COL} IS NOT NULL\n",
    "    AND m.{COMPOUND_COL} != ''\n",
    "    AND m.{INTENSITY_COL} IS NOT NULL\n",
    "    AND CAST(m.{INTENSITY_COL} AS DOUBLE) > 0\n",
    "\"\"\")\n",
    "\n",
    "print('Collecting metabolomics data (may take a few minutes)...')\n",
    "met_raw = met_spark.toPandas()\n",
    "met_raw['intensity'] = met_raw['intensity'].astype(float)  # defensive cast\n",
    "\n",
    "print(f'\\nMetabolomics rows: {len(met_raw):,}')\n",
    "print(f'Unique samples:   {met_raw[\"sample_id\"].nunique()}')\n",
    "print(f'Unique files:     {met_raw[\"file_id\"].nunique()}')\n",
    "print(f'Unique compounds: {met_raw[\"compound_name\"].nunique():,}')\n",
    "print(f'Intensity range:  [{met_raw[\"intensity\"].min():.2e}, {met_raw[\"intensity\"].max():.2e}]')\n",
    "print(met_raw.head(5).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ionization mode breakdown:\n",
      "ionization\n",
      "POS        19494\n",
      "NEG        11933\n",
      "unknown     2262\n",
      "\n",
      "LC column breakdown:\n",
      "lc_column\n",
      "C18        16998\n",
      "HILICZ     14429\n",
      "unknown     2262\n"
     ]
    }
   ],
   "source": [
    "# Map file_id → ionization mode / column type from met_file_name\n",
    "file_meta = overlap_inv[['met_file_id', 'met_file_name', 'sample_id']].rename(\n",
    "    columns={'met_file_id': 'file_id'}\n",
    ").drop_duplicates()\n",
    "file_meta['ionization'] = np.where(\n",
    "    file_meta['met_file_name'].str.contains('_POS_', na=False), 'POS',\n",
    "    np.where(file_meta['met_file_name'].str.contains('_NEG_', na=False), 'NEG', 'unknown')\n",
    ")\n",
    "file_meta['lc_column'] = np.where(\n",
    "    file_meta['met_file_name'].str.contains('HILICZ', na=False), 'HILICZ',\n",
    "    np.where(file_meta['met_file_name'].str.contains('C18', na=False), 'C18', 'unknown')\n",
    ")\n",
    "\n",
    "met_raw = met_raw.merge(file_meta[['file_id', 'ionization', 'lc_column']],\n",
    "                        on='file_id', how='left')\n",
    "print('Ionization mode breakdown:')\n",
    "print(met_raw['ionization'].value_counts().to_string())\n",
    "print('\\nLC column breakdown:')\n",
    "print(met_raw['lc_column'].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Compound Annotation and Amino Acid Mapping\n",
    "\n",
    "Map detected compounds to GapMind amino acid pathways via case-insensitive\n",
    "substring matching on compound names.\n",
    "\n",
    "If `KEGG_COL` was found in Part 2, also attempt KEGG-based matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique compounds in data: 1,944\n",
      "Compounds mapped to aa pathways: 50\n",
      "\n",
      "Compounds matched per aa pathway:\n",
      "arg            2\n",
      "asn            3\n",
      "chorismate     2\n",
      "gln            1\n",
      "gly           10\n",
      "leu            8\n",
      "met            1\n",
      "phe            3\n",
      "pro            4\n",
      "ser            5\n",
      "thr            1\n",
      "trp            2\n",
      "tyr            2\n",
      "val            6\n",
      "\n",
      "aa pathways with NO compound match: ['cys', 'his', 'ile', 'lys']\n"
     ]
    }
   ],
   "source": [
    "# Mapping: GapMind aa pathway → compound name fragments (case-insensitive substring match)\n",
    "# chorismate is the aromatic aa precursor; include shikimic acid as metabolomics proxy\n",
    "AA_PATHWAY_TO_PATTERNS = {\n",
    "    'arg':       ['arginine'],\n",
    "    'asn':       ['asparagine'],\n",
    "    'chorismate': ['chorismate', 'chorismic acid', 'shikimic acid', 'shikimate'],\n",
    "    'cys':       ['cysteine'],\n",
    "    'gln':       ['glutamine'],\n",
    "    'gly':       ['glycine'],\n",
    "    'his':       ['histidine'],\n",
    "    'ile':       ['isoleucine'],\n",
    "    'leu':       ['leucine'],\n",
    "    'lys':       ['lysine'],\n",
    "    'met':       ['methionine'],\n",
    "    'phe':       ['phenylalanine'],\n",
    "    'pro':       ['proline'],\n",
    "    'ser':       ['serine'],\n",
    "    'thr':       ['threonine'],\n",
    "    'trp':       ['tryptophan'],\n",
    "    'tyr':       ['tyrosine'],\n",
    "    'val':       ['valine'],\n",
    "}\n",
    "\n",
    "# Build a compound → pathway mapping by substring match\n",
    "all_compounds = met_raw['compound_name'].dropna().unique()\n",
    "compound_lower = pd.Series(all_compounds).str.lower()\n",
    "\n",
    "compound_pathway_map = {}  # compound_name → aa_pathway\n",
    "for pathway, patterns in AA_PATHWAY_TO_PATTERNS.items():\n",
    "    for compound in all_compounds:\n",
    "        cl = compound.lower()\n",
    "        if any(p in cl for p in patterns):\n",
    "            # Avoid mapping glutamine to glutamic acid and vice versa\n",
    "            if pathway == 'gln' and 'glutamic' in cl:\n",
    "                continue\n",
    "            compound_pathway_map[compound] = pathway\n",
    "\n",
    "print(f'Total unique compounds in data: {len(all_compounds):,}')\n",
    "print(f'Compounds mapped to aa pathways: {len(compound_pathway_map)}')\n",
    "print()\n",
    "# Show matches per pathway\n",
    "pathway_match_counts = pd.Series(compound_pathway_map).value_counts().sort_index()\n",
    "print('Compounds matched per aa pathway:')\n",
    "print(pathway_match_counts.to_string())\n",
    "\n",
    "# Pathways with no matches\n",
    "missing = [p for p in AA_PATHWAY_TO_PATTERNS if p not in pathway_match_counts.index]\n",
    "if missing:\n",
    "    print(f'\\naa pathways with NO compound match: {missing}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched amino acid compounds:\n",
      "                                    compound_name aa_pathway\n",
      "                                         Arginine        arg\n",
      "                                     Homoarginine        arg\n",
      "                                       Asparagine        asn\n",
      "                                    DL-Asparagine        asn\n",
      "                               N-Acetylasparagine        asn\n",
      "                                    Shikimic Acid chorismate\n",
      "                           3-Dehydroshikimic acid chorismate\n",
      "                                     DL-Glutamine        gln\n",
      "                                    Glycylglycine        gly\n",
      "                                  N-Acetylglycine        gly\n",
      "                                  N-oleoylglycine        gly\n",
      "                                   Suberylglycine        gly\n",
      "                           2-Methylbutyrylglycine        gly\n",
      "                       (S)-4-hydroxyphenylglycine        gly\n",
      "                           Phenylpropionylglycine        gly\n",
      "                                  Hexanoylglycine        gly\n",
      "                                Capryloyl Glycine        gly\n",
      "                                Isovalerylglycine        gly\n",
      "                              Alloisoleucine, DL-        leu\n",
      "                 N-[(+)-Jasmonoyl]-(L)-isoleucine        leu\n",
      "N-[(9H-fluoren-9-ylmethoxy)carbonyl]-L-isoleucine        leu\n",
      "                                          Leucine        leu\n",
      "                                      Ketoleucine        leu\n",
      "                               Norleucine, (+/-)-        leu\n",
      "                                    Acetylleucine        leu\n",
      "                               N-Acetyl-L-leucine        leu\n",
      "                                       Methionine        met\n",
      "                              L-Phenylalanine, 23        phe\n",
      "                                    Phenylalanine        phe\n",
      "                         N-Acetyl-L-phenylalanine        phe\n",
      "                                        D-Proline        pro\n",
      "                                  proline dl-form        pro\n",
      "                                  Histidylproline        pro\n",
      "                          Cis-4-Hydroxy-D-Proline        pro\n",
      "                                       d,l-Serine        ser\n",
      "                                     L-homoserine        ser\n",
      "                                    3-Hexylserine        ser\n",
      "                                        DL-Serine        ser\n",
      "                               3-Phenyl-DL-Serine        ser\n",
      "                                Allothreonine, L-        thr\n",
      "                                       Tryptophan        trp\n",
      "             N-[4-(1-adamantyl)benzoyl]tryptophan        trp\n",
      "                                         Tyrosine        tyr\n",
      "                                      DL-Tyrosine        tyr\n",
      "                                        Norvaline        val\n",
      "                                     Glycylvaline        val\n",
      "                                     DL-norvaline        val\n",
      "                                  Alanylnorvaline        val\n",
      "                                        DL-valine        val\n",
      "                                L-Leucyl-L-Valine        val\n"
     ]
    }
   ],
   "source": [
    "# Show matched compound names for verification\n",
    "matched_df = pd.DataFrame(\n",
    "    list(compound_pathway_map.items()), columns=['compound_name', 'aa_pathway']\n",
    ").sort_values('aa_pathway')\n",
    "\n",
    "print('Matched amino acid compounds:')\n",
    "print(matched_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Normalization and Per-Sample Aggregation\n",
    "\n",
    "Strategy:\n",
    "1. Log1p-transform intensity values (standard for LC-MS data; handles dynamic range)\n",
    "2. For each (sample, compound): take **max** across all met files\n",
    "   (same compound detected in HILICZ_POS, C18_POS, C18_NEG — take strongest signal)\n",
    "3. Pivot to sample × compound matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cell-17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After aggregation (max per sample-compound): 26,465 rows\n",
      "Unique samples:   175\n",
      "Unique compounds: 1,944\n",
      "\n",
      "log_intensity distribution:\n",
      "count    26465.000000\n",
      "mean        16.111302\n",
      "std          1.647851\n",
      "min         11.946451\n",
      "25%         14.952030\n",
      "50%         15.942837\n",
      "75%         17.138308\n",
      "max         22.236964\n"
     ]
    }
   ],
   "source": [
    "# Log1p transform\n",
    "met_raw['log_intensity'] = np.log1p(met_raw['intensity'])\n",
    "\n",
    "# Aggregate per (sample_id, compound_name): take max log_intensity across files\n",
    "met_agg = (\n",
    "    met_raw.groupby(['sample_id', 'compound_name'])['log_intensity']\n",
    "    .max()\n",
    "    .reset_index()\n",
    "    .rename(columns={'log_intensity': 'log_intensity_max'})\n",
    ")\n",
    "\n",
    "print(f'After aggregation (max per sample-compound): {len(met_agg):,} rows')\n",
    "print(f'Unique samples:   {met_agg[\"sample_id\"].nunique()}')\n",
    "print(f'Unique compounds: {met_agg[\"compound_name\"].nunique():,}')\n",
    "print(f'\\nlog_intensity distribution:')\n",
    "print(met_agg['log_intensity_max'].describe().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cell-18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prevalent compounds (>=10% samples): 448\n",
      "AA compounds always kept:                          50\n",
      "Total compounds after filter:                      475\n",
      "Filtered rows:                                     19,583\n"
     ]
    }
   ],
   "source": [
    "# Filter to compounds detected in at least 10% of samples (min prevalence filter)\n",
    "# This removes rare contaminants and reduces sparsity in the matrix\n",
    "MIN_PREVALENCE = 0.10\n",
    "n_samples_total = met_agg['sample_id'].nunique()\n",
    "compound_prevalence = (\n",
    "    met_agg.groupby('compound_name')['sample_id']\n",
    "    .nunique()\n",
    "    .rename('n_samples')\n",
    "    / n_samples_total\n",
    ")\n",
    "prevalent_compounds = compound_prevalence[compound_prevalence >= MIN_PREVALENCE].index\n",
    "\n",
    "# Always keep aa-mapped compounds regardless of prevalence\n",
    "aa_compounds = set(compound_pathway_map.keys())\n",
    "keep_compounds = set(prevalent_compounds) | aa_compounds\n",
    "\n",
    "met_filtered = met_agg[met_agg['compound_name'].isin(keep_compounds)].copy()\n",
    "\n",
    "print(f'Prevalent compounds (>={MIN_PREVALENCE:.0%} samples): {len(prevalent_compounds):,}')\n",
    "print(f'AA compounds always kept:                          {len(aa_compounds)}')\n",
    "print(f'Total compounds after filter:                      {len(keep_compounds):,}')\n",
    "print(f'Filtered rows:                                     {len(met_filtered):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cell-19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metabolomics matrix (wide): (175, 476)\n",
      "  Samples:   175\n",
      "  Compounds: 475\n",
      "\n",
      "Sparsity: 76.4% missing\n"
     ]
    }
   ],
   "source": [
    "# Pivot to wide format: samples × compounds\n",
    "met_wide = met_filtered.pivot_table(\n",
    "    index='sample_id',\n",
    "    columns='compound_name',\n",
    "    values='log_intensity_max',\n",
    "    aggfunc='first'\n",
    ").reset_index()\n",
    "\n",
    "compound_cols = [c for c in met_wide.columns if c != 'sample_id']\n",
    "print(f'Metabolomics matrix (wide): {met_wide.shape}')\n",
    "print(f'  Samples:   {len(met_wide)}')\n",
    "print(f'  Compounds: {len(compound_cols)}')\n",
    "print(f'\\nSparsity: {met_wide[compound_cols].isna().mean().mean():.1%} missing')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: Amino Acid Metabolomics Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cell-21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amino acid metabolomics rows: 1,036\n",
      "Samples with ≥1 AA compound: 131\n",
      "AA pathways covered:         14 / 18\n",
      "\n",
      "AA pathway detection rates:\n",
      "aa_pathway  n_samples_detected      pct\n",
      "       gly                 114 0.651429\n",
      "       phe                  88 0.502857\n",
      "       arg                  80 0.457143\n",
      "       asn                  73 0.417143\n",
      "       leu                  69 0.394286\n",
      "chorismate                  65 0.371429\n",
      "       ser                  59 0.337143\n",
      "       val                  54 0.308571\n",
      "       trp                  44 0.251429\n",
      "       tyr                  26 0.148571\n",
      "       thr                  23 0.131429\n",
      "       met                  18 0.102857\n",
      "       pro                   9 0.051429\n",
      "       gln                   4 0.022857\n"
     ]
    }
   ],
   "source": [
    "# Build amino acid metabolomics table: long format with pathway mapping\n",
    "aa_met = met_filtered[met_filtered['compound_name'].isin(aa_compounds)].copy()\n",
    "aa_met['aa_pathway'] = aa_met['compound_name'].map(compound_pathway_map)\n",
    "\n",
    "print(f'Amino acid metabolomics rows: {len(aa_met):,}')\n",
    "print(f'Samples with ≥1 AA compound: {aa_met[\"sample_id\"].nunique()}')\n",
    "print(f'AA pathways covered:         {aa_met[\"aa_pathway\"].nunique()} / 18')\n",
    "print()\n",
    "\n",
    "# Per-pathway: how many samples have a detection?\n",
    "aa_coverage = (\n",
    "    aa_met.groupby('aa_pathway')['sample_id']\n",
    "    .nunique()\n",
    "    .rename('n_samples_detected')\n",
    "    .reset_index()\n",
    "    .sort_values('n_samples_detected', ascending=False)\n",
    ")\n",
    "aa_coverage['pct'] = aa_coverage['n_samples_detected'] / n_samples_total\n",
    "print('AA pathway detection rates:')\n",
    "print(aa_coverage.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cell-22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AA by pathway rows (sample × pathway): 726\n",
      "       log_intensity\n",
      "count     726.000000\n",
      "mean       16.608985\n",
      "std         1.356363\n",
      "min        12.559582\n",
      "25%        15.694084\n",
      "50%        16.398032\n",
      "75%        17.381750\n",
      "max        21.443584\n"
     ]
    }
   ],
   "source": [
    "# For each aa_pathway × sample: aggregate across multiple compound matches\n",
    "# (e.g., 'L-Leucine' and 'leucine' both map to 'leu' — take max)\n",
    "aa_by_pathway = (\n",
    "    aa_met.groupby(['sample_id', 'aa_pathway'])['log_intensity_max']\n",
    "    .max()\n",
    "    .rename('log_intensity')\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(f'AA by pathway rows (sample × pathway): {len(aa_by_pathway):,}')\n",
    "print(aa_by_pathway.describe().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 7: Abiotic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cell-24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== abiotic_features schema ===\n",
      "+-------------------------------------------------+---------+-------+\n",
      "|col_name                                         |data_type|comment|\n",
      "+-------------------------------------------------+---------+-------+\n",
      "|sample_id                                        |string   |NULL   |\n",
      "|annotations_ammonium_has_numeric_value           |double   |NULL   |\n",
      "|annotations_ammonium_nitrogen_has_numeric_value  |double   |NULL   |\n",
      "|annotations_calcium_has_numeric_value            |double   |NULL   |\n",
      "|annotations_carb_nitro_ratio_has_numeric_value   |double   |NULL   |\n",
      "|annotations_chlorophyll_has_numeric_value        |double   |NULL   |\n",
      "|annotations_conduc_has_numeric_value             |double   |NULL   |\n",
      "|annotations_depth_has_maximum_numeric_value      |double   |NULL   |\n",
      "|annotations_depth_has_minimum_numeric_value      |double   |NULL   |\n",
      "|annotations_depth_has_numeric_value              |double   |NULL   |\n",
      "|annotations_diss_org_carb_has_numeric_value      |double   |NULL   |\n",
      "|annotations_diss_oxygen_has_numeric_value        |double   |NULL   |\n",
      "|annotations_magnesium_has_numeric_value          |double   |NULL   |\n",
      "|annotations_manganese_has_numeric_value          |double   |NULL   |\n",
      "|annotations_ph                                   |double   |NULL   |\n",
      "|annotations_potassium_has_numeric_value          |double   |NULL   |\n",
      "|annotations_samp_size_has_numeric_value          |double   |NULL   |\n",
      "|annotations_soluble_react_phosp_has_numeric_value|double   |NULL   |\n",
      "|annotations_temp_has_numeric_value               |double   |NULL   |\n",
      "|annotations_tot_nitro_content_has_numeric_value  |double   |NULL   |\n",
      "|annotations_tot_org_carb_has_numeric_value       |double   |NULL   |\n",
      "|annotations_tot_phosp_has_numeric_value          |double   |NULL   |\n",
      "+-------------------------------------------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explore abiotic_features schema first\n",
    "print('=== abiotic_features schema ===')\n",
    "spark.sql('DESCRIBE nmdc_arkin.abiotic_features').show(40, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cell-25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sample abiotic_features rows ===\n",
      "+--------------------+--------------------------------------+-----------------------------------------------+-------------------------------------+----------------------------------------------+-----------------------------------------+------------------------------------+-------------------------------------------+-------------------------------------------+-----------------------------------+-------------------------------------------+-----------------------------------------+---------------------------------------+---------------------------------------+--------------+---------------------------------------+---------------------------------------+-------------------------------------------------+----------------------------------+-----------------------------------------------+------------------------------------------+---------------------------------------+\n",
      "|sample_id           |annotations_ammonium_has_numeric_value|annotations_ammonium_nitrogen_has_numeric_value|annotations_calcium_has_numeric_value|annotations_carb_nitro_ratio_has_numeric_value|annotations_chlorophyll_has_numeric_value|annotations_conduc_has_numeric_value|annotations_depth_has_maximum_numeric_value|annotations_depth_has_minimum_numeric_value|annotations_depth_has_numeric_value|annotations_diss_org_carb_has_numeric_value|annotations_diss_oxygen_has_numeric_value|annotations_magnesium_has_numeric_value|annotations_manganese_has_numeric_value|annotations_ph|annotations_potassium_has_numeric_value|annotations_samp_size_has_numeric_value|annotations_soluble_react_phosp_has_numeric_value|annotations_temp_has_numeric_value|annotations_tot_nitro_content_has_numeric_value|annotations_tot_org_carb_has_numeric_value|annotations_tot_phosp_has_numeric_value|\n",
      "+--------------------+--------------------------------------+-----------------------------------------------+-------------------------------------+----------------------------------------------+-----------------------------------------+------------------------------------+-------------------------------------------+-------------------------------------------+-----------------------------------+-------------------------------------------+-----------------------------------------+---------------------------------------+---------------------------------------+--------------+---------------------------------------+---------------------------------------+-------------------------------------------------+----------------------------------+-----------------------------------------------+------------------------------------------+---------------------------------------+\n",
      "|nmdc:bsm-11-042nd237|0.0                                   |0.0                                            |0.0                                  |0.0                                           |0.0                                      |0.0                                 |0.0                                        |0.02                                       |0.0                                |0.0                                        |0.0                                      |0.0                                    |0.0                                    |0.0           |0.0                                    |0.0                                    |0.0                                              |0.0                               |0.0                                            |0.0                                       |0.0                                    |\n",
      "|nmdc:bsm-11-622k6044|0.0                                   |0.0                                            |0.0                                  |0.0                                           |0.0                                      |0.0                                 |0.0                                        |0.02                                       |0.0                                |0.0                                        |0.0                                      |0.0                                    |0.0                                    |0.0           |0.0                                    |0.0                                    |0.0                                              |0.0                               |0.0                                            |0.0                                       |0.0                                    |\n",
      "|nmdc:bsm-11-65a4xw75|0.0                                   |0.0                                            |0.0                                  |0.0                                           |0.0                                      |0.0                                 |0.0                                        |0.02                                       |0.0                                |0.0                                        |0.0                                      |0.0                                    |0.0                                    |0.0           |0.0                                    |0.0                                    |0.0                                              |0.0                               |0.0                                            |0.0                                       |0.0                                    |\n",
      "+--------------------+--------------------------------------+-----------------------------------------------+-------------------------------------+----------------------------------------------+-----------------------------------------+------------------------------------+-------------------------------------------+-------------------------------------------+-----------------------------------+-------------------------------------------+-----------------------------------------+---------------------------------------+---------------------------------------+--------------+---------------------------------------+---------------------------------------+-------------------------------------------------+----------------------------------+-----------------------------------------------+------------------------------------------+---------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample abiotic_features rows\n",
    "print('=== Sample abiotic_features rows ===')\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM nmdc_arkin.abiotic_features\n",
    "    LIMIT 3\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cell-26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"ts\": \"2026-02-25 17:49:52.888\", \"level\": \"ERROR\", \"logger\": \"SQLQueryContextLogger\", \"msg\": \"[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `a`.`annotations_tot_org_carb` cannot be resolved. Did you mean one of the following? [`a`.`annotations_ph`, `a`.`annotations_temp_has_numeric_value`, `a`.`annotations_depth_has_numeric_value`, `a`.`annotations_tot_org_carb_has_numeric_value`, `a`.`annotations_conduc_has_numeric_value`]. SQLSTATE: 42703; line 7 pos 13;\\n'Distinct\\n+- 'Project [sample_id#490, cast(annotations_ph#504 as double) AS ph#583, cast(annotations_temp_has_numeric_value#508 as double) AS temp_c#584, cast(annotations_depth_has_numeric_value#499 as double) AS depth_m#585, cast('a.annotations_tot_org_carb as double) AS tot_org_carb#586, cast('a.annotations_tot_nitro_content as double) AS tot_nitro#587, cast('a.annotations_water_content as double) AS water_content#588]\\n   +- Filter sample_id#490 IN (list#589 [])\\n      :  +- Intersect false\\n      :     :- Project [sample_id#246]\\n      :     :  +- Join Inner, (file_id#235 = file_id#248)\\n      :     :     :- SubqueryAlias b2\\n      :     :     :  +- SubqueryAlias spark_catalog.nmdc_arkin.omics_files_table\\n      :     :     :     +- Relation spark_catalog.nmdc_arkin.omics_files_table[file_id#235,file_name#236,file_url#237,file_size_bytes#238,file_type#239,file_type_description#240,md5_checksum#241,omics_processing_id#242,omics_processing_name#243,workflow_id#244,workflow_type#245,sample_id#246,study_id#247] parquet\\n      :     :     +- SubqueryAlias c2\\n      :     :        +- Distinct\\n      :     :           +- Project [file_id#248]\\n      :     :              +- SubqueryAlias spark_catalog.nmdc_arkin.centrifuge_gold\\n      :     :                 +- Relation spark_catalog.nmdc_arkin.centrifuge_gold[file_id#248,file_name#249,rank#250,taxid#251L,taxid_lineage#252,lineage#253,label#254,numReads#255L,abundance_raw#256,species_count#257L,abundance#258,abundance_clr#259,rn#260L] parquet\\n      :     +- Project [sample_id#601]\\n      :        +- Join Inner, (file_id#590 = file_id#89)\\n      :           :- SubqueryAlias b3\\n      :           :  +- SubqueryAlias spark_catalog.nmdc_arkin.omics_files_table\\n      :           :     +- Relation spark_catalog.nmdc_arkin.omics_files_table[file_id#590,file_name#591,file_url#592,file_size_bytes#593,file_type#594,file_type_description#595,md5_checksum#596,omics_processing_id#597,omics_processing_name#598,workflow_id#599,workflow_type#600,sample_id#601,study_id#602] parquet\\n      :           +- SubqueryAlias m3\\n      :              +- Distinct\\n      :                 +- Project [file_id#89]\\n      :                    +- SubqueryAlias spark_catalog.nmdc_arkin.metabolomics_gold\\n      :                       +- Relation spark_catalog.nmdc_arkin.metabolomics_gold[file_id#89,file_name#90,feature_id#91,Apex Scan Number#92,Area#93,Associated Mass Features after Deconvolution#94,Calculated m/z#95,Confidence Score#96,Dispersity Index#97,Entropy Similarity#98,Intensity#99,Ion Formula#100,Ion Type#101,Is Largest Ion after Deconvolution#102,Isotopologue Similarity#103,Isotopologue Type#104,Library mzs in Query (fraction)#105,MS2 Spectrum#106,Mass Feature ID#107L,Molecular Formula#108,Monoisotopic Mass Feature ID#109,Persistence#110,Polarity#111,Retention Time (min)#112,Sample Name#113,... 38 more fields] parquet\\n      +- SubqueryAlias a\\n         +- SubqueryAlias spark_catalog.nmdc_arkin.abiotic_features\\n            +- Relation spark_catalog.nmdc_arkin.abiotic_features[sample_id#490,annotations_ammonium_has_numeric_value#491,annotations_ammonium_nitrogen_has_numeric_value#492,annotations_calcium_has_numeric_value#493,annotations_carb_nitro_ratio_has_numeric_value#494,annotations_chlorophyll_has_numeric_value#495,annotations_conduc_has_numeric_value#496,annotations_depth_has_maximum_numeric_value#497,annotations_depth_has_minimum_numeric_value#498,annotations_depth_has_numeric_value#499,annotations_diss_org_carb_has_numeric_value#500,annotations_diss_oxygen_has_numeric_value#501,annotations_magnesium_has_numeric_value#502,annotations_manganese_has_numeric_value#503,annotations_ph#504,annotations_potassium_has_numeric_value#505,annotations_samp_size_has_numeric_value#506,annotations_soluble_react_phosp_has_numeric_value#507,annotations_temp_has_numeric_value#508,annotations_tot_nitro_content_has_numeric_value#509,annotations_tot_org_carb_has_numeric_value#510,annotations_tot_phosp_has_numeric_value#511] parquet\\n\\n\\nJVM stacktrace:\\norg.apache.spark.sql.catalyst.ExtendedAnalysisException\\n\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:401)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:169)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:404)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:402)\\n\\tat scala.collection.immutable.List.foreach(List.scala:334)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:299)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:299)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\n\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\n\\tat scala.util.Try$.apply(Try.scala:217)\\n\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\n\\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\\n\\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\\n\\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$5(Dataset.scala:139)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:136)\\n\\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$4(SparkSession.scala:499)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:490)\\n\\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:2764)\\n\\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:2608)\\n\\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:2499)\\n\\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:322)\\n\\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:224)\\n\\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:196)\\n\\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:341)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:341)\\n\\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\\n\\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\\n\\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:186)\\n\\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:102)\\n\\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\\n\\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:340)\\n\\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:196)\\n\\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:125)\\n\\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:347)\", \"context\": {\"errorClass\": \"UNRESOLVED_COLUMN.WITH_SUGGESTION\"}, \"exception\": {\"class\": \"_MultiThreadedRendezvous\", \"msg\": \"<_MultiThreadedRendezvous of RPC that terminated with:\\n\\tstatus = StatusCode.INTERNAL\\n\\tdetails = \\\"[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `a`.`annotations_tot_org_carb` cannot be resolved. Did you mean one of the following? [`a`.`annotations_ph`, `a`.`annotations_temp_has_numeric_value`, `a`.`annotations_depth_has_numeric_value`, `a`.`annotations_tot_org_carb_has_numeric_value`, `a`.`annotations_conduc_has_numeric_value`]. SQLSTATE: 42703; line 7 pos 13;\\n'Distinct\\n+- 'Project [sample_id#490, cast(annotations_ph#504 as double) AS ph#583, cast(annotations_temp_has_numeric_value#508 as double) AS temp_c#584, cast(annotations_depth_has_numeric_value#499 as double) AS depth_m#585, cast('a.annotations_tot_org_carb as double) AS tot_org_carb#586, cast('a.annotations_tot_nitro_content as double) AS tot_nitro#587, cast('a.annotations_water_content as double) AS water_content#588]\\n   +- Filter sample_id#490 IN (list#589 [])\\n      :  +- Intersect false\\n      :     :- Project [sample_id#246]\\n      :     :  +- Join Inner, (file_id#235 = file_id#248)\\n      :     :     :- SubqueryAlias b2\\n      :     :     :  +- SubqueryAlias spark_catalog.nmdc_arkin.omics_files_table\\n      :     :     :     +- Relation spark_catalog.nmdc_arkin.omics_files_table[file_id#235,file_name#236,file_url#237,file_size_bytes#238,file_type#239,file_type_description#240,md5_checksum#241,omics_processing_id#242,omics_processing_name#243,workflow_id#244,workflow_type#245,sample_id#246,study_id#247] parquet\\n      :     :     +- SubqueryAlias c2\\n      :     :        +- Distinct\\n      :     :           +- Project [file_id#248]\\n      :     :              +- SubqueryAlias spark_catalog.nmdc_arkin.centrifuge_gold\\n      :     :                 +- Relation spark_catalog.nmdc_arkin.centrifuge_gold[file_id#248,file_name#249,rank#250,taxid#251L,taxid_lineage#252,lineage#253,label#254,numReads#255L,abundance_raw#256,species_count#257L,abundance#258,abundance_clr#259,rn#260L] parquet\\n      :     +- Project [sample_id#601]\\n      :        +- Join Inner, (file_id#590 = file_id#89)\\n      :           :- SubqueryAlias b3\\n...\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {grpc_status:13, grpc_message:\\\"[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `a`.`annotations_tot_org_carb` cannot be resolved. Did you mean one of the following? [`a`.`annotations_ph`, `a`.`annotations_temp_has_numeric_value`, `a`.`annotations_depth_has_numeric_value`, `a`.`annotations_tot_org_carb_has_numeric_value`, `a`.`annotations_conduc_has_numeric_value`]. SQLSTATE: 42703; line 7 pos 13;\\\\n\\\\'Distinct\\\\n+- \\\\'Project [sample_id#490, cast(annotations_ph#504 as double) AS ph#583, cast(annotations_temp_has_numeric_value#508 as double) AS temp_c#584, cast(annotations_depth_has_numeric_value#499 as double) AS depth_m#585, cast(\\\\'a.annotations_tot_org_carb as double) AS tot_org_carb#586, cast(\\\\'a.annotations_tot_nitro_content as double) AS tot_nitro#587, cast(\\\\'a.annotations_water_content as double) AS water_content#588]\\\\n   +- Filter sample_id#490 IN (list#589 [])\\\\n      :  +- Intersect false\\\\n      :     :- Project [sample_id#246]\\\\n      :     :  +- Join Inner, (file_id#235 = file_id#248)\\\\n      :     :     :- SubqueryAlias b2\\\\n      :     :     :  +- SubqueryAlias spark_catalog.nmdc_arkin.omics_files_table\\\\n      :     :     :     +- Relation spark_catalog.nmdc_arkin.omics_files_table[file_id#235,file_name#236,file_url#237,file_size_bytes#238,file_type#239,file_type_description#240,md5_checksum#241,omics_processing_id#242,omics_processing_name#243,workflow_id#244,workflow_type#245,sample_id#246,study_id#247] parquet\\\\n      :     :     +- SubqueryAlias c2\\\\n      :     :        +- Distinct\\\\n      :     :           +- Project [file_id#248]\\\\n      :     :              +- SubqueryAlias spark_catalog.nmdc_arkin.centrifuge_gold\\\\n      :     :                 +- Relation spark_catalog.nmdc_arkin.centrifuge_gold[file_id#248,file_name#249,rank#250,taxid#251L,taxid_lineage#252,lineage#253,label#254,numReads#255L,abundance_raw#256,species_count#257L,abundance#258,abundance_clr#259,rn#260L] parquet\\\\n      :     +- Project [sample_id#601]\\\\n      :        +- Join Inner, (file_id#590 = file_id#89)\\\\n      :           :- SubqueryAlias b3\\\\n...\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"_execute_and_fetch_as_iterator\", \"file\": \"/usr/local/spark/python/pyspark/sql/connect/client/core.py\", \"line\": \"1523\"}, {\"class\": null, \"method\": \"__next__\", \"file\": \"<frozen _collections_abc>\", \"line\": \"360\"}, {\"class\": null, \"method\": \"send\", \"file\": \"/usr/local/spark/python/pyspark/sql/connect/client/reattach.py\", \"line\": \"138\"}, {\"class\": null, \"method\": \"_has_next\", \"file\": \"/usr/local/spark/python/pyspark/sql/connect/client/reattach.py\", \"line\": \"190\"}, {\"class\": null, \"method\": \"_has_next\", \"file\": \"/usr/local/spark/python/pyspark/sql/connect/client/reattach.py\", \"line\": \"162\"}, {\"class\": null, \"method\": \"_call_iter\", \"file\": \"/usr/local/spark/python/pyspark/sql/connect/client/reattach.py\", \"line\": \"281\"}, {\"class\": null, \"method\": \"_call_iter\", \"file\": \"/usr/local/spark/python/pyspark/sql/connect/client/reattach.py\", \"line\": \"261\"}, {\"class\": null, \"method\": \"<lambda>\", \"file\": \"/usr/local/spark/python/pyspark/sql/connect/client/reattach.py\", \"line\": \"163\"}, {\"class\": null, \"method\": \"__next__\", \"file\": \"/opt/conda/lib/python3.13/site-packages/grpc/_channel.py\", \"line\": \"538\"}, {\"class\": null, \"method\": \"_next\", \"file\": \"/opt/conda/lib/python3.13/site-packages/grpc/_channel.py\", \"line\": \"956\"}]}}\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `a`.`annotations_tot_org_carb` cannot be resolved. Did you mean one of the following? [`a`.`annotations_ph`, `a`.`annotations_temp_has_numeric_value`, `a`.`annotations_depth_has_numeric_value`, `a`.`annotations_tot_org_carb_has_numeric_value`, `a`.`annotations_conduc_has_numeric_value`]. SQLSTATE: 42703; line 7 pos 13;\n'Distinct\n+- 'Project [sample_id#490, cast(annotations_ph#504 as double) AS ph#583, cast(annotations_temp_has_numeric_value#508 as double) AS temp_c#584, cast(annotations_depth_has_numeric_value#499 as double) AS depth_m#585, cast('a.annotations_tot_org_carb as double) AS tot_org_carb#586, cast('a.annotations_tot_nitro_content as double) AS tot_nitro#587, cast('a.annotations_water_content as double) AS water_content#588]\n   +- Filter sample_id#490 IN (list#589 [])\n      :  +- Intersect false\n      :     :- Project [sample_id#246]\n      :     :  +- Join Inner, (file_id#235 = file_id#248)\n      :     :     :- SubqueryAlias b2\n      :     :     :  +- SubqueryAlias spark_catalog.nmdc_arkin.omics_files_table\n      :     :     :     +- Relation spark_catalog.nmdc_arkin.omics_files_table[file_id#235,file_name#236,file_url#237,file_size_bytes#238,file_type#239,file_type_description#240,md5_checksum#241,omics_processing_id#242,omics_processing_name#243,workflow_id#244,workflow_type#245,sample_id#246,study_id#247] parquet\n      :     :     +- SubqueryAlias c2\n      :     :        +- Distinct\n      :     :           +- Project [file_id#248]\n      :     :              +- SubqueryAlias spark_catalog.nmdc_arkin.centrifuge_gold\n      :     :                 +- Relation spark_catalog.nmdc_arkin.centrifuge_gold[file_id#248,file_name#249,rank#250,taxid#251L,taxid_lineage#252,lineage#253,label#254,numReads#255L,abundance_raw#256,species_count#257L,abundance#258,abundance_clr#259,rn#260L] parquet\n      :     +- Project [sample_id#601]\n      :        +- Join Inner, (file_id#590 = file_id#89)\n      :           :- SubqueryAlias b3\n      :           :  +- SubqueryAlias spark_catalog.nmdc_arkin.omics_files_table\n      :           :     +- Relation spark_catalog.nmdc_arkin.omics_files_table[file_id#590,file_name#591,file_url#592,file_size_bytes#593,file_type#594,file_type_description#595,md5_checksum#596,omics_processing_id#597,omics_processing_name#598,workflow_id#599,workflow_type#600,sample_id#601,study_id#602] parquet\n      :           +- SubqueryAlias m3\n      :              +- Distinct\n      :                 +- Project [file_id#89]\n      :                    +- SubqueryAlias spark_catalog.nmdc_arkin.metabolomics_gold\n      :                       +- Relation spark_catalog.nmdc_arkin.metabolomics_gold[file_id#89,file_name#90,feature_id#91,Apex Scan Number#92,Area#93,Associated Mass Features after Deconvolution#94,Calculated m/z#95,Confidence Score#96,Dispersity Index#97,Entropy Similarity#98,Intensity#99,Ion Formula#100,Ion Type#101,Is Largest Ion after Deconvolution#102,Isotopologue Similarity#103,Isotopologue Type#104,Library mzs in Query (fraction)#105,MS2 Spectrum#106,Mass Feature ID#107L,Molecular Formula#108,Monoisotopic Mass Feature ID#109,Persistence#110,Polarity#111,Retention Time (min)#112,Sample Name#113,... 38 more fields] parquet\n      +- SubqueryAlias a\n         +- SubqueryAlias spark_catalog.nmdc_arkin.abiotic_features\n            +- Relation spark_catalog.nmdc_arkin.abiotic_features[sample_id#490,annotations_ammonium_has_numeric_value#491,annotations_ammonium_nitrogen_has_numeric_value#492,annotations_calcium_has_numeric_value#493,annotations_carb_nitro_ratio_has_numeric_value#494,annotations_chlorophyll_has_numeric_value#495,annotations_conduc_has_numeric_value#496,annotations_depth_has_maximum_numeric_value#497,annotations_depth_has_minimum_numeric_value#498,annotations_depth_has_numeric_value#499,annotations_diss_org_carb_has_numeric_value#500,annotations_diss_oxygen_has_numeric_value#501,annotations_magnesium_has_numeric_value#502,annotations_manganese_has_numeric_value#503,annotations_ph#504,annotations_potassium_has_numeric_value#505,annotations_samp_size_has_numeric_value#506,annotations_soluble_react_phosp_has_numeric_value#507,annotations_temp_has_numeric_value#508,annotations_tot_nitro_content_has_numeric_value#509,annotations_tot_org_carb_has_numeric_value#510,annotations_tot_phosp_has_numeric_value#511] parquet\n\n\nJVM stacktrace:\norg.apache.spark.sql.catalyst.ExtendedAnalysisException\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:401)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:169)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:404)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:402)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:402)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:402)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:402)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:299)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:299)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$5(Dataset.scala:139)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:136)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$4(SparkSession.scala:499)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:490)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:2764)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:2608)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:2499)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:322)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:224)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:196)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:341)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:341)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:186)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:102)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:340)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:196)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:125)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:347)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Extract abiotic features for overlap samples.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Note: all columns in abiotic_features are string-typed — CAST to DOUBLE before use.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Join via omics_files_table (sample_id key).\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Use INTERSECT subquery to restrict to overlap samples.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m abiotic_spark = \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\"\"\u001b[39;49m\n\u001b[32m      6\u001b[39m \u001b[33;43m    SELECT DISTINCT\u001b[39;49m\n\u001b[32m      7\u001b[39m \u001b[33;43m        a.sample_id,\u001b[39;49m\n\u001b[32m      8\u001b[39m \u001b[33;43m        CAST(a.annotations_ph AS DOUBLE)                            AS ph,\u001b[39;49m\n\u001b[32m      9\u001b[39m \u001b[33;43m        CAST(a.annotations_temp_has_numeric_value AS DOUBLE)        AS temp_c,\u001b[39;49m\n\u001b[32m     10\u001b[39m \u001b[33;43m        CAST(a.annotations_depth_has_numeric_value AS DOUBLE)       AS depth_m,\u001b[39;49m\n\u001b[32m     11\u001b[39m \u001b[33;43m        CAST(a.annotations_tot_org_carb AS DOUBLE)                  AS tot_org_carb,\u001b[39;49m\n\u001b[32m     12\u001b[39m \u001b[33;43m        CAST(a.annotations_tot_nitro_content AS DOUBLE)             AS tot_nitro,\u001b[39;49m\n\u001b[32m     13\u001b[39m \u001b[33;43m        CAST(a.annotations_water_content AS DOUBLE)                 AS water_content\u001b[39;49m\n\u001b[32m     14\u001b[39m \u001b[33;43m    FROM nmdc_arkin.abiotic_features a\u001b[39;49m\n\u001b[32m     15\u001b[39m \u001b[33;43m    WHERE a.sample_id IN (\u001b[39;49m\n\u001b[32m     16\u001b[39m \u001b[33;43m        SELECT b2.sample_id FROM \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mBRIDGE_TBL\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m b2\u001b[39;49m\n\u001b[32m     17\u001b[39m \u001b[33;43m        JOIN (SELECT DISTINCT file_id FROM nmdc_arkin.centrifuge_gold) c2\u001b[39;49m\n\u001b[32m     18\u001b[39m \u001b[33;43m          ON b2.file_id = c2.file_id\u001b[39;49m\n\u001b[32m     19\u001b[39m \u001b[33;43m        INTERSECT\u001b[39;49m\n\u001b[32m     20\u001b[39m \u001b[33;43m        SELECT b3.sample_id FROM \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mBRIDGE_TBL\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m b3\u001b[39;49m\n\u001b[32m     21\u001b[39m \u001b[33;43m        JOIN (SELECT DISTINCT file_id FROM nmdc_arkin.metabolomics_gold) m3\u001b[39;49m\n\u001b[32m     22\u001b[39m \u001b[33;43m          ON b3.file_id = m3.file_id\u001b[39;49m\n\u001b[32m     23\u001b[39m \u001b[33;43m    )\u001b[39;49m\n\u001b[32m     24\u001b[39m \u001b[33;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m abiotic = abiotic_spark.toPandas()\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Defensive float cast (Spark CAST still returns Decimal in some versions)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/spark/python/pyspark/sql/connect/session.py:769\u001b[39m, in \u001b[36mSparkSession.sql\u001b[39m\u001b[34m(self, sqlQuery, args, **kwargs)\u001b[39m\n\u001b[32m    766\u001b[39m         _views.append(SubqueryAlias(df._plan, name))\n\u001b[32m    768\u001b[39m cmd = SQL(sqlQuery, _args, _named_args, _views)\n\u001b[32m--> \u001b[39m\u001b[32m769\u001b[39m data, properties, ei = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    770\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33msql_command_result\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m properties:\n\u001b[32m    771\u001b[39m     df = DataFrame(CachedRelation(properties[\u001b[33m\"\u001b[39m\u001b[33msql_command_result\u001b[39m\u001b[33m\"\u001b[39m]), \u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/spark/python/pyspark/sql/connect/client/core.py:1148\u001b[39m, in \u001b[36mSparkConnectClient.execute_command\u001b[39m\u001b[34m(self, command, observations)\u001b[39m\n\u001b[32m   1146\u001b[39m     req.user_context.user_id = \u001b[38;5;28mself\u001b[39m._user_id\n\u001b[32m   1147\u001b[39m req.plan.command.CopyFrom(command)\n\u001b[32m-> \u001b[39m\u001b[32m1148\u001b[39m data, _, metrics, observed_metrics, properties = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_and_fetch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[38;5;66;03m# Create a query execution object.\u001b[39;00m\n\u001b[32m   1152\u001b[39m ei = ExecutionInfo(metrics, observed_metrics)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/spark/python/pyspark/sql/connect/client/core.py:1560\u001b[39m, in \u001b[36mSparkConnectClient._execute_and_fetch\u001b[39m\u001b[34m(self, req, observations, self_destruct)\u001b[39m\n\u001b[32m   1557\u001b[39m properties: Dict[\u001b[38;5;28mstr\u001b[39m, Any] = {}\n\u001b[32m   1559\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Progress(handlers=\u001b[38;5;28mself\u001b[39m._progress_handlers, operation_id=req.operation_id) \u001b[38;5;28;01mas\u001b[39;00m progress:\n\u001b[32m-> \u001b[39m\u001b[32m1560\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_and_fetch_as_iterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1561\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress\u001b[49m\n\u001b[32m   1562\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1563\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mStructType\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1564\u001b[39m \u001b[43m            \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/spark/python/pyspark/sql/connect/client/core.py:1537\u001b[39m, in \u001b[36mSparkConnectClient._execute_and_fetch_as_iterator\u001b[39m\u001b[34m(self, req, observations, progress)\u001b[39m\n\u001b[32m   1535\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m kb\n\u001b[32m   1536\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[32m-> \u001b[39m\u001b[32m1537\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/spark/python/pyspark/sql/connect/client/core.py:1811\u001b[39m, in \u001b[36mSparkConnectClient._handle_error\u001b[39m\u001b[34m(self, error)\u001b[39m\n\u001b[32m   1809\u001b[39m     \u001b[38;5;28mself\u001b[39m.thread_local.inside_error_handling = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1810\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, grpc.RpcError):\n\u001b[32m-> \u001b[39m\u001b[32m1811\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_rpc_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[32m   1813\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/spark/python/pyspark/sql/connect/client/core.py:1882\u001b[39m, in \u001b[36mSparkConnectClient._handle_rpc_error\u001b[39m\u001b[34m(self, rpc_error)\u001b[39m\n\u001b[32m   1879\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m info.metadata[\u001b[33m\"\u001b[39m\u001b[33merrorClass\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mINVALID_HANDLE.SESSION_CHANGED\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1880\u001b[39m                 \u001b[38;5;28mself\u001b[39m._closed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1882\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m convert_exception(\n\u001b[32m   1883\u001b[39m                 info,\n\u001b[32m   1884\u001b[39m                 status.message,\n\u001b[32m   1885\u001b[39m                 \u001b[38;5;28mself\u001b[39m._fetch_enriched_error(info),\n\u001b[32m   1886\u001b[39m                 \u001b[38;5;28mself\u001b[39m._display_server_stack_trace(),\n\u001b[32m   1887\u001b[39m             ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1889\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m SparkConnectGrpcException(status.message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mAnalysisException\u001b[39m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `a`.`annotations_tot_org_carb` cannot be resolved. Did you mean one of the following? [`a`.`annotations_ph`, `a`.`annotations_temp_has_numeric_value`, `a`.`annotations_depth_has_numeric_value`, `a`.`annotations_tot_org_carb_has_numeric_value`, `a`.`annotations_conduc_has_numeric_value`]. SQLSTATE: 42703; line 7 pos 13;\n'Distinct\n+- 'Project [sample_id#490, cast(annotations_ph#504 as double) AS ph#583, cast(annotations_temp_has_numeric_value#508 as double) AS temp_c#584, cast(annotations_depth_has_numeric_value#499 as double) AS depth_m#585, cast('a.annotations_tot_org_carb as double) AS tot_org_carb#586, cast('a.annotations_tot_nitro_content as double) AS tot_nitro#587, cast('a.annotations_water_content as double) AS water_content#588]\n   +- Filter sample_id#490 IN (list#589 [])\n      :  +- Intersect false\n      :     :- Project [sample_id#246]\n      :     :  +- Join Inner, (file_id#235 = file_id#248)\n      :     :     :- SubqueryAlias b2\n      :     :     :  +- SubqueryAlias spark_catalog.nmdc_arkin.omics_files_table\n      :     :     :     +- Relation spark_catalog.nmdc_arkin.omics_files_table[file_id#235,file_name#236,file_url#237,file_size_bytes#238,file_type#239,file_type_description#240,md5_checksum#241,omics_processing_id#242,omics_processing_name#243,workflow_id#244,workflow_type#245,sample_id#246,study_id#247] parquet\n      :     :     +- SubqueryAlias c2\n      :     :        +- Distinct\n      :     :           +- Project [file_id#248]\n      :     :              +- SubqueryAlias spark_catalog.nmdc_arkin.centrifuge_gold\n      :     :                 +- Relation spark_catalog.nmdc_arkin.centrifuge_gold[file_id#248,file_name#249,rank#250,taxid#251L,taxid_lineage#252,lineage#253,label#254,numReads#255L,abundance_raw#256,species_count#257L,abundance#258,abundance_clr#259,rn#260L] parquet\n      :     +- Project [sample_id#601]\n      :        +- Join Inner, (file_id#590 = file_id#89)\n      :           :- SubqueryAlias b3\n      :           :  +- SubqueryAlias spark_catalog.nmdc_arkin.omics_files_table\n      :           :     +- Relation spark_catalog.nmdc_arkin.omics_files_table[file_id#590,file_name#591,file_url#592,file_size_bytes#593,file_type#594,file_type_description#595,md5_checksum#596,omics_processing_id#597,omics_processing_name#598,workflow_id#599,workflow_type#600,sample_id#601,study_id#602] parquet\n      :           +- SubqueryAlias m3\n      :              +- Distinct\n      :                 +- Project [file_id#89]\n      :                    +- SubqueryAlias spark_catalog.nmdc_arkin.metabolomics_gold\n      :                       +- Relation spark_catalog.nmdc_arkin.metabolomics_gold[file_id#89,file_name#90,feature_id#91,Apex Scan Number#92,Area#93,Associated Mass Features after Deconvolution#94,Calculated m/z#95,Confidence Score#96,Dispersity Index#97,Entropy Similarity#98,Intensity#99,Ion Formula#100,Ion Type#101,Is Largest Ion after Deconvolution#102,Isotopologue Similarity#103,Isotopologue Type#104,Library mzs in Query (fraction)#105,MS2 Spectrum#106,Mass Feature ID#107L,Molecular Formula#108,Monoisotopic Mass Feature ID#109,Persistence#110,Polarity#111,Retention Time (min)#112,Sample Name#113,... 38 more fields] parquet\n      +- SubqueryAlias a\n         +- SubqueryAlias spark_catalog.nmdc_arkin.abiotic_features\n            +- Relation spark_catalog.nmdc_arkin.abiotic_features[sample_id#490,annotations_ammonium_has_numeric_value#491,annotations_ammonium_nitrogen_has_numeric_value#492,annotations_calcium_has_numeric_value#493,annotations_carb_nitro_ratio_has_numeric_value#494,annotations_chlorophyll_has_numeric_value#495,annotations_conduc_has_numeric_value#496,annotations_depth_has_maximum_numeric_value#497,annotations_depth_has_minimum_numeric_value#498,annotations_depth_has_numeric_value#499,annotations_diss_org_carb_has_numeric_value#500,annotations_diss_oxygen_has_numeric_value#501,annotations_magnesium_has_numeric_value#502,annotations_manganese_has_numeric_value#503,annotations_ph#504,annotations_potassium_has_numeric_value#505,annotations_samp_size_has_numeric_value#506,annotations_soluble_react_phosp_has_numeric_value#507,annotations_temp_has_numeric_value#508,annotations_tot_nitro_content_has_numeric_value#509,annotations_tot_org_carb_has_numeric_value#510,annotations_tot_phosp_has_numeric_value#511] parquet\n\n\nJVM stacktrace:\norg.apache.spark.sql.catalyst.ExtendedAnalysisException\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:401)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:169)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:404)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:402)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:402)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:402)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:402)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:299)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:299)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$5(Dataset.scala:139)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:136)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$4(SparkSession.scala:499)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:490)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:2764)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:2608)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:2499)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:322)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:224)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:196)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:341)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:341)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:186)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:102)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:340)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:196)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:125)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:347)"
     ]
    }
   ],
   "source": [
    "# Extract abiotic features for overlap samples.\n",
    "# Note: all columns in abiotic_features are string-typed — CAST to DOUBLE before use.\n",
    "# Join via omics_files_table (sample_id key).\n",
    "# Use INTERSECT subquery to restrict to overlap samples.\n",
    "abiotic_spark = spark.sql(f\"\"\"\n",
    "    SELECT DISTINCT\n",
    "        a.sample_id,\n",
    "        CAST(a.annotations_ph AS DOUBLE)                            AS ph,\n",
    "        CAST(a.annotations_temp_has_numeric_value AS DOUBLE)        AS temp_c,\n",
    "        CAST(a.annotations_depth_has_numeric_value AS DOUBLE)       AS depth_m,\n",
    "        CAST(a.annotations_tot_org_carb AS DOUBLE)                  AS tot_org_carb,\n",
    "        CAST(a.annotations_tot_nitro_content AS DOUBLE)             AS tot_nitro,\n",
    "        CAST(a.annotations_water_content AS DOUBLE)                 AS water_content\n",
    "    FROM nmdc_arkin.abiotic_features a\n",
    "    WHERE a.sample_id IN (\n",
    "        SELECT b2.sample_id FROM {BRIDGE_TBL} b2\n",
    "        JOIN (SELECT DISTINCT file_id FROM nmdc_arkin.centrifuge_gold) c2\n",
    "          ON b2.file_id = c2.file_id\n",
    "        INTERSECT\n",
    "        SELECT b3.sample_id FROM {BRIDGE_TBL} b3\n",
    "        JOIN (SELECT DISTINCT file_id FROM nmdc_arkin.metabolomics_gold) m3\n",
    "          ON b3.file_id = m3.file_id\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "abiotic = abiotic_spark.toPandas()\n",
    "# Defensive float cast (Spark CAST still returns Decimal in some versions)\n",
    "for col in ['ph', 'temp_c', 'depth_m', 'tot_org_carb', 'tot_nitro', 'water_content']:\n",
    "    if col in abiotic.columns:\n",
    "        abiotic[col] = pd.to_numeric(abiotic[col], errors='coerce')\n",
    "\n",
    "print(f'Abiotic features rows: {len(abiotic)}')\n",
    "print(f'Unique samples:        {abiotic[\"sample_id\"].nunique()}')\n",
    "print()\n",
    "print('Non-null rates per feature:')\n",
    "print(abiotic.notna().mean().to_string())\n",
    "print()\n",
    "print(abiotic.describe().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": "# Extract abiotic features for overlap samples.\n# All numeric columns in abiotic_features are already double-typed.\n# Column naming: most use _has_numeric_value suffix; annotations_ph is the exception.\n# Replace annotations_water_content (doesn't exist) with diss_org_carb and conduc.\n# Join via sample_id directly (abiotic_features has sample_id as primary key).\nabiotic_spark = spark.sql(f\"\"\"\n    SELECT DISTINCT\n        a.sample_id,\n        a.annotations_ph                                    AS ph,\n        a.annotations_temp_has_numeric_value                AS temp_c,\n        a.annotations_depth_has_numeric_value               AS depth_m,\n        a.annotations_tot_org_carb_has_numeric_value        AS tot_org_carb,\n        a.annotations_tot_nitro_content_has_numeric_value   AS tot_nitro,\n        a.annotations_diss_org_carb_has_numeric_value       AS diss_org_carb,\n        a.annotations_conduc_has_numeric_value              AS conductance\n    FROM nmdc_arkin.abiotic_features a\n    WHERE a.sample_id IN (\n        SELECT b2.sample_id FROM {BRIDGE_TBL} b2\n        JOIN (SELECT DISTINCT file_id FROM nmdc_arkin.centrifuge_gold) c2\n          ON b2.file_id = c2.file_id\n        INTERSECT\n        SELECT b3.sample_id FROM {BRIDGE_TBL} b3\n        JOIN (SELECT DISTINCT file_id FROM nmdc_arkin.metabolomics_gold) m3\n          ON b3.file_id = m3.file_id\n    )\n\"\"\")\n\nabiotic = abiotic_spark.toPandas()\nabiotic_num_cols = ['ph', 'temp_c', 'depth_m', 'tot_org_carb',\n                    'tot_nitro', 'diss_org_carb', 'conductance']\nfor col in abiotic_num_cols:\n    abiotic[col] = pd.to_numeric(abiotic[col], errors='coerce')\n\n# Treat 0.0 as missing (abiotic_features stores 0 for unmeasured variables)\nfor col in abiotic_num_cols:\n    abiotic[col] = abiotic[col].replace(0.0, np.nan)\n\nprint(f'Abiotic features rows: {len(abiotic)}')\nprint(f'Unique samples:        {abiotic[\"sample_id\"].nunique()}')\nprint()\nprint('Non-null rates per feature (after replacing 0 → NaN):')\nprint(abiotic[abiotic_num_cols].notna().mean().round(3).to_string())\nprint()\nprint(abiotic[abiotic_num_cols].describe().to_string())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": "# Start from community_matrix (220 samples × 86 cols)\n# Inner join with metabolomics wide matrix\nanalysis_matrix = community_matrix.merge(met_wide, on='sample_id', how='inner')\nprint(f'After joining pathway + metabolomics: {analysis_matrix.shape}')\nprint(f'  Samples: {len(analysis_matrix)}')\n\n# Left join abiotic features\nabiotic_num_cols = ['ph', 'temp_c', 'depth_m', 'tot_org_carb',\n                    'tot_nitro', 'diss_org_carb', 'conductance']\nif len(abiotic) > 0 and abiotic['sample_id'].nunique() > 0:\n    analysis_matrix = analysis_matrix.merge(\n        abiotic.drop_duplicates(subset='sample_id'),\n        on='sample_id', how='left'\n    )\n    print(f'After joining abiotic features: {analysis_matrix.shape}')\n    non_null = {c: int(analysis_matrix[c].notna().sum()) for c in abiotic_num_cols\n                if c in analysis_matrix.columns}\n    print(f'  Samples with abiotic data: {non_null}')\nelse:\n    print('No abiotic features found for overlap samples — skipping abiotic join')\n    for col in abiotic_num_cols:\n        analysis_matrix[col] = np.nan\n\nprint(f'\\nFinal analysis matrix shape: {analysis_matrix.shape}')\nprint(f'Samples: {analysis_matrix[\"sample_id\"].nunique()}')"
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 9: Metabolomics Distribution Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: log-intensity distributions for amino acid compounds, colored by ecosystem type\n",
    "# Merge ecosystem type into aa_by_pathway\n",
    "eco_map = community_matrix[['sample_id', 'ecosystem_type']].drop_duplicates()\n",
    "aa_plot = aa_by_pathway.merge(eco_map, on='sample_id', how='left')\n",
    "aa_plot['ecosystem_type'] = aa_plot['ecosystem_type'].fillna('Unknown')\n",
    "\n",
    "# Filter to aa_pathways with detections in at least 20 samples\n",
    "well_detected = aa_coverage[aa_coverage['n_samples_detected'] >= 20]['aa_pathway'].tolist()\n",
    "aa_plot_filtered = aa_plot[aa_plot['aa_pathway'].isin(well_detected)]\n",
    "\n",
    "n_pathways_plot = len(well_detected)\n",
    "if n_pathways_plot > 0:\n",
    "    fig, axes = plt.subplots(\n",
    "        n_pathways_plot, 1,\n",
    "        figsize=(8, max(6, n_pathways_plot * 1.5)),\n",
    "        sharex=False\n",
    "    )\n",
    "    if n_pathways_plot == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    palette = {'Soil': '#c8a96e', 'Freshwater': '#4e8fb5', 'Unknown': '#aaaaaa'}\n",
    "\n",
    "    for ax, pathway in zip(axes, sorted(well_detected)):\n",
    "        subset = aa_plot_filtered[aa_plot_filtered['aa_pathway'] == pathway]\n",
    "        for eco, grp in subset.groupby('ecosystem_type'):\n",
    "            color = palette.get(eco, '#888888')\n",
    "            ax.hist(grp['log_intensity'], bins=25, alpha=0.5,\n",
    "                    label=eco, color=color, density=True)\n",
    "        ax.set_title(f'aa pathway: {pathway}', fontsize=9)\n",
    "        ax.set_ylabel('Density', fontsize=8)\n",
    "        ax.tick_params(labelsize=7)\n",
    "        ax.legend(fontsize=7)\n",
    "\n",
    "    axes[-1].set_xlabel('log(intensity + 1)', fontsize=9)\n",
    "    plt.suptitle('Amino Acid Compound Intensities by Ecosystem Type',\n",
    "                 fontsize=11, y=1.01)\n",
    "    plt.tight_layout()\n",
    "    fig_path = os.path.join(FIGURES_DIR, 'metabolomics_distribution.png')\n",
    "    plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f'Saved: figures/metabolomics_distribution.png')\n",
    "else:\n",
    "    print('No aa pathways with sufficient detections for plotting')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 10: Save Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metabolomics matrix (wide: samples × compounds)\n",
    "met_path = os.path.join(DATA_DIR, 'metabolomics_matrix.csv')\n",
    "met_wide.to_csv(met_path, index=False)\n",
    "print(f'Saved: data/metabolomics_matrix.csv  ({met_wide.shape})')\n",
    "\n",
    "# Save amino acid metabolites (long: sample × pathway)\n",
    "aa_path = os.path.join(DATA_DIR, 'amino_acid_metabolites.csv')\n",
    "aa_by_pathway.to_csv(aa_path, index=False)\n",
    "print(f'Saved: data/amino_acid_metabolites.csv  ({aa_by_pathway.shape})')\n",
    "\n",
    "# Save analysis-ready matrix\n",
    "ar_path = os.path.join(DATA_DIR, 'analysis_ready_matrix.csv')\n",
    "analysis_matrix.to_csv(ar_path, index=False)\n",
    "print(f'Saved: data/analysis_ready_matrix.csv  ({analysis_matrix.shape})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-33",
   "metadata": {},
   "outputs": [],
   "source": "# NB04 Summary\nMETADATA_COLS = ['sample_id', 'study_id', 'ecosystem_category', 'ecosystem_type',\n                 'ecosystem_subtype', 'specific_ecosystem',\n                 'ph', 'temp_c', 'depth_m', 'tot_org_carb',\n                 'tot_nitro', 'diss_org_carb', 'conductance']\npathway_cols_cm  = [c for c in community_matrix.columns\n                    if c not in ['sample_id', 'study_id', 'ecosystem_category',\n                                 'ecosystem_type', 'ecosystem_subtype', 'specific_ecosystem']]\ncompound_cols_ar = [c for c in analysis_matrix.columns\n                    if c not in METADATA_COLS and c not in pathway_cols_cm]\n\nprint('=== NB04 Summary ===')\nprint(f'Samples in analysis_ready_matrix:  {analysis_matrix[\"sample_id\"].nunique()}')\nprint(f'Pathway columns (from NB03):        {len(pathway_cols_cm)}')\nprint(f'Compound columns (metabolomics):    {len(compound_cols_ar)}')\nabiotic_num_cols = ['ph', 'temp_c', 'depth_m', 'tot_org_carb',\n                    'tot_nitro', 'diss_org_carb', 'conductance']\nnon_null = {c: int(analysis_matrix[c].notna().sum())\n            for c in abiotic_num_cols if c in analysis_matrix.columns}\nprint(f'Abiotic feature non-null counts:    {non_null}')\nprint()\nprint('AA pathways with metabolomics coverage:')\nprint(aa_coverage.to_string(index=False))\nprint()\nprint('Ecosystem breakdown in analysis_ready_matrix:')\nprint(analysis_matrix['ecosystem_type'].value_counts(dropna=False).to_string())"
  },
  {
   "cell_type": "markdown",
   "id": "cell-34",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary and Decisions for NB05\n",
    "\n",
    "| Question | Finding |\n",
    "|---|---|\n",
    "| Samples in analysis_ready_matrix | ??? |\n",
    "| Metabolomics compounds (prevalent) | ??? |\n",
    "| AA pathways with metabolomics detections | ??? / 18 |\n",
    "| Samples with AA metabolomics coverage | ??? |\n",
    "| Abiotic features available | pH: ???, temp: ??? |\n",
    "| Ecosystem split | Soil: ???, Freshwater: ???, Unknown: ??? |\n",
    "\n",
    "**Decision for NB05**:  \n",
    "Proceed to statistical analysis using `data/analysis_ready_matrix.csv` +\n",
    "`data/amino_acid_metabolites.csv`.  \n",
    "For H1 (Black Queen): correlate per-sample mean AA pathway completeness  \n",
    "vs per-sample mean AA metabolite intensity.  \n",
    "For H2: UMAP/PCA of community pathway profiles, colored by ecosystem type."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}