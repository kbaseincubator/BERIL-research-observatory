{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# NB03: Community Pathway Completeness Matrix\n",
    "\n",
    "**Project**: Community Metabolic Ecology via NMDC × Pangenome Integration  \n",
    "**Requires**: BERDL JupyterHub (Spark — `get_spark_session()` injected into kernel)  \n",
    "\n",
    "## Purpose\n",
    "\n",
    "Compute community-weighted GapMind pathway completeness scores per NMDC sample.\n",
    "\n",
    "**Formula**:  \n",
    "`completeness(sample s, pathway p) = Σᵢ [ (aᵢ / Σⱼ aⱼ_mapped) × frac_complete(taxon_i, p) ]`  \n",
    "where aᵢ is centrifuge abundance for taxon i, and the denominator normalizes over\n",
    "mapped taxa only, so weights sum to 1 per sample per pathway.\n",
    "\n",
    "## Inputs (from NB01/NB02)\n",
    "\n",
    "- `data/taxon_bridge.tsv` — centrifuge taxon → GTDB species clade_id + mapping tier\n",
    "- `data/nmdc_sample_inventory.csv` — sample × clf_file_id × met_file_id\n",
    "- `data/bridge_quality.csv` — per-file bridge coverage and QC flag\n",
    "- `nmdc_arkin.centrifuge_gold` + `nmdc_arkin.omics_files_table` — species abundances\n",
    "- `kbase_ke_pangenome.gapmind_pathways` — 305M rows (filtered to bridged clades)\n",
    "- `kbase_ke_pangenome.gtdb_species_clade` — clade_id → GTDB_species bridge\n",
    "\n",
    "## Outputs\n",
    "\n",
    "- `data/species_pathway_completeness.csv` — per-GTDB-clade × per-pathway completeness\n",
    "- `data/community_pathway_matrix.csv` — per-sample × per-pathway community-weighted completeness\n",
    "- `figures/pathway_completeness_heatmap.png` — mean completeness by pathway × ecosystem type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.connect.session.SparkSession at 0x779cdbe156a0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On BERDL JupyterHub — get_spark_session() is injected into the kernel; no import needed\n",
    "spark = get_spark_session()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_DIR: /home/cjneely/repos/BERIL-research-observatory/projects/nmdc_community_metabolic_ecology/data\n",
      "FIGURES_DIR: /home/cjneely/repos/BERIL-research-observatory/projects/nmdc_community_metabolic_ecology/figures\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "PROJECT_DIR = os.path.abspath(os.path.join(os.path.dirname('__file__'), '..'))\n",
    "DATA_DIR = os.path.join(PROJECT_DIR, 'data')\n",
    "FIGURES_DIR = os.path.join(PROJECT_DIR, 'figures')\n",
    "\n",
    "print(f'DATA_DIR: {DATA_DIR}')\n",
    "print(f'FIGURES_DIR: {FIGURES_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Load NB02 Outputs\n",
    "\n",
    "Load cached bridge and inventory files from NB02. These are loaded from disk (CSV/TSV),\n",
    "so they are regular pandas DataFrames (no PyArrow ChunkedArray issue) and can be safely\n",
    "passed to `spark.createDataFrame()` if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bridge rows: 4894  |  mapping tiers:\n",
      "mapping_tier\n",
      "unmapped                 2022\n",
      "species_exact            1375\n",
      "genus_proxy_ambiguous    1352\n",
      "genus_proxy_unique        145\n",
      "\n",
      "Inventory rows: 646, unique samples: 221\n",
      "Bridge quality rows: 220\n",
      "Samples passing QC (>=30%): 220\n"
     ]
    }
   ],
   "source": [
    "# Load NB02 outputs from disk\n",
    "bridge = pd.read_csv(os.path.join(DATA_DIR, 'taxon_bridge.tsv'), sep='\\t')\n",
    "inventory = pd.read_csv(os.path.join(DATA_DIR, 'nmdc_sample_inventory.csv'))\n",
    "bridge_quality = pd.read_csv(os.path.join(DATA_DIR, 'bridge_quality.csv'))\n",
    "\n",
    "print(f'Bridge rows: {len(bridge)}  |  mapping tiers:')\n",
    "print(bridge['mapping_tier'].value_counts().to_string())\n",
    "print(f'\\nInventory rows: {len(inventory)}, unique samples: {inventory[\"sample_id\"].nunique()}')\n",
    "print(f'Bridge quality rows: {len(bridge_quality)}')\n",
    "print(f'Samples passing QC (>=30%): {bridge_quality[\"passes_bridge_qc\"].sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlap samples passing bridge QC: 220\n",
      "Sample sample_ids: ['nmdc:bsm-11-ahfq0n74', 'nmdc:bsm-11-v0hqhp22', 'nmdc:bsm-11-nx7zwq61', 'nmdc:bsm-11-h3e54f51', 'nmdc:bsm-11-65a4xw75']\n",
      "\n",
      "Unique mapped GTDB clade IDs: 1837\n"
     ]
    }
   ],
   "source": [
    "# Determine the set of overlap samples and their classifier files\n",
    "# Use samples that appear in the inventory AND pass bridge QC\n",
    "qc_pass_files = set(bridge_quality[bridge_quality['passes_bridge_qc']]['file_id'].tolist())\n",
    "\n",
    "# One clf_file per sample: take the QC-passing file (if multiple, take first)\n",
    "inventory_qc = inventory[inventory['clf_file_id'].isin(qc_pass_files)].copy()\n",
    "# Drop duplicate samples, keeping first clf_file_id\n",
    "sample_file_map = (\n",
    "    inventory_qc[['sample_id', 'clf_file_id']]\n",
    "    .drop_duplicates(subset='sample_id')\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "overlap_sample_ids = sample_file_map['sample_id'].tolist()\n",
    "\n",
    "print(f'Overlap samples passing bridge QC: {len(overlap_sample_ids)}')\n",
    "print('Sample sample_ids:', overlap_sample_ids[:5])\n",
    "\n",
    "# Get mapped clade IDs from bridge (native Python list — safe to use in SQL IN clauses)\n",
    "mapped_bridge = bridge[bridge['mapping_tier'] != 'unmapped'].dropna(subset=['gtdb_species_clade_id'])\n",
    "mapped_clade_ids = mapped_bridge['gtdb_species_clade_id'].unique().tolist()\n",
    "print(f'\\nUnique mapped GTDB clade IDs: {len(mapped_clade_ids)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: GapMind Species-Level Pathway Completeness\n",
    "\n",
    "Run two-stage aggregation on `gapmind_pathways` (305M rows), filtered to the\n",
    "~1,500 GTDB clades that appear in the taxon bridge.  \n",
    "**Stay in Spark until the final ~(n_clades × n_pathways) aggregation.**\n",
    "\n",
    "Pitfall: GapMind has multiple rows per genome-pathway pair — always GROUP BY\n",
    "and MAX score first (see `docs/pitfalls.md` [pangenome_pathway_geography])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": "# gapmind_pathways.clade_name = gtdb_species_clade_id format\n# (e.g., 's__Rhizobium_phaseoli--RS_GCF_001234567.1')\n# taxon_bridge.gtdb_species_clade_id is already in this format.\n# No GTDB_species lookup needed — use mapped_clade_ids directly as clade_name filter.\n\nmapped_clade_names = mapped_clade_ids  # These match clade_name in gapmind_pathways\nprint(f'Mapped clade names for GapMind filter: {len(mapped_clade_names)}')\nprint('Sample:', mapped_clade_names[:5])\nprint()\nprint('NOTE: gapmind_pathways.clade_name = gtdb_species_clade_id format')\nprint('      (includes genome accession suffix, e.g., s__X--RS_GCF_...)')"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered 1837 clade names as Spark temp view mapped_clade_names_tmp\n"
     ]
    }
   ],
   "source": [
    "# Register mapped clade names as a Spark temp view for efficient JOIN.\n",
    "# pd.DataFrame from Python list (not from Spark.toPandas()) — safe to use with createDataFrame().\n",
    "\n",
    "clade_names_df = pd.DataFrame({'clade_name': mapped_clade_names})\n",
    "spark.createDataFrame(clade_names_df).createOrReplaceTempView('mapped_clade_names_tmp')\n",
    "print(f'Registered {len(clade_names_df)} clade names as Spark temp view mapped_clade_names_tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": "# Step 2a: Verify GapMind schema and confirm clade_name format matches mapped_clade_names.\n# Note: Use LIMIT queries only — avoid full 305M-row scans that trigger Spark Connect\n# session reconnects and silently destroy temp views.\n\nprint('=== gapmind_pathways schema ===')\nspark.sql('DESCRIBE kbase_ke_pangenome.gapmind_pathways').show(20, truncate=False)\n\n# Sample actual clade_name values to confirm format\nprint('\\nSample clade_name values from gapmind_pathways (confirm format):')\nspark.sql(\"\"\"\n    SELECT DISTINCT clade_name\n    FROM kbase_ke_pangenome.gapmind_pathways\n    LIMIT 5\n\"\"\").show(truncate=False)\n\n# Re-register temp view here (defensive: session may have reconnected during prior cells)\nspark.createDataFrame(\n    pd.DataFrame({'clade_name': mapped_clade_names})\n).createOrReplaceTempView('mapped_clade_names_tmp')\nprint(f'Temp view re-registered: {len(mapped_clade_names)} clade names')\n\n# Verify the JOIN actually returns rows before committing to the full aggregation\nprint('\\nVerify temp view JOIN (5-row sample — should return results):')\nspark.sql(\"\"\"\n    SELECT g.clade_name, g.pathway, g.score_category, g.metabolic_category\n    FROM kbase_ke_pangenome.gapmind_pathways g\n    JOIN mapped_clade_names_tmp m ON g.clade_name = m.clade_name\n    LIMIT 5\n\"\"\").show(truncate=False)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": "# Re-register temp view (defensive: Spark Connect session may reconnect between cells)\nspark.createDataFrame(\n    pd.DataFrame({'clade_name': mapped_clade_names})\n).createOrReplaceTempView('mapped_clade_names_tmp')\n\n# Check how many GapMind rows are in the mapped clades subset\nn_gapmind_mapped = spark.sql(\"\"\"\n    SELECT COUNT(*) as n\n    FROM kbase_ke_pangenome.gapmind_pathways g\n    JOIN mapped_clade_names_tmp m ON g.clade_name = m.clade_name\n\"\"\").collect()[0]['n']\nprint(f'GapMind rows for mapped clades: {n_gapmind_mapped:,} (vs 305M total)')\n\n# Count distinct pathways in mapped subset\nn_pathways = spark.sql(\"\"\"\n    SELECT COUNT(DISTINCT pathway) as n\n    FROM kbase_ke_pangenome.gapmind_pathways g\n    JOIN mapped_clade_names_tmp m ON g.clade_name = m.clade_name\n\"\"\").collect()[0]['n']\nprint(f'Distinct pathways in mapped clades: {n_pathways}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": "# Re-register temp view immediately before the expensive aggregation (belt-and-suspenders)\nspark.createDataFrame(\n    pd.DataFrame({'clade_name': mapped_clade_names})\n).createOrReplaceTempView('mapped_clade_names_tmp')\n\n# Two-stage GapMind aggregation (stay in Spark until final result):\n# Stage 1: MAX score per (clade, genome, pathway) — eliminates duplicate rows per genome-pathway pair\n# Stage 2: AVG across genomes per (clade, pathway) — species-level completeness\n#\n# Result: ~(n_mapped_clades × n_pathways) rows — safe to .toPandas()\n\nspecies_completeness_spark = spark.sql(\"\"\"\n    WITH best_scores AS (\n        SELECT g.clade_name, g.genome_id, g.pathway, g.metabolic_category,\n               MAX(CASE g.score_category\n                   WHEN 'complete'           THEN 5\n                   WHEN 'likely_complete'    THEN 4\n                   WHEN 'steps_missing_low'  THEN 3\n                   WHEN 'steps_missing_medium' THEN 2\n                   WHEN 'not_present'        THEN 1\n                   ELSE 0 END) AS best_score\n        FROM kbase_ke_pangenome.gapmind_pathways g\n        JOIN mapped_clade_names_tmp m ON g.clade_name = m.clade_name\n        GROUP BY g.clade_name, g.genome_id, g.pathway, g.metabolic_category\n    )\n    SELECT\n        clade_name,\n        pathway,\n        metabolic_category,\n        AVG(best_score) AS mean_best_score,\n        AVG(CASE WHEN best_score >= 5 THEN 1.0 ELSE 0.0 END) AS frac_complete,\n        AVG(CASE WHEN best_score >= 4 THEN 1.0 ELSE 0.0 END) AS frac_likely_complete,\n        COUNT(DISTINCT genome_id) AS n_genomes\n    FROM best_scores\n    GROUP BY clade_name, pathway, metabolic_category\n\"\"\")\n\n# Collect to pandas — this is the expensive Spark job (~10+ min for large clade sets)\nprint('Running GapMind aggregation (may take several minutes)...')\nspecies_completeness = species_completeness_spark.toPandas()\n\nprint(f'Species pathway completeness rows: {len(species_completeness):,}')\nprint(f'Clades: {species_completeness[\"clade_name\"].nunique()}')\nprint(f'Pathways: {species_completeness[\"pathway\"].nunique()}')\nprint(f'Metabolic categories: {species_completeness[\"metabolic_category\"].unique().tolist()}')\nprint(species_completeness.head(5).to_string())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": "# gapmind_pathways.clade_name = gtdb_species_clade_id — just rename for clarity.\n# No merge needed; the values are already the same IDs used in taxon_bridge.\nspecies_completeness = species_completeness.rename(\n    columns={'clade_name': 'gtdb_species_clade_id'}\n)\n\nprint(f'Species pathway completeness rows: {len(species_completeness):,}')\nprint(f'Unique gtdb_species_clade_id: {species_completeness[\"gtdb_species_clade_id\"].nunique()}')\nprint(f'Pathways: {species_completeness[\"pathway\"].nunique()}')\nprint(species_completeness.head(3).to_string())"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data/species_pathway_completeness.csv (0 rows)\n",
      "  Clades: 0\n",
      "  Pathways: 0\n"
     ]
    }
   ],
   "source": [
    "# Save species-level pathway completeness\n",
    "sp_path = os.path.join(DATA_DIR, 'species_pathway_completeness.csv')\n",
    "species_completeness.to_csv(sp_path, index=False)\n",
    "print(f'Saved: data/species_pathway_completeness.csv ({len(species_completeness):,} rows)')\n",
    "print(f'  Clades: {species_completeness[\"clade_name\"].nunique()}')\n",
    "print(f'  Pathways: {species_completeness[\"pathway\"].nunique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Community-Weighted Pathway Completeness Per Sample\n",
    "\n",
    "For each overlap sample:\n",
    "1. Get species-rank centrifuge abundances\n",
    "2. Join taxon → `gtdb_species_clade_id` via `taxon_bridge`\n",
    "3. Join `gtdb_species_clade_id` → `frac_complete` per pathway via `species_pathway_completeness`\n",
    "4. Compute community-weighted mean, normalized by mapped abundance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Species-rank rows for overlap samples: 126,905\n",
      "Unique classifier files: 220\n",
      "                 file_id                        taxon_name  abundance\n",
      "0  nmdc:dobj-11-kbd8zm70       Staphylococcus pettenkoferi   0.000037\n",
      "1  nmdc:dobj-11-kbxytm81  Actinoalloteichus sp. AHMU CJ021   0.000028\n",
      "2  nmdc:dobj-11-kbxytm81              Variovorax sp. PMC12   0.001012\n"
     ]
    }
   ],
   "source": [
    "# Load centrifuge species-rank data for overlap samples via Spark.\n",
    "# Use omics_files_table to restrict to overlap samples (no pandas→Spark roundtrip).\n",
    "BRIDGE_TBL = 'nmdc_arkin.omics_files_table'\n",
    "\n",
    "clf_data_spark = spark.sql(f\"\"\"\n",
    "    SELECT c.file_id, c.label AS taxon_name, c.abundance\n",
    "    FROM nmdc_arkin.centrifuge_gold c\n",
    "    JOIN {BRIDGE_TBL} b ON c.file_id = b.file_id\n",
    "    WHERE b.sample_id IN (\n",
    "        SELECT b2.sample_id FROM {BRIDGE_TBL} b2\n",
    "        JOIN (SELECT DISTINCT file_id FROM nmdc_arkin.centrifuge_gold) c2 ON b2.file_id = c2.file_id\n",
    "        INTERSECT\n",
    "        SELECT b3.sample_id FROM {BRIDGE_TBL} b3\n",
    "        JOIN (SELECT DISTINCT file_id FROM nmdc_arkin.metabolomics_gold) m3 ON b3.file_id = m3.file_id\n",
    "    )\n",
    "    AND LOWER(c.rank) = 'species'\n",
    "    AND c.label IS NOT NULL AND c.label != ''\n",
    "    AND c.abundance > 0\n",
    "\"\"\")\n",
    "\n",
    "clf_data = clf_data_spark.toPandas()\n",
    "print(f'Species-rank rows for overlap samples: {len(clf_data):,}')\n",
    "print(f'Unique classifier files: {clf_data[\"file_id\"].nunique()}')\n",
    "print(clf_data.head(3).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after merging sample_id: 126,905\n",
      "Unique samples: 220\n"
     ]
    }
   ],
   "source": [
    "# Map file_id → sample_id using sample_file_map (one clf_file per sample)\n",
    "clf_data = clf_data.merge(sample_file_map.rename(columns={'clf_file_id': 'file_id'}),\n",
    "                          on='file_id', how='inner')\n",
    "print(f'Rows after merging sample_id: {len(clf_data):,}')\n",
    "print(f'Unique samples: {clf_data[\"sample_id\"].nunique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows mapped to a GTDB clade: 112,794 / 126,905 (88.9%)\n",
      "Fraction of total abundance covered by mapped taxa: 93.5%\n",
      "\n",
      "Mapping tier breakdown:\n",
      "mapping_tier\n",
      "genus_proxy_ambiguous    57355\n",
      "species_exact            51026\n",
      "NaN                      14111\n",
      "genus_proxy_unique        4413\n"
     ]
    }
   ],
   "source": [
    "# Join centrifuge taxa with taxon_bridge to get gtdb_species_clade_id\n",
    "# Use all mapped tiers (species_exact, genus_proxy_unique, genus_proxy_ambiguous)\n",
    "bridge_mapped = bridge[bridge['mapping_tier'] != 'unmapped'][[\n",
    "    'taxon_name', 'gtdb_species_clade_id', 'mapping_tier'\n",
    "]].dropna(subset=['gtdb_species_clade_id']).drop_duplicates(subset='taxon_name')\n",
    "\n",
    "clf_bridged = clf_data.merge(bridge_mapped, on='taxon_name', how='left')\n",
    "\n",
    "n_mapped = clf_bridged['gtdb_species_clade_id'].notna().sum()\n",
    "n_total = len(clf_bridged)\n",
    "mapped_abund_frac = (\n",
    "    clf_bridged[clf_bridged['gtdb_species_clade_id'].notna()]['abundance'].sum()\n",
    "    / clf_bridged['abundance'].sum()\n",
    ")\n",
    "\n",
    "print(f'Rows mapped to a GTDB clade: {n_mapped:,} / {n_total:,} ({n_mapped/n_total:.1%})')\n",
    "print(f'Fraction of total abundance covered by mapped taxa: {mapped_abund_frac:.1%}')\n",
    "print('\\nMapping tier breakdown:')\n",
    "print(clf_bridged['mapping_tier'].value_counts(dropna=False).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cell-18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after joining with completeness: 0\n",
      "Unique samples: 0\n",
      "Unique pathways: 0\n"
     ]
    }
   ],
   "source": [
    "# Join with species pathway completeness\n",
    "# completeness table is keyed by gtdb_species_clade_id + pathway\n",
    "\n",
    "clf_with_completeness = clf_bridged[clf_bridged['gtdb_species_clade_id'].notna()].merge(\n",
    "    species_completeness[['gtdb_species_clade_id', 'pathway', 'metabolic_category',\n",
    "                           'frac_complete', 'frac_likely_complete', 'mean_best_score']],\n",
    "    on='gtdb_species_clade_id',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f'Rows after joining with completeness: {len(clf_with_completeness):,}')\n",
    "print(f'Unique samples: {clf_with_completeness[\"sample_id\"].nunique()}')\n",
    "print(f'Unique pathways: {clf_with_completeness[\"pathway\"].nunique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cell-19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Community matrix (long): 0 rows\n",
      "Samples: 0\n",
      "Pathways: 0\n",
      "       n_mapped_taxa  total_mapped_abund\n",
      "count            0.0                 0.0\n",
      "mean             NaN                 NaN\n",
      "std              NaN                 NaN\n",
      "min              NaN                 NaN\n",
      "25%              NaN                 NaN\n",
      "50%              NaN                 NaN\n",
      "75%              NaN                 NaN\n",
      "max              NaN                 NaN\n"
     ]
    }
   ],
   "source": [
    "# Compute community-weighted pathway completeness per sample\n",
    "#\n",
    "# For each (sample, pathway):\n",
    "#   total_mapped_abund = Σ abundance_i  (only over taxa with a completeness score for this pathway)\n",
    "#   community_completeness = Σ (abundance_i / total_mapped_abund) × frac_complete_i\n",
    "#\n",
    "# This gives a value in [0,1]: fraction of community (by mapped abundance) with a complete pathway.\n",
    "\n",
    "# Step 1: total mapped abundance per sample per pathway (denominator)\n",
    "sample_pathway_totals = (\n",
    "    clf_with_completeness\n",
    "    .groupby(['sample_id', 'pathway'])['abundance']\n",
    "    .sum()\n",
    "    .rename('total_mapped_abund')\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Step 2: merge denominator back and compute weighted completeness\n",
    "clf_w = clf_with_completeness.merge(sample_pathway_totals, on=['sample_id', 'pathway'])\n",
    "clf_w['weight'] = clf_w['abundance'] / clf_w['total_mapped_abund']\n",
    "clf_w['weighted_frac_complete'] = clf_w['weight'] * clf_w['frac_complete']\n",
    "clf_w['weighted_frac_likely_complete'] = clf_w['weight'] * clf_w['frac_likely_complete']\n",
    "\n",
    "# Step 3: aggregate by sample × pathway\n",
    "community_matrix_long = (\n",
    "    clf_w.groupby(['sample_id', 'pathway', 'metabolic_category'])\n",
    "    .agg(\n",
    "        community_frac_complete=('weighted_frac_complete', 'sum'),\n",
    "        community_frac_likely_complete=('weighted_frac_likely_complete', 'sum'),\n",
    "        n_mapped_taxa=('taxon_name', 'nunique'),\n",
    "        total_mapped_abund=('total_mapped_abund', 'first')\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(f'Community matrix (long): {len(community_matrix_long):,} rows')\n",
    "print(f'Samples: {community_matrix_long[\"sample_id\"].nunique()}')\n",
    "print(f'Pathways: {community_matrix_long[\"pathway\"].nunique()}')\n",
    "print(community_matrix_long.describe().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cell-20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Community pathway matrix (wide): (0, 1)\n",
      "  Samples: 0\n",
      "  Pathways: 0\n",
      "\n",
      "Mean community completeness per pathway (top 10 most complete):\n",
      "Series([], )\n",
      "\n",
      "Bottom 10 (least complete):\n",
      "Series([], )\n"
     ]
    }
   ],
   "source": [
    "# Pivot to wide format: samples (rows) × pathways (columns)\n",
    "community_matrix_wide = community_matrix_long.pivot_table(\n",
    "    index='sample_id',\n",
    "    columns='pathway',\n",
    "    values='community_frac_complete',\n",
    "    aggfunc='first'\n",
    ").reset_index()\n",
    "\n",
    "print(f'Community pathway matrix (wide): {community_matrix_wide.shape}')\n",
    "print(f'  Samples: {len(community_matrix_wide)}')\n",
    "print(f'  Pathways: {community_matrix_wide.shape[1] - 1}')\n",
    "\n",
    "# Summary: completeness score distributions\n",
    "pathway_cols = [c for c in community_matrix_wide.columns if c != 'sample_id']\n",
    "print(f'\\nMean community completeness per pathway (top 10 most complete):')\n",
    "pathway_means = community_matrix_wide[pathway_cols].mean().sort_values(ascending=False)\n",
    "print(pathway_means.head(10).to_string())\n",
    "print(f'\\nBottom 10 (least complete):')\n",
    "print(pathway_means.tail(10).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cell-21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample ecosystem metadata: 6361 rows\n",
      "ecosystem_type distribution:\n",
      "ecosystem_type\n",
      "NaN             5659\n",
      "Soil             348\n",
      "Freshwater       325\n",
      "Unclassified      29\n"
     ]
    }
   ],
   "source": [
    "# Add ecosystem type from omics_files_table (has study_id) + study_table\n",
    "# Use study_id from omics_files_table → join to study_table → get ecosystem_category, ecosystem_type\n",
    "\n",
    "sample_study = spark.sql(\"\"\"\n",
    "    SELECT DISTINCT b.sample_id, b.study_id\n",
    "    FROM nmdc_arkin.omics_files_table b\n",
    "    JOIN (SELECT DISTINCT file_id FROM nmdc_arkin.centrifuge_gold) c ON b.file_id = c.file_id\n",
    "    WHERE b.sample_id IS NOT NULL AND b.study_id IS NOT NULL\n",
    "\"\"\").toPandas()\n",
    "\n",
    "study_meta = spark.sql(\"\"\"\n",
    "    SELECT study_id, ecosystem_category, ecosystem_type, ecosystem_subtype, specific_ecosystem\n",
    "    FROM nmdc_arkin.study_table\n",
    "\"\"\").toPandas()\n",
    "\n",
    "# Join sample_study → study_meta\n",
    "sample_ecosystem = sample_study.merge(study_meta, on='study_id', how='left')\n",
    "# Keep only one row per sample_id (take first study_id if multiple)\n",
    "sample_ecosystem = (\n",
    "    sample_ecosystem\n",
    "    .sort_values('study_id')\n",
    "    .drop_duplicates(subset='sample_id')\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(f'Sample ecosystem metadata: {len(sample_ecosystem)} rows')\n",
    "print('ecosystem_type distribution:')\n",
    "print(sample_ecosystem['ecosystem_type'].value_counts(dropna=False).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cell-22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Community matrix with ecosystem: (0, 6)\n",
      "ecosystem_type counts for overlap samples:\n",
      "Series([], )\n"
     ]
    }
   ],
   "source": [
    "# Merge ecosystem metadata into community_matrix_wide\n",
    "community_matrix_wide = community_matrix_wide.merge(\n",
    "    sample_ecosystem[['sample_id', 'study_id', 'ecosystem_category',\n",
    "                       'ecosystem_type', 'ecosystem_subtype', 'specific_ecosystem']],\n",
    "    on='sample_id', how='left'\n",
    ")\n",
    "\n",
    "print(f'Community matrix with ecosystem: {community_matrix_wide.shape}')\n",
    "print('ecosystem_type counts for overlap samples:')\n",
    "print(community_matrix_wide['ecosystem_type'].value_counts(dropna=False).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Heatmap — Mean Completeness by Pathway × Ecosystem Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cell-24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heatmap: 0 pathways × 0 ecosystem types\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Compute mean community completeness per pathway × ecosystem_type\n",
    "# Use long-format matrix (includes metabolic_category for ordering)\n",
    "\n",
    "# Merge ecosystem into long format\n",
    "community_long_eco = community_matrix_long.merge(\n",
    "    sample_ecosystem[['sample_id', 'ecosystem_type']],\n",
    "    on='sample_id', how='left'\n",
    ")\n",
    "community_long_eco['ecosystem_type'] = community_long_eco['ecosystem_type'].fillna('Unknown')\n",
    "\n",
    "# Mean completeness per pathway × ecosystem_type\n",
    "heatmap_data = (\n",
    "    community_long_eco\n",
    "    .groupby(['pathway', 'metabolic_category', 'ecosystem_type'])['community_frac_complete']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={'community_frac_complete': 'mean_completeness'})\n",
    ")\n",
    "\n",
    "# Pivot for heatmap\n",
    "heatmap_pivot = heatmap_data.pivot_table(\n",
    "    index='pathway',\n",
    "    columns='ecosystem_type',\n",
    "    values='mean_completeness'\n",
    ")\n",
    "\n",
    "# Sort pathways by metabolic_category then by mean completeness\n",
    "pathway_category = community_long_eco[['pathway', 'metabolic_category']].drop_duplicates()\n",
    "heatmap_pivot = heatmap_pivot.join(\n",
    "    pathway_category.set_index('pathway')['metabolic_category']\n",
    ").sort_values(['metabolic_category', 'pathway']).drop(columns='metabolic_category')\n",
    "\n",
    "print(f'Heatmap: {heatmap_pivot.shape[0]} pathways × {heatmap_pivot.shape[1]} ecosystem types')\n",
    "print(heatmap_pivot.head(5).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cell-25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: figures/pathway_completeness_heatmap.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.13/site-packages/seaborn/matrix.py:309: UserWarning: Attempting to set identical low and high xlims makes transformation singular; automatically expanding.\n",
      "  ax.set(xlim=(0, self.data.shape[1]), ylim=(0, self.data.shape[0]))\n",
      "/opt/conda/lib/python3.13/site-packages/seaborn/matrix.py:309: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n",
      "  ax.set(xlim=(0, self.data.shape[1]), ylim=(0, self.data.shape[0]))\n"
     ]
    }
   ],
   "source": [
    "# Plot heatmap: pathways (rows) × ecosystem types (columns)\n",
    "n_pathways_plot = heatmap_pivot.shape[0]\n",
    "n_ecosystems = heatmap_pivot.shape[1]\n",
    "\n",
    "fig_height = max(8, n_pathways_plot * 0.22)\n",
    "fig, ax = plt.subplots(figsize=(max(6, n_ecosystems * 1.5), fig_height))\n",
    "\n",
    "sns.heatmap(\n",
    "    heatmap_pivot,\n",
    "    ax=ax,\n",
    "    cmap='YlOrRd',\n",
    "    vmin=0, vmax=1,\n",
    "    linewidths=0.3,\n",
    "    linecolor='white',\n",
    "    annot=(n_pathways_plot <= 30),  # only annotate if <= 30 pathways\n",
    "    fmt='.2f' if n_pathways_plot <= 30 else '',\n",
    "    cbar_kws={'label': 'Mean community pathway completeness'},\n",
    ")\n",
    "\n",
    "ax.set_title('Community Pathway Completeness by Ecosystem Type', fontsize=13, pad=12)\n",
    "ax.set_xlabel('Ecosystem type', fontsize=11)\n",
    "ax.set_ylabel('Pathway', fontsize=11)\n",
    "ax.tick_params(axis='y', labelsize=8)\n",
    "ax.tick_params(axis='x', labelsize=9, rotation=30)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig_path = os.path.join(FIGURES_DIR, 'pathway_completeness_heatmap.png')\n",
    "plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f'Saved: figures/pathway_completeness_heatmap.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Save Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cell-27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data/community_pathway_matrix.csv\n",
      "  Shape: (0, 6)\n",
      "  Samples: 0\n",
      "  Pathway columns: 0\n",
      "Saved: data/community_pathway_matrix_long.csv (0 rows)\n"
     ]
    }
   ],
   "source": [
    "# Save community pathway matrix (wide format — used in NB05 statistical analysis)\n",
    "matrix_path = os.path.join(DATA_DIR, 'community_pathway_matrix.csv')\n",
    "community_matrix_wide.to_csv(matrix_path, index=False)\n",
    "print(f'Saved: data/community_pathway_matrix.csv')\n",
    "print(f'  Shape: {community_matrix_wide.shape}')\n",
    "print(f'  Samples: {community_matrix_wide[\"sample_id\"].nunique()}')\n",
    "print(f'  Pathway columns: {len(pathway_cols)}')\n",
    "\n",
    "# Save long-format community matrix (useful for NB05 joins and groupby)\n",
    "long_path = os.path.join(DATA_DIR, 'community_pathway_matrix_long.csv')\n",
    "community_matrix_long.to_csv(long_path, index=False)\n",
    "print(f'Saved: data/community_pathway_matrix_long.csv ({len(community_matrix_long):,} rows)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": "# Summary: key statistics for NB05\nprint('=== NB03 Summary ===')\nprint(f'Overlap samples with pathway completeness data: {community_matrix_wide[\"sample_id\"].nunique()}')\nprint(f'Pathways computed: {len(pathway_cols)}')\nprint(f'Metabolic categories: {community_long_eco[\"metabolic_category\"].unique().tolist()}')\n\n# metabolic_category values in gapmind_pathways are 'aa' and 'carbon' (not 'amino_acid')\naa_mask = community_long_eco['metabolic_category'] == 'aa'\nn_aa_pathways = community_long_eco.loc[aa_mask, 'pathway'].nunique() if aa_mask.sum() > 0 else 0\nprint(f'Amino acid (aa) pathways: {n_aa_pathways}')\nprint()\nprint('Mean community completeness across all samples (frac_complete):')\nprint(f'  All pathways: {community_matrix_long[\"community_frac_complete\"].mean():.3f}')\nif aa_mask.sum() > 0:\n    print(f'  Amino acid (aa) pathways: '\n          f'{community_long_eco.loc[aa_mask, \"community_frac_complete\"].mean():.3f}')\nprint()\nprint('Ecosystem breakdown for overlap samples:')\nprint(community_matrix_wide[['ecosystem_category', 'ecosystem_type']]\n      .value_counts(dropna=False).to_string())"
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary and Decisions for NB04\n",
    "\n",
    "| Question | Finding |\n",
    "|---|---|\n",
    "| Samples in community matrix | ??? |\n",
    "| Pathways computed | ??? |\n",
    "| Amino acid pathways | ??? |\n",
    "| Mean community completeness (all pathways) | ??? |\n",
    "| Mean community completeness (AA pathways) | ??? |\n",
    "| Ecosystem types represented | ??? |\n",
    "\n",
    "**Decision for NB04**:  \n",
    "Proceed to metabolomics processing using `data/community_pathway_matrix.csv`.  \n",
    "Join with `metabolomics_gold` on met_file_id from `nmdc_sample_inventory.csv`.  \n",
    "Target amino acid compounds from `metabolomics_gold` for the Black Queen test (H1)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}