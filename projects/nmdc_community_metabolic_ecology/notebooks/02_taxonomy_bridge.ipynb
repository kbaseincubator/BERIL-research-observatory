{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# NB02: File→Sample Bridge and Taxonomy→GTDB Mapping\n",
    "\n",
    "**Project**: Community Metabolic Ecology via NMDC × Pangenome Integration  \n",
    "**Requires**: BERDL JupyterHub (Spark — `get_spark_session()` injected into kernel)  \n",
    "\n",
    "## Purpose\n",
    "\n",
    "NB01 confirmed that classifier files (`nmdc:dobj-11-*`) and metabolomics files (`nmdc:dobj-12-*`)  \n",
    "share **zero file_id overlap** — they are different workflow output types. The shared identifier  \n",
    "is the **biosample/sample_id** (e.g., `nmdc:bsm-11-*`). This notebook:\n",
    "\n",
    "1. **Part 1**: Find the `file_id → sample_id` bridge in `nmdc_arkin`\n",
    "2. **Part 2**: Build sample inventory — samples with both metagenomics classifier AND metabolomics data\n",
    "3. **Part 3**: Map NMDC taxon names (centrifuge_gold) → GTDB species (`gtdb_species_clade`)\n",
    "4. **Part 4**: Compute bridge quality per sample\n",
    "5. **Part 5**: Save outputs\n",
    "\n",
    "## Inputs\n",
    "\n",
    "- `nmdc_arkin` tables: `centrifuge_gold`, `metabolomics_gold`, `abiotic_features`, `study_table`\n",
    "- `kbase_ke_pangenome`: `gtdb_species_clade`\n",
    "\n",
    "## Outputs\n",
    "\n",
    "- `data/sample_file_bridge.csv` — sample_id ↔ file_id mapping for all omics types\n",
    "- `data/nmdc_sample_inventory.csv` — updated with samples having paired omics (replaces NB01 empty file)\n",
    "- `data/taxon_bridge.tsv` — NMDC taxon name → GTDB species clade mappings with confidence tiers\n",
    "- `data/bridge_quality.csv` — per-sample fraction of community abundance mapped to pangenome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On BERDL JupyterHub — get_spark_session() is injected into the kernel; no import needed\n",
    "spark = get_spark_session()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PROJECT_DIR = os.path.abspath(os.path.join(os.path.dirname('__file__'), '..'))\n",
    "DATA_DIR = os.path.join(PROJECT_DIR, 'data')\n",
    "FIGURES_DIR = os.path.join(PROJECT_DIR, 'figures')\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
    "print(f'DATA_DIR: {DATA_DIR}')\n",
    "print(f'FIGURES_DIR: {FIGURES_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Find the file_id → sample_id Bridge\n",
    "\n",
    "NB01 found that classifier tables and metabolomics tables use non-overlapping `file_id` namespaces.  \n",
    "The NMDC data model links files to biosamples through workflow activities.  \n",
    "We explore candidate tables in `nmdc_arkin` to find a `file_id → sample_id` mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1a: Inspect candidate bridge tables — look for tables that have both file_id and sample_id\n",
    "# Candidates from schema doc: sample_tokens_v1, taxonomy_dim, taxstring_lookup,\n",
    "# embedding_metadata, taxonomy_embeddings, trait_unified\n",
    "\n",
    "candidate_tables = [\n",
    "    'sample_tokens_v1',\n",
    "    'taxonomy_dim',\n",
    "    'taxstring_lookup',\n",
    "    'embedding_metadata',\n",
    "    'taxonomy_embeddings',\n",
    "    'trait_unified',\n",
    "    'biochemical_features',\n",
    "    'biochemical_embeddings',\n",
    "    'abiotic_embeddings',\n",
    "]\n",
    "\n",
    "bridge_candidates = []  # tables that have both file_id and sample_id\n",
    "\n",
    "for tbl in candidate_tables:\n",
    "    try:\n",
    "        schema = spark.sql(f'DESCRIBE nmdc_arkin.{tbl}').toPandas()\n",
    "        cols = set(schema['col_name'].tolist())\n",
    "        has_file = 'file_id' in cols\n",
    "        has_sample = 'sample_id' in cols\n",
    "        n_cols = len(cols)\n",
    "        print(f'{tbl}: {n_cols} cols, file_id={has_file}, sample_id={has_sample}')\n",
    "        if has_file or has_sample:\n",
    "            print(f'  -> columns: {sorted(cols)[:12]}')\n",
    "        if has_file and has_sample:\n",
    "            bridge_candidates.append(tbl)\n",
    "            print(f'  *** BRIDGE CANDIDATE ***')\n",
    "    except Exception as e:\n",
    "        print(f'{tbl}: ERROR — {e}')\n",
    "\n",
    "print(f'\\nBridge candidates (have both file_id and sample_id): {bridge_candidates}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1b: List all tables in nmdc_arkin to find any additional candidates\n",
    "all_tables = spark.sql('SHOW TABLES IN nmdc_arkin').toPandas()\n",
    "print(f'Total tables in nmdc_arkin: {len(all_tables)}')\n",
    "print(all_tables['tableName'].sort_values().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1c: Scan ALL tables in nmdc_arkin for file_id OR sample_id columns\n",
    "# This finds any table not covered by the candidate list above\n",
    "\n",
    "table_names = all_tables['tableName'].tolist()\n",
    "file_id_tables = []\n",
    "sample_id_tables = []\n",
    "both_id_tables = []\n",
    "\n",
    "for tbl in table_names:\n",
    "    try:\n",
    "        schema = spark.sql(f'DESCRIBE nmdc_arkin.{tbl}').toPandas()\n",
    "        cols = set(schema['col_name'].tolist())\n",
    "        has_file = 'file_id' in cols\n",
    "        has_sample = 'sample_id' in cols\n",
    "        if has_file:\n",
    "            file_id_tables.append(tbl)\n",
    "        if has_sample:\n",
    "            sample_id_tables.append(tbl)\n",
    "        if has_file and has_sample:\n",
    "            both_id_tables.append(tbl)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "print('Tables with file_id:', file_id_tables)\n",
    "print()\n",
    "print('Tables with sample_id:', sample_id_tables)\n",
    "print()\n",
    "print('Tables with BOTH file_id and sample_id (bridge candidates):', both_id_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1d: Inspect the bridge table(s) found above\n",
    "# If both_id_tables is non-empty, show schema and sample rows for each\n",
    "\n",
    "if both_id_tables:\n",
    "    for tbl in both_id_tables:\n",
    "        print(f'\\n=== nmdc_arkin.{tbl} schema ===')\n",
    "        spark.sql(f'DESCRIBE nmdc_arkin.{tbl}').show(30, truncate=False)\n",
    "\n",
    "        n = spark.sql(f'SELECT COUNT(*) as n FROM nmdc_arkin.{tbl}').collect()[0]['n']\n",
    "        print(f'Row count: {n}')\n",
    "\n",
    "        print(f'Sample rows:')\n",
    "        spark.sql(f'SELECT * FROM nmdc_arkin.{tbl} LIMIT 5').show(truncate=False)\n",
    "else:\n",
    "    print('No table with both file_id and sample_id found in nmdc_arkin.')\n",
    "    print('Will attempt bridge via file_name parsing (Step 1e).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1e: Explore sample_id format in abiotic_features\n",
    "# to understand what ID format we need to bridge TO\n",
    "print('=== abiotic_features sample_id examples ===')\n",
    "abiotic_ids = spark.sql(\"\"\"\n",
    "    SELECT sample_id\n",
    "    FROM nmdc_arkin.abiotic_features\n",
    "    LIMIT 10\n",
    "\"\"\").toPandas()\n",
    "print(abiotic_ids['sample_id'].tolist())\n",
    "\n",
    "# Also check study_table: do study_ids appear in sample_ids?\n",
    "print('\\nStudy_id examples from study_table:')\n",
    "study_ids = spark.sql(\"\"\"\n",
    "    SELECT study_id FROM nmdc_arkin.study_table LIMIT 5\n",
    "\"\"\").toPandas()\n",
    "print(study_ids['study_id'].tolist())\n",
    "\n",
    "# Check if sample_id in abiotic_features starts with 'nmdc:bsm-'\n",
    "bsm_count = spark.sql(\"\"\"\n",
    "    SELECT COUNT(*) as n\n",
    "    FROM nmdc_arkin.abiotic_features\n",
    "    WHERE sample_id LIKE 'nmdc:bsm-%'\n",
    "\"\"\").collect()[0]['n']\n",
    "print(f'\\nabiotic_features rows where sample_id starts with nmdc:bsm-: {bsm_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1f: Check if the sample_tokens_v1 table has more columns than documented\n",
    "# and whether it contains file_id or a file reference field\n",
    "try:\n",
    "    print('=== sample_tokens_v1 schema ===')\n",
    "    spark.sql('DESCRIBE nmdc_arkin.sample_tokens_v1').show(40, truncate=False)\n",
    "    n = spark.sql('SELECT COUNT(*) as n FROM nmdc_arkin.sample_tokens_v1').collect()[0]['n']\n",
    "    print(f'Row count: {n}')\n",
    "    print('Sample rows:')\n",
    "    spark.sql('SELECT * FROM nmdc_arkin.sample_tokens_v1 LIMIT 3').show(truncate=False)\n",
    "except Exception as e:\n",
    "    print(f'sample_tokens_v1 error: {e}')\n",
    "\n",
    "# Also check embeddings_v1 for sample_id format\n",
    "try:\n",
    "    print('\\n=== embeddings_v1 schema ===')\n",
    "    spark.sql('DESCRIBE nmdc_arkin.embeddings_v1').show(20, truncate=False)\n",
    "    n = spark.sql('SELECT COUNT(*) as n FROM nmdc_arkin.embeddings_v1').collect()[0]['n']\n",
    "    print(f'Row count: {n}')\n",
    "    # Show non-vector columns (sample_id etc.)\n",
    "    emb_schema = spark.sql('DESCRIBE nmdc_arkin.embeddings_v1').toPandas()\n",
    "    str_cols = emb_schema[emb_schema['data_type'] == 'string']['col_name'].tolist()\n",
    "    if str_cols:\n",
    "        cols_sql = ', '.join([f'`{c}`' for c in str_cols[:6]])\n",
    "        spark.sql(f'SELECT {cols_sql} FROM nmdc_arkin.embeddings_v1 LIMIT 5').show(truncate=False)\n",
    "except Exception as e:\n",
    "    print(f'embeddings_v1 error: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1g: Try to build the bridge from the best candidate found above.\n",
    "# If a table with both file_id and sample_id was found, use it directly.\n",
    "# Otherwise, try to link through file_name: kraken file_names encode the\n",
    "# workflow activity ID (e.g., 'nmdc_wfrbt-11-krmkys65.1_kraken2_report.tsv').\n",
    "# Check if any table maps workflow activity IDs to biosample IDs.\n",
    "\n",
    "file_bridge_df = None  # will be set below if a bridge is found\n",
    "\n",
    "if both_id_tables:\n",
    "    # Use the first table with both IDs\n",
    "    bridge_tbl = both_id_tables[0]\n",
    "    print(f'Using bridge table: nmdc_arkin.{bridge_tbl}')\n",
    "    file_bridge_df = spark.sql(f\"\"\"\n",
    "        SELECT DISTINCT file_id, sample_id\n",
    "        FROM nmdc_arkin.{bridge_tbl}\n",
    "        WHERE file_id IS NOT NULL AND sample_id IS NOT NULL\n",
    "    \"\"\").toPandas()\n",
    "    print(f'Bridge rows (distinct file_id, sample_id): {len(file_bridge_df)}')\n",
    "    print('Sample rows:')\n",
    "    print(file_bridge_df.head(5).to_string())\n",
    "\n",
    "    # Check coverage: how many classifier file_ids are in the bridge?\n",
    "    clf_files = spark.sql(\"\"\"\n",
    "        SELECT DISTINCT file_id FROM nmdc_arkin.centrifuge_gold\n",
    "    \"\"\").toPandas()['file_id']\n",
    "    met_files = spark.sql(\"\"\"\n",
    "        SELECT DISTINCT file_id FROM nmdc_arkin.metabolomics_gold\n",
    "    \"\"\").toPandas()['file_id']\n",
    "\n",
    "    n_clf_bridged = file_bridge_df['file_id'].isin(clf_files).sum()\n",
    "    n_met_bridged = file_bridge_df['file_id'].isin(met_files).sum()\n",
    "    print(f'\\nClassifier file_ids in bridge: {n_clf_bridged} / {len(clf_files)}')\n",
    "    print(f'Metabolomics file_ids in bridge: {n_met_bridged} / {len(met_files)}')\n",
    "else:\n",
    "    print('No direct bridge table found. Attempting file_name-based bridge...')\n",
    "    # Inspect file_name patterns in classifier and metabolomics tables\n",
    "    print('Classifier file_name samples:')\n",
    "    spark.sql(\"\"\"\n",
    "        SELECT DISTINCT file_id, file_name\n",
    "        FROM nmdc_arkin.centrifuge_gold\n",
    "        LIMIT 5\n",
    "    \"\"\").show(truncate=False)\n",
    "    print('Metabolomics file_name samples:')\n",
    "    spark.sql(\"\"\"\n",
    "        SELECT DISTINCT file_id, file_name\n",
    "        FROM nmdc_arkin.metabolomics_gold\n",
    "        LIMIT 5\n",
    "    \"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1h: If no direct bridge was found, check nmdc_ncbi_biosamples for a cross-reference\n",
    "# The nmdc_ncbi_biosamples database may contain a file → biosample link\n",
    "\n",
    "if file_bridge_df is None or len(file_bridge_df) == 0:\n",
    "    print('Exploring nmdc_ncbi_biosamples for file → sample links...')\n",
    "    try:\n",
    "        ncbi_tables = spark.sql('SHOW TABLES IN nmdc_ncbi_biosamples').toPandas()\n",
    "        print('Tables in nmdc_ncbi_biosamples:', ncbi_tables['tableName'].tolist())\n",
    "\n",
    "        # Check biosamples_ids for file_id or dobj references\n",
    "        try:\n",
    "            spark.sql('DESCRIBE nmdc_ncbi_biosamples.biosamples_ids').show(20, truncate=False)\n",
    "            spark.sql('SELECT * FROM nmdc_ncbi_biosamples.biosamples_ids LIMIT 5').show(truncate=False)\n",
    "        except Exception as e:\n",
    "            print(f'biosamples_ids: {e}')\n",
    "\n",
    "        # Check biosamples_links for file_id\n",
    "        try:\n",
    "            link_schema = spark.sql('DESCRIBE nmdc_ncbi_biosamples.biosamples_links').toPandas()\n",
    "            print('\\nbiosamples_links columns:', link_schema['col_name'].tolist())\n",
    "            spark.sql('SELECT * FROM nmdc_ncbi_biosamples.biosamples_links LIMIT 5').show(truncate=False)\n",
    "        except Exception as e:\n",
    "            print(f'biosamples_links: {e}')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'nmdc_ncbi_biosamples exploration error: {e}')\n",
    "else:\n",
    "    print(f'Bridge already found ({len(file_bridge_df)} rows). Skipping nmdc_ncbi_biosamples.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Sample Inventory — Samples with Both Omics Data Types\n",
    "\n",
    "Using the bridge found in Part 1, identify `sample_id` values that have both  \n",
    "a metagenomics classifier file (centrifuge_gold) AND a metabolomics file (metabolomics_gold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build sample overlap via the bridge\n",
    "# Strategy: file_bridge_df maps file_id → sample_id\n",
    "# Join centrifuge_gold file_ids through bridge to get sample_ids with metagenomics\n",
    "# Join metabolomics_gold file_ids through bridge to get sample_ids with metabolomics\n",
    "# Intersect the two sample_id sets\n",
    "\n",
    "if file_bridge_df is not None and len(file_bridge_df) > 0:\n",
    "    # Register bridge as a Spark temp view for SQL joins\n",
    "    bridge_spark = spark.createDataFrame(file_bridge_df[['file_id', 'sample_id']].drop_duplicates())\n",
    "    bridge_spark.createOrReplaceTempView('file_sample_bridge')\n",
    "\n",
    "    # Samples with metagenomics (centrifuge) classifier data\n",
    "    clf_samples = spark.sql(\"\"\"\n",
    "        SELECT DISTINCT b.sample_id\n",
    "        FROM (SELECT DISTINCT file_id FROM nmdc_arkin.centrifuge_gold) c\n",
    "        JOIN file_sample_bridge b ON c.file_id = b.file_id\n",
    "    \"\"\").toPandas()\n",
    "\n",
    "    # Samples with metabolomics data\n",
    "    met_samples = spark.sql(\"\"\"\n",
    "        SELECT DISTINCT b.sample_id\n",
    "        FROM (SELECT DISTINCT file_id FROM nmdc_arkin.metabolomics_gold) m\n",
    "        JOIN file_sample_bridge b ON m.file_id = b.file_id\n",
    "    \"\"\").toPandas()\n",
    "\n",
    "    # Samples with BOTH\n",
    "    clf_set = set(clf_samples['sample_id'])\n",
    "    met_set = set(met_samples['sample_id'])\n",
    "    overlap_samples = clf_set & met_set\n",
    "\n",
    "    print(f'Samples with classifier data (centrifuge): {len(clf_set)}')\n",
    "    print(f'Samples with metabolomics data:            {len(met_set)}')\n",
    "    print(f'Samples with BOTH (overlap):               {len(overlap_samples)}')\n",
    "    overlap_sample_ids = list(overlap_samples)\n",
    "else:\n",
    "    print('ERROR: No file→sample bridge available. Cannot compute sample overlap.')\n",
    "    print('Manual inspection of bridge tables is required before proceeding.')\n",
    "    # Set fallback empty state\n",
    "    clf_set = set()\n",
    "    met_set = set()\n",
    "    overlap_sample_ids = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge ecosystem metadata from study_table for overlap samples\n",
    "# abiotic_features uses sample_id — join directly\n",
    "\n",
    "if overlap_sample_ids:\n",
    "    # Load abiotic features for overlap samples\n",
    "    abiotic_df = spark.sql('SELECT * FROM nmdc_arkin.abiotic_features').toPandas()\n",
    "    abiotic_overlap = abiotic_df[abiotic_df['sample_id'].isin(overlap_sample_ids)].copy()\n",
    "    print(f'Abiotic features rows for overlap samples: {len(abiotic_overlap)}')\n",
    "\n",
    "    # Study table for ecosystem categorization\n",
    "    # study_id format: nmdc:sty-11-XXXXXXXX\n",
    "    # sample_id format: nmdc:bsm-11-XXXXXXXX (if biosample prefix)\n",
    "    # Link via file_bridge_df: get file_id → study linkage from file_name\n",
    "    # file_name for classifier: 'nmdc_wfrbt-11-krmkys65.1_kraken2_report.tsv'\n",
    "    # Study linkage must come through study_table\n",
    "\n",
    "    # For now, check what sample_id values look like\n",
    "    if overlap_sample_ids:\n",
    "        print(f'\\nFirst 10 overlap sample_ids: {overlap_sample_ids[:10]}')\n",
    "else:\n",
    "    print('No overlap samples — skipping ecosystem metadata merge.')\n",
    "    abiotic_overlap = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build per-sample file inventory with omics type labels\n",
    "# For each overlap sample: list its classifier file_id and metabolomics file_id\n",
    "\n",
    "if overlap_sample_ids and file_bridge_df is not None:\n",
    "    # Get classifier file_ids per sample\n",
    "    clf_file_df = spark.sql(\"\"\"\n",
    "        SELECT DISTINCT b.sample_id, c.file_id as clf_file_id, c.file_name as clf_file_name\n",
    "        FROM (SELECT DISTINCT file_id, file_name FROM nmdc_arkin.centrifuge_gold) c\n",
    "        JOIN file_sample_bridge b ON c.file_id = b.file_id\n",
    "    \"\"\").toPandas()\n",
    "    clf_file_df = clf_file_df[clf_file_df['sample_id'].isin(overlap_sample_ids)]\n",
    "\n",
    "    # Get metabolomics file_ids per sample\n",
    "    met_file_df = spark.sql(\"\"\"\n",
    "        SELECT DISTINCT b.sample_id, m.file_id as met_file_id, m.file_name as met_file_name\n",
    "        FROM (SELECT DISTINCT file_id, file_name FROM nmdc_arkin.metabolomics_gold) m\n",
    "        JOIN file_sample_bridge b ON m.file_id = b.file_id\n",
    "    \"\"\").toPandas()\n",
    "    met_file_df = met_file_df[met_file_df['sample_id'].isin(overlap_sample_ids)]\n",
    "\n",
    "    # Combine into inventory\n",
    "    inventory = clf_file_df.merge(met_file_df, on='sample_id', how='inner')\n",
    "    print(f'Inventory rows (sample × clf_file × met_file): {len(inventory)}')\n",
    "    print(f'Unique samples in inventory: {inventory[\"sample_id\"].nunique()}')\n",
    "    print(inventory.head(5).to_string())\n",
    "else:\n",
    "    print('Cannot build inventory — no bridge or no overlap samples.')\n",
    "    inventory = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: NMDC Taxon Names → GTDB Species Clade\n",
    "\n",
    "Map species-rank taxon names from `centrifuge_gold` to GTDB species clades  \n",
    "in `kbase_ke_pangenome.gtdb_species_clade` using normalized name matching.  \n",
    "\n",
    "Approach (following `enigma_contamination_functional_potential` NB02):\n",
    "- Normalize genus name from NCBI species name (remove 's__' prefix, lowercase)\n",
    "- Match at species level first (exact GTDB species name match)\n",
    "- Fall back to genus-level match for unresolved taxa\n",
    "- Report confidence tier: `species_exact`, `genus_proxy`, `unmapped`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all unique species-rank taxa from centrifuge_gold\n",
    "# Using all files (not just overlap) to build a comprehensive bridge table\n",
    "# centrifuge_gold: rank='species', label=taxon name, abundance=relative abundance\n",
    "\n",
    "centrifuge_species = spark.sql(\"\"\"\n",
    "    SELECT label as taxon_name, COUNT(DISTINCT file_id) as n_files,\n",
    "           AVG(abundance) as mean_abundance\n",
    "    FROM nmdc_arkin.centrifuge_gold\n",
    "    WHERE LOWER(rank) = 'species'\n",
    "      AND label IS NOT NULL\n",
    "      AND label != ''\n",
    "    GROUP BY label\n",
    "    ORDER BY n_files DESC\n",
    "\"\"\").toPandas()\n",
    "\n",
    "print(f'Unique species-rank taxa in centrifuge_gold: {len(centrifuge_species)}')\n",
    "print('Most common taxa:')\n",
    "print(centrifuge_species.head(10).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization functions for name matching\n",
    "def norm_species(x: str) -> str:\n",
    "    \"\"\"Normalize NCBI species name for GTDB matching.\n",
    "    NCBI: 'Candidatus Methylotenera versatilis' -> 'methylotenera_versatilis'\n",
    "    GTDB: 's__Methylotenera_versatilis' -> 'methylotenera_versatilis'\n",
    "    \"\"\"\n",
    "    if pd.isna(x) or not str(x).strip():\n",
    "        return ''\n",
    "    x = str(x).strip().lower()\n",
    "    # Remove GTDB prefix if present\n",
    "    x = re.sub(r'^s__', '', x)\n",
    "    # Remove 'candidatus' prefix\n",
    "    x = re.sub(r'^candidatus\\s+', '', x)\n",
    "    # Replace spaces and special chars with underscore\n",
    "    x = re.sub(r'[^a-z0-9]+', '_', x)\n",
    "    return x.strip('_')\n",
    "\n",
    "\n",
    "def norm_genus(x: str) -> str:\n",
    "    \"\"\"Extract and normalize genus from species name.\"\"\"\n",
    "    if pd.isna(x) or not str(x).strip():\n",
    "        return ''\n",
    "    x = str(x).strip().lower()\n",
    "    x = re.sub(r'^s__', '', x)\n",
    "    x = re.sub(r'^g__', '', x)\n",
    "    x = re.sub(r'^candidatus\\s+', '', x)\n",
    "    # Take first word as genus\n",
    "    parts = re.split(r'[^a-z0-9]', x)\n",
    "    genus = parts[0] if parts else ''\n",
    "    return re.sub(r'[^a-z0-9]', '_', genus).strip('_')\n",
    "\n",
    "\n",
    "# Prepare centrifuge taxa with normalized names\n",
    "centrifuge_species['species_norm'] = centrifuge_species['taxon_name'].map(norm_species)\n",
    "centrifuge_species['genus_norm'] = centrifuge_species['taxon_name'].map(norm_genus)\n",
    "centrifuge_species = centrifuge_species[centrifuge_species['species_norm'] != '']\n",
    "print(f'Taxa with non-empty normalized name: {len(centrifuge_species)}')\n",
    "print(centrifuge_species[['taxon_name', 'species_norm', 'genus_norm']].head(10).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GTDB species clades from pangenome\n",
    "# GTDB species names look like: 's__Methylotenera_versatilis'\n",
    "gtdb_species = spark.sql(\"\"\"\n",
    "    SELECT DISTINCT gtdb_species_clade_id, GTDB_species\n",
    "    FROM kbase_ke_pangenome.gtdb_species_clade\n",
    "\"\"\").toPandas()\n",
    "\n",
    "print(f'GTDB species clades: {len(gtdb_species)}')\n",
    "print('Sample GTDB species names:')\n",
    "print(gtdb_species['GTDB_species'].head(10).tolist())\n",
    "\n",
    "# Build normalized species and genus columns for GTDB\n",
    "gtdb_species['species_norm'] = gtdb_species['GTDB_species'].map(norm_species)\n",
    "gtdb_species['genus_norm'] = gtdb_species['GTDB_species'].map(norm_genus)\n",
    "gtdb_species = gtdb_species[gtdb_species['species_norm'] != ''].drop_duplicates()\n",
    "\n",
    "print(f'\\nGTDB species with non-empty normalized name: {len(gtdb_species)}')\n",
    "print(gtdb_species[['GTDB_species', 'species_norm', 'genus_norm']].head(5).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3a: Species-level exact match (normalized NCBI species name == normalized GTDB species name)\n",
    "species_exact = centrifuge_species.merge(\n",
    "    gtdb_species[['gtdb_species_clade_id', 'GTDB_species', 'species_norm']],\n",
    "    on='species_norm',\n",
    "    how='left'\n",
    ")\n",
    "species_exact['mapping_tier'] = species_exact['gtdb_species_clade_id'].apply(\n",
    "    lambda v: 'species_exact' if pd.notna(v) else 'unmapped'\n",
    ")\n",
    "\n",
    "mapped_species = species_exact[species_exact['mapping_tier'] == 'species_exact']\n",
    "still_unmapped = species_exact[species_exact['mapping_tier'] == 'unmapped']\n",
    "\n",
    "print(f'Species-exact matches: {mapped_species[\"taxon_name\"].nunique()}')\n",
    "print(f'Unmapped after species-exact: {still_unmapped[\"taxon_name\"].nunique()}')\n",
    "print(f'Species-exact match rate: {mapped_species[\"taxon_name\"].nunique() / len(centrifuge_species):.1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3b: Genus-level fallback for unmapped taxa\n",
    "# Match on normalized genus; if one GTDB clade per genus, use it (species proxy)\n",
    "# If multiple clades per genus, record as multi_clade_ambiguous\n",
    "\n",
    "# Count GTDB species per genus\n",
    "gtdb_genus_counts = (\n",
    "    gtdb_species.groupby('genus_norm')['gtdb_species_clade_id']\n",
    "    .nunique()\n",
    "    .rename('n_gtdb_clades_for_genus')\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Best representative GTDB clade per genus (use first alphabetically when ambiguous)\n",
    "gtdb_genus_rep = (\n",
    "    gtdb_species.sort_values(['genus_norm', 'GTDB_species'])\n",
    "    .drop_duplicates(subset=['genus_norm'])\n",
    "    [['genus_norm', 'gtdb_species_clade_id', 'GTDB_species']]\n",
    "    .rename(columns={'GTDB_species': 'GTDB_species_representative'})\n",
    ")\n",
    "\n",
    "genus_fallback = still_unmapped[['taxon_name', 'species_norm', 'genus_norm', 'n_files', 'mean_abundance']].merge(\n",
    "    gtdb_genus_rep,\n",
    "    on='genus_norm',\n",
    "    how='left'\n",
    ").merge(\n",
    "    gtdb_genus_counts,\n",
    "    on='genus_norm',\n",
    "    how='left'\n",
    ")\n",
    "genus_fallback['n_gtdb_clades_for_genus'] = genus_fallback['n_gtdb_clades_for_genus'].fillna(0).astype(int)\n",
    "genus_fallback['mapping_tier'] = genus_fallback.apply(\n",
    "    lambda row: (\n",
    "        'genus_proxy_unique' if row['n_gtdb_clades_for_genus'] == 1\n",
    "        else 'genus_proxy_ambiguous' if row['n_gtdb_clades_for_genus'] > 1\n",
    "        else 'unmapped'\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print('Genus fallback mapping tier distribution:')\n",
    "print(genus_fallback['mapping_tier'].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3c: Combine species_exact and genus_fallback into the full bridge table\n",
    "\n",
    "# Standardize columns for concatenation\n",
    "species_exact_out = mapped_species[['taxon_name', 'species_norm', 'genus_norm',\n",
    "                                     'gtdb_species_clade_id', 'GTDB_species',\n",
    "                                     'n_files', 'mean_abundance', 'mapping_tier']].copy()\n",
    "species_exact_out['n_gtdb_clades_for_genus'] = 1\n",
    "species_exact_out['GTDB_species_representative'] = species_exact_out['GTDB_species']\n",
    "\n",
    "genus_fallback_out = genus_fallback.copy()\n",
    "genus_fallback_out['GTDB_species'] = genus_fallback_out.get('GTDB_species_representative',\n",
    "                                                              pd.Series([''] * len(genus_fallback)))\n",
    "\n",
    "# Keep only unmapped from genus_fallback (species-exact already captured)\n",
    "genus_mapped = genus_fallback_out[genus_fallback_out['mapping_tier'] != 'unmapped'].copy()\n",
    "truly_unmapped = genus_fallback_out[genus_fallback_out['mapping_tier'] == 'unmapped'].copy()\n",
    "\n",
    "bridge = pd.concat([\n",
    "    species_exact_out[['taxon_name', 'species_norm', 'genus_norm',\n",
    "                        'gtdb_species_clade_id', 'GTDB_species',\n",
    "                        'n_files', 'mean_abundance', 'mapping_tier', 'n_gtdb_clades_for_genus']],\n",
    "    genus_mapped[['taxon_name', 'species_norm', 'genus_norm',\n",
    "                   'gtdb_species_clade_id', 'GTDB_species_representative',\n",
    "                   'n_files', 'mean_abundance', 'mapping_tier', 'n_gtdb_clades_for_genus']].rename(\n",
    "                   columns={'GTDB_species_representative': 'GTDB_species'}),\n",
    "    truly_unmapped[['taxon_name', 'species_norm', 'genus_norm',\n",
    "                     'n_files', 'mean_abundance', 'mapping_tier', 'n_gtdb_clades_for_genus']],\n",
    "], ignore_index=True)\n",
    "\n",
    "print(f'Total bridge rows: {len(bridge)}')\n",
    "print('Mapping tier distribution:')\n",
    "print(bridge['mapping_tier'].value_counts().to_string())\n",
    "\n",
    "mapped_frac = bridge[bridge['mapping_tier'] != 'unmapped']['taxon_name'].nunique() / len(bridge)\n",
    "print(f'\\nOverall taxon mapping rate: {mapped_frac:.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Bridge Quality Per Sample\n",
    "\n",
    "For each overlap sample (samples with both classifier and metabolomics data),  \n",
    "compute the fraction of community abundance that has been mapped to a GTDB clade.  \n",
    "\n",
    "Quality metric: `bridge_coverage = Σ(abundance_i) for mapped taxa / Σ(abundance_all_taxa)`  \n",
    "Flag samples below 30% bridge coverage for exclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute bridge quality per classifier file\n",
    "# For each file: sum abundance of mapped taxa / total abundance\n",
    "\n",
    "if overlap_sample_ids and file_bridge_df is not None:\n",
    "    # Get species-rank centrifuge data for overlap samples\n",
    "    # Get file_ids for overlap samples first\n",
    "    overlap_clf_files = clf_file_df[clf_file_df['sample_id'].isin(overlap_sample_ids)]['clf_file_id'].tolist()\n",
    "\n",
    "    if overlap_clf_files:\n",
    "        # Create temp view of overlap file IDs\n",
    "        overlap_clf_spark = spark.createDataFrame(\n",
    "            pd.DataFrame({'clf_file_id': overlap_clf_files})\n",
    "        )\n",
    "        overlap_clf_spark.createOrReplaceTempView('overlap_clf_files_tmp')\n",
    "\n",
    "        # Get species-rank rows for overlap files\n",
    "        clf_data = spark.sql(\"\"\"\n",
    "            SELECT c.file_id, c.label as taxon_name, c.abundance\n",
    "            FROM nmdc_arkin.centrifuge_gold c\n",
    "            JOIN overlap_clf_files_tmp o ON c.file_id = o.clf_file_id\n",
    "            WHERE LOWER(c.rank) = 'species'\n",
    "              AND c.label IS NOT NULL AND c.label != ''\n",
    "              AND c.abundance > 0\n",
    "        \"\"\").toPandas()\n",
    "\n",
    "        print(f'Species-rank rows for overlap files: {len(clf_data)}')\n",
    "        print(f'Files in clf_data: {clf_data[\"file_id\"].nunique()}')\n",
    "\n",
    "        # Join with bridge to mark mapped vs unmapped taxa\n",
    "        mapped_taxa = set(bridge[bridge['mapping_tier'] != 'unmapped']['taxon_name'])\n",
    "        clf_data['is_mapped'] = clf_data['taxon_name'].isin(mapped_taxa)\n",
    "\n",
    "        # Compute bridge coverage per file\n",
    "        bridge_quality_file = clf_data.groupby('file_id').agg(\n",
    "            total_abundance=('abundance', 'sum'),\n",
    "            mapped_abundance=('abundance', lambda x: x[clf_data.loc[x.index, 'is_mapped']].sum()),\n",
    "            n_taxa=('taxon_name', 'nunique'),\n",
    "            n_mapped_taxa=('taxon_name', lambda x: x[clf_data.loc[x.index, 'is_mapped']].nunique())\n",
    "        ).reset_index()\n",
    "        bridge_quality_file['bridge_coverage'] = (\n",
    "            bridge_quality_file['mapped_abundance'] / bridge_quality_file['total_abundance']\n",
    "        ).fillna(0)\n",
    "\n",
    "        # Map file_id → sample_id\n",
    "        bridge_quality_file = bridge_quality_file.merge(\n",
    "            clf_file_df[['sample_id', 'clf_file_id']].rename(columns={'clf_file_id': 'file_id'}),\n",
    "            on='file_id', how='left'\n",
    "        )\n",
    "\n",
    "        print(f'\\nBridge quality summary:')\n",
    "        print(bridge_quality_file[['file_id', 'sample_id', 'bridge_coverage',\n",
    "                                   'n_taxa', 'n_mapped_taxa']].describe().to_string())\n",
    "        print(f'\\nSamples with >30% bridge coverage: '\n",
    "              f'{(bridge_quality_file[\"bridge_coverage\"] >= 0.30).sum()} / {len(bridge_quality_file)}')\n",
    "    else:\n",
    "        print('No overlap classifier files found for overlap samples.')\n",
    "        bridge_quality_file = pd.DataFrame()\n",
    "else:\n",
    "    print('No overlap samples or bridge — skipping bridge quality computation.')\n",
    "    bridge_quality_file = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize bridge quality by coverage threshold\n",
    "if len(bridge_quality_file) > 0:\n",
    "    print('Bridge coverage distribution:')\n",
    "    for threshold in [0.10, 0.20, 0.30, 0.50, 0.70]:\n",
    "        n = (bridge_quality_file['bridge_coverage'] >= threshold).sum()\n",
    "        print(f'  >= {threshold:.0%}: {n} / {len(bridge_quality_file)} files '\n",
    "              f'({n/len(bridge_quality_file):.1%})')\n",
    "\n",
    "    # Flag samples\n",
    "    BRIDGE_THRESHOLD = 0.30\n",
    "    bridge_quality_file['passes_bridge_qc'] = bridge_quality_file['bridge_coverage'] >= BRIDGE_THRESHOLD\n",
    "    n_pass = bridge_quality_file['passes_bridge_qc'].sum()\n",
    "    print(f'\\nSamples passing QC (>={BRIDGE_THRESHOLD:.0%}): {n_pass}')\n",
    "    print(f'Samples failing QC (<{BRIDGE_THRESHOLD:.0%}): {len(bridge_quality_file) - n_pass}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Save Outputs and Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save bridge table\n",
    "bridge_path = os.path.join(DATA_DIR, 'taxon_bridge.tsv')\n",
    "bridge.to_csv(bridge_path, sep='\\t', index=False)\n",
    "print(f'Saved: data/taxon_bridge.tsv ({len(bridge)} rows)')\n",
    "\n",
    "# Save bridge quality\n",
    "if len(bridge_quality_file) > 0:\n",
    "    bq_path = os.path.join(DATA_DIR, 'bridge_quality.csv')\n",
    "    bridge_quality_file.to_csv(bq_path, index=False)\n",
    "    print(f'Saved: data/bridge_quality.csv ({len(bridge_quality_file)} rows)')\n",
    "\n",
    "# Save/update sample inventory (replaces empty NB01 version if overlap found)\n",
    "if len(inventory) > 0:\n",
    "    inv_path = os.path.join(DATA_DIR, 'nmdc_sample_inventory.csv')\n",
    "    inventory.to_csv(inv_path, index=False)\n",
    "    print(f'Saved: data/nmdc_sample_inventory.csv ({len(inventory)} rows, '\n",
    "          f'{inventory[\"sample_id\"].nunique()} unique samples)')\n",
    "\n",
    "# Save file→sample bridge\n",
    "if file_bridge_df is not None and len(file_bridge_df) > 0:\n",
    "    fb_path = os.path.join(DATA_DIR, 'sample_file_bridge.csv')\n",
    "    file_bridge_df.to_csv(fb_path, index=False)\n",
    "    print(f'Saved: data/sample_file_bridge.csv ({len(file_bridge_df)} rows)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure: Bridge quality distribution and mapping tier breakdown\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "fig.suptitle('Taxonomy Bridge Quality', fontsize=14)\n",
    "\n",
    "# Panel 1: Bridge coverage histogram\n",
    "ax = axes[0]\n",
    "if len(bridge_quality_file) > 0:\n",
    "    ax.hist(bridge_quality_file['bridge_coverage'], bins=30,\n",
    "            color='#4C9BE8', edgecolor='white')\n",
    "    ax.axvline(0.30, color='red', linestyle='--', label='30% threshold')\n",
    "    ax.set_xlabel('Bridge coverage (fraction of abundance mapped)')\n",
    "    ax.set_ylabel('Number of samples')\n",
    "    ax.set_title('Per-sample bridge coverage')\n",
    "    ax.legend()\n",
    "else:\n",
    "    ax.text(0.5, 0.5, 'No bridge quality data', ha='center', va='center',\n",
    "            transform=ax.transAxes)\n",
    "\n",
    "# Panel 2: Mapping tier breakdown (n taxa in each tier, weighted by n_files)\n",
    "ax2 = axes[1]\n",
    "if len(bridge) > 0:\n",
    "    tier_counts = bridge['mapping_tier'].value_counts()\n",
    "    colors = {\n",
    "        'species_exact': '#2ecc71',\n",
    "        'genus_proxy_unique': '#f39c12',\n",
    "        'genus_proxy_ambiguous': '#e67e22',\n",
    "        'unmapped': '#e74c3c'\n",
    "    }\n",
    "    bar_colors = [colors.get(t, '#95a5a6') for t in tier_counts.index]\n",
    "    ax2.bar(range(len(tier_counts)), tier_counts.values,\n",
    "            color=bar_colors, edgecolor='white')\n",
    "    ax2.set_xticks(range(len(tier_counts)))\n",
    "    ax2.set_xticklabels(tier_counts.index, rotation=30, ha='right', fontsize=9)\n",
    "    ax2.set_ylabel('Number of unique taxa')\n",
    "    ax2.set_title('Taxon mapping tier distribution')\n",
    "    for i, val in enumerate(tier_counts.values):\n",
    "        ax2.text(i, val + 0.5, str(val), ha='center', fontsize=9)\n",
    "else:\n",
    "    ax2.text(0.5, 0.5, 'No bridge data', ha='center', va='center', transform=ax2.transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig_path = os.path.join(FIGURES_DIR, 'bridge_quality_distribution.png')\n",
    "plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f'Saved: figures/bridge_quality_distribution.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary and Decisions for NB03\n",
    "\n",
    "| Question | Finding |\n",
    "|---|---|\n",
    "| file→sample bridge table | ??? (fill after running Part 1) |\n",
    "| Samples with both classifier + metabolomics | ??? |\n",
    "| Species-exact GTDB matches | ??? of ??? centrifuge taxa |\n",
    "| Genus-proxy unique matches | ??? |\n",
    "| Genus-proxy ambiguous | ??? |\n",
    "| Overall taxon mapping rate | ???% |\n",
    "| Samples passing 30% bridge QC | ??? |\n",
    "| Classifier used for bridge | centrifuge_gold (61.3% species-rank, best from NB01) |\n",
    "\n",
    "**Decision for NB03**:  \n",
    "Use samples passing 30% bridge QC for community-weighted pathway completeness computation.  \n",
    "Use `species_exact` + `genus_proxy_unique` clades for community weighting in NB03.  \n",
    "Exclude `genus_proxy_ambiguous` from community weighting (or use abundance-weighted mean across all matching clades — document choice)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
