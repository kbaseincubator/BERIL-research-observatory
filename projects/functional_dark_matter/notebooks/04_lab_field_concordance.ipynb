{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB04: Lab-Field Concordance & NMDC Validation\n",
    "\n",
    "**Requires BERDL JupyterHub** — `get_spark_session()` is only available in JupyterHub kernels.\n",
    "\n",
    "## Purpose\n",
    "\n",
    "Test whether dark genes' lab fitness conditions predict the environmental contexts where\n",
    "they appear in nature. Two independent tests:\n",
    "\n",
    "1. **Lab-field concordance**: Pre-registered mapping from FB experiment groups to expected\n",
    "   environmental categories, then test if carrier genomes are enriched in the predicted environments\n",
    "2. **NMDC independent validation**: For taxa carrying dark genes, check if their abundance in\n",
    "   NMDC metagenomic samples correlates with abiotic variables matching the lab fitness conditions\n",
    "\n",
    "## Inputs\n",
    "\n",
    "- `data/dark_genes_only.tsv` from NB01\n",
    "- `data/carrier_noncarrier_tests.tsv` from NB03\n",
    "- `data/carrier_genome_map.tsv` from NB03\n",
    "- `data/biogeographic_profiles.tsv` from NB03\n",
    "\n",
    "## Outputs\n",
    "\n",
    "- `data/lab_field_concordance.tsv` — per-cluster concordance test results\n",
    "- `data/nmdc_validation.tsv` — NMDC abiotic correlation results\n",
    "- `figures/fig11_concordance_matrix.png`\n",
    "- `figures/fig12_nmdc_correlations.png`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T17:35:32.049211Z",
     "iopub.status.busy": "2026-02-25T17:35:32.049091Z",
     "iopub.status.idle": "2026-02-25T17:35:34.259673Z",
     "shell.execute_reply": "2026-02-25T17:35:34.258823Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project dir: /home/aparkin/BERIL-research-observatory/projects/functional_dark_matter\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# Path setup\n",
    "if os.path.basename(os.getcwd()) == 'notebooks':\n",
    "    PROJECT_DIR = os.path.dirname(os.getcwd())\n",
    "else:\n",
    "    PROJECT_DIR = os.getcwd()\n",
    "    _d = PROJECT_DIR\n",
    "    while _d != '/':\n",
    "        if os.path.exists(os.path.join(_d, 'PROJECT.md')):\n",
    "            break\n",
    "        _d = os.path.dirname(_d)\n",
    "\n",
    "DATA_DIR = os.path.join(PROJECT_DIR, 'data')\n",
    "FIG_DIR = os.path.join(PROJECT_DIR, 'figures')\n",
    "os.makedirs(FIG_DIR, exist_ok=True)\n",
    "\n",
    "spark = get_spark_session()\n",
    "print(f'Project dir: {PROJECT_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Pre-Registered Condition-Environment Mapping\n",
    "\n",
    "Define the mapping from FB experiment condition classes to expected environmental\n",
    "categories **before** looking at the biogeographic data. This prevents post-hoc\n",
    "cherry-picking of associations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T17:35:34.309677Z",
     "iopub.status.busy": "2026-02-25T17:35:34.309067Z",
     "iopub.status.idle": "2026-02-25T17:35:34.317045Z",
     "shell.execute_reply": "2026-02-25T17:35:34.316108Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-registered condition-environment mapping:\n",
      "  stress -> ['contaminated', 'soil_sediment', 'wastewater_engineered']\n",
      "  carbon source -> ['soil_sediment', 'freshwater', 'plant_associated']\n",
      "    NMDC abiotic: ['annotations_tot_org_carb_has_numeric_value', 'annotations_diss_org_carb_has_numeric_value']\n",
      "  nitrogen source -> ['soil_sediment', 'freshwater', 'wastewater_engineered']\n",
      "    NMDC abiotic: ['annotations_tot_nitro_content_has_numeric_value', 'annotations_ammonium_has_numeric_value', 'annotations_ammonium_nitrogen_has_numeric_value']\n",
      "  pH -> ['soil_sediment', 'extreme', 'freshwater']\n",
      "    NMDC abiotic: ['annotations_ph']\n",
      "  motility -> ['soil_sediment', 'freshwater', 'plant_associated']\n",
      "  anaerobic -> ['soil_sediment', 'freshwater', 'animal_associated']\n",
      "    NMDC abiotic: ['annotations_diss_oxygen_has_numeric_value']\n"
     ]
    }
   ],
   "source": [
    "# Pre-registered mapping: FB condition class -> expected environment categories\n",
    "# Each condition class maps to one or more env_category values from NB03's classification\n",
    "CONDITION_ENV_MAP = {\n",
    "    'stress': {\n",
    "        'expected_envs': ['contaminated', 'soil_sediment', 'wastewater_engineered'],\n",
    "        'description': 'Stress conditions (metals, oxidative, osmotic) -> contaminated/variable environments',\n",
    "        'nmdc_abiotic': [],  # stress is heterogeneous, no single abiotic variable\n",
    "    },\n",
    "    'carbon source': {\n",
    "        'expected_envs': ['soil_sediment', 'freshwater', 'plant_associated'],\n",
    "        'description': 'Carbon utilization -> carbon-rich environments (soil, freshwater, rhizosphere)',\n",
    "        'nmdc_abiotic': ['annotations_tot_org_carb_has_numeric_value',\n",
    "                         'annotations_diss_org_carb_has_numeric_value'],\n",
    "    },\n",
    "    'nitrogen source': {\n",
    "        'expected_envs': ['soil_sediment', 'freshwater', 'wastewater_engineered'],\n",
    "        'description': 'Nitrogen utilization -> nitrogen-variable environments',\n",
    "        'nmdc_abiotic': ['annotations_tot_nitro_content_has_numeric_value',\n",
    "                         'annotations_ammonium_has_numeric_value',\n",
    "                         'annotations_ammonium_nitrogen_has_numeric_value'],\n",
    "    },\n",
    "    'pH': {\n",
    "        'expected_envs': ['soil_sediment', 'extreme', 'freshwater'],\n",
    "        'description': 'pH stress -> pH-variable environments',\n",
    "        'nmdc_abiotic': ['annotations_ph'],\n",
    "    },\n",
    "    'motility': {\n",
    "        'expected_envs': ['soil_sediment', 'freshwater', 'plant_associated'],\n",
    "        'description': 'Motility -> structured environments requiring chemotaxis',\n",
    "        'nmdc_abiotic': [],\n",
    "    },\n",
    "    'anaerobic': {\n",
    "        'expected_envs': ['soil_sediment', 'freshwater', 'animal_associated'],\n",
    "        'description': 'Anaerobic growth -> low-oxygen environments',\n",
    "        'nmdc_abiotic': ['annotations_diss_oxygen_has_numeric_value'],\n",
    "    },\n",
    "}\n",
    "\n",
    "print('Pre-registered condition-environment mapping:')\n",
    "for cond, info in CONDITION_ENV_MAP.items():\n",
    "    print(f'  {cond} -> {info[\"expected_envs\"]}')\n",
    "    if info['nmdc_abiotic']:\n",
    "        print(f'    NMDC abiotic: {info[\"nmdc_abiotic\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T17:35:34.319327Z",
     "iopub.status.busy": "2026-02-25T17:35:34.319059Z",
     "iopub.status.idle": "2026-02-25T17:35:34.602099Z",
     "shell.execute_reply": "2026-02-25T17:35:34.601284Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carrier test results: 151 clusters\n",
      "Carrier genome map: 8,139 pairs\n",
      "Species profiles: 31 species\n",
      "\n",
      "Clusters with mapped condition class: 121\n",
      "top_condition_class\n",
      "stress             65\n",
      "carbon source      38\n",
      "nitrogen source    11\n",
      "pH                  4\n",
      "motility            2\n",
      "anaerobic           1\n"
     ]
    }
   ],
   "source": [
    "# Load NB03 results\n",
    "dark = pd.read_csv(os.path.join(DATA_DIR, 'dark_genes_only.tsv'), sep='\\t', low_memory=False)\n",
    "carrier_tests = pd.read_csv(os.path.join(DATA_DIR, 'carrier_noncarrier_tests.tsv'), sep='\\t')\n",
    "carrier_map = pd.read_csv(os.path.join(DATA_DIR, 'carrier_genome_map.tsv'), sep='\\t')\n",
    "profiles = pd.read_csv(os.path.join(DATA_DIR, 'biogeographic_profiles.tsv'), sep='\\t')\n",
    "\n",
    "print(f'Carrier test results: {len(carrier_tests)} clusters')\n",
    "print(f'Carrier genome map: {len(carrier_map):,} pairs')\n",
    "print(f'Species profiles: {len(profiles)} species')\n",
    "\n",
    "# Filter to clusters with condition class in our pre-registered mapping\n",
    "mapped_conditions = list(CONDITION_ENV_MAP.keys())\n",
    "concordance_clusters = carrier_tests[\n",
    "    carrier_tests['top_condition_class'].isin(mapped_conditions)\n",
    "].copy()\n",
    "print(f'\\nClusters with mapped condition class: {len(concordance_clusters)}')\n",
    "print(concordance_clusters['top_condition_class'].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Lab-Field Concordance Test\n",
    "\n",
    "For each cluster with a mapped condition class AND environment data for carriers,\n",
    "test whether carriers are enriched in the predicted environment categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T17:35:34.604964Z",
     "iopub.status.busy": "2026-02-25T17:35:34.604842Z",
     "iopub.status.idle": "2026-02-25T17:35:37.878296Z",
     "shell.execute_reply": "2026-02-25T17:35:37.877224Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env data for 1,131 genomes in 10 species\n"
     ]
    }
   ],
   "source": [
    "# Re-extract environment data for carrier and non-carrier genomes\n",
    "# We need the raw env categories per genome, not just the top carrier env from NB03\n",
    "\n",
    "# Load all genomes in target species (from NB03 approach)\n",
    "target_species = concordance_clusters['gtdb_species_clade_id'].unique().tolist()\n",
    "species_sdf = spark.createDataFrame(\n",
    "    [(str(s),) for s in target_species],\n",
    "    ['gtdb_species_clade_id']\n",
    ")\n",
    "species_sdf.createOrReplaceTempView('concordance_species')\n",
    "\n",
    "all_genomes = spark.sql(\"\"\"\n",
    "    SELECT g.genome_id, g.gtdb_species_clade_id, g.ncbi_biosample_id\n",
    "    FROM kbase_ke_pangenome.genome g\n",
    "    JOIN concordance_species cs ON g.gtdb_species_clade_id = cs.gtdb_species_clade_id\n",
    "\"\"\").toPandas()\n",
    "\n",
    "# Pivot NCBI env for these genomes\n",
    "genome_ids_sdf = spark.createDataFrame(\n",
    "    [(str(gid), str(bs)) for gid, bs in zip(\n",
    "        all_genomes['genome_id'].tolist(),\n",
    "        all_genomes['ncbi_biosample_id'].fillna('').tolist()\n",
    "    ) if bs],\n",
    "    ['genome_id', 'accession']\n",
    ")\n",
    "genome_ids_sdf.createOrReplaceTempView('concordance_genomes')\n",
    "\n",
    "env_data = spark.sql(\"\"\"\n",
    "    SELECT cg.genome_id,\n",
    "           MAX(CASE WHEN ne.harmonized_name = 'isolation_source' THEN ne.content END) as isolation_source,\n",
    "           MAX(CASE WHEN ne.harmonized_name = 'env_broad_scale' THEN ne.content END) as env_broad_scale,\n",
    "           MAX(CASE WHEN ne.harmonized_name = 'host' THEN ne.content END) as host\n",
    "    FROM concordance_genomes cg\n",
    "    JOIN kbase_ke_pangenome.ncbi_env ne ON cg.accession = ne.accession\n",
    "    GROUP BY cg.genome_id\n",
    "\"\"\").toPandas()\n",
    "\n",
    "print(f'Env data for {len(env_data):,} genomes in {len(target_species)} species')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T17:35:37.880927Z",
     "iopub.status.busy": "2026-02-25T17:35:37.880667Z",
     "iopub.status.idle": "2026-02-25T17:35:38.304273Z",
     "shell.execute_reply": "2026-02-25T17:35:38.303318Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment lookup: 1,131 genomes\n",
      "env_category\n",
      "other_unknown            442\n",
      "human_associated         427\n",
      "human_clinical           142\n",
      "soil_sediment             37\n",
      "animal_associated         25\n",
      "plant_associated          23\n",
      "freshwater                14\n",
      "wastewater_engineered     12\n",
      "marine_saline              6\n",
      "contaminated               3\n"
     ]
    }
   ],
   "source": [
    "# Classify environments (same function as NB03)\n",
    "def classify_environment(row):\n",
    "    source = str(row.get('isolation_source', '')).lower()\n",
    "    host = str(row.get('host', '')).lower()\n",
    "    if any(kw in source for kw in ['contaminated', 'polluted', 'mining', 'acid mine',\n",
    "                                    'industrial', 'heavy metal', 'uranium', 'chromium']):\n",
    "        return 'contaminated'\n",
    "    if any(kw in source for kw in ['blood', 'sputum', 'urine', 'wound', 'clinical',\n",
    "                                    'patient', 'hospital', 'human']):\n",
    "        return 'human_clinical'\n",
    "    if 'homo sapiens' in host or 'human' in host:\n",
    "        return 'human_associated'\n",
    "    if any(kw in source for kw in ['animal', 'bovine', 'chicken', 'pig', 'cattle',\n",
    "                                    'poultry', 'feces', 'gut', 'intestin', 'rumen']):\n",
    "        return 'animal_associated'\n",
    "    if any(kw in source for kw in ['soil', 'rhizosphere', 'root', 'compost', 'peat',\n",
    "                                    'agricultural', 'sediment']):\n",
    "        return 'soil_sediment'\n",
    "    if any(kw in source for kw in ['marine', 'ocean', 'sea', 'seawater', 'coastal',\n",
    "                                    'saline', 'brackish', 'salt', 'brine', 'hypersaline']):\n",
    "        return 'marine_saline'\n",
    "    if any(kw in source for kw in ['freshwater', 'lake', 'river', 'pond', 'stream',\n",
    "                                    'groundwater', 'spring', 'aquifer']):\n",
    "        return 'freshwater'\n",
    "    if any(kw in source for kw in ['plant', 'leaf', 'stem', 'flower', 'seed',\n",
    "                                    'phyllosphere', 'endophyte']):\n",
    "        return 'plant_associated'\n",
    "    if any(kw in source for kw in ['wastewater', 'sewage', 'activated sludge',\n",
    "                                    'bioreactor', 'ferment']):\n",
    "        return 'wastewater_engineered'\n",
    "    if any(kw in source for kw in ['hot spring', 'hydrothermal', 'volcanic',\n",
    "                                    'permafrost', 'acidic', 'alkaline']):\n",
    "        return 'extreme'\n",
    "    return 'other_unknown'\n",
    "\n",
    "env_data['env_category'] = env_data.apply(classify_environment, axis=1)\n",
    "env_lookup = env_data.set_index('genome_id')['env_category'].to_dict()\n",
    "print(f'Environment lookup: {len(env_lookup):,} genomes')\n",
    "print(env_data['env_category'].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T17:35:38.306489Z",
     "iopub.status.busy": "2026-02-25T17:35:38.306371Z",
     "iopub.status.idle": "2026-02-25T17:35:38.352869Z",
     "shell.execute_reply": "2026-02-25T17:35:38.352006Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concordance tests completed: 47 clusters\n",
      "  By condition class:\n",
      "top_condition_class\n",
      "carbon source      23\n",
      "nitrogen source     9\n",
      "stress              8\n",
      "pH                  4\n",
      "motility            2\n",
      "anaerobic           1\n"
     ]
    }
   ],
   "source": [
    "# Build carrier sets from carrier_map\n",
    "carrier_sets = carrier_map.groupby('gene_cluster_id')['genome_id'].apply(set).to_dict()\n",
    "genomes_per_species = all_genomes.groupby('gtdb_species_clade_id')['genome_id'].apply(set).to_dict()\n",
    "\n",
    "# Run concordance test for each cluster\n",
    "concordance_results = []\n",
    "\n",
    "for _, row in concordance_clusters.iterrows():\n",
    "    cid = row['gene_cluster_id']\n",
    "    sid = row['gtdb_species_clade_id']\n",
    "    cond_class = row['top_condition_class']\n",
    "    expected_envs = CONDITION_ENV_MAP[cond_class]['expected_envs']\n",
    "    \n",
    "    all_gids = genomes_per_species.get(sid, set())\n",
    "    carrier_gids = carrier_sets.get(cid, set()) & all_gids\n",
    "    noncarrier_gids = all_gids - carrier_gids\n",
    "    \n",
    "    # Get env categories for carriers and non-carriers\n",
    "    carrier_envs = [env_lookup.get(g) for g in carrier_gids if g in env_lookup]\n",
    "    noncarrier_envs = [env_lookup.get(g) for g in noncarrier_gids if g in env_lookup]\n",
    "    \n",
    "    if len(carrier_envs) < 3 or len(noncarrier_envs) < 3:\n",
    "        continue\n",
    "    \n",
    "    # Count carriers in expected vs non-expected environments\n",
    "    a = sum(1 for e in carrier_envs if e in expected_envs)  # carrier + expected\n",
    "    b = sum(1 for e in carrier_envs if e not in expected_envs)  # carrier + not expected\n",
    "    c = sum(1 for e in noncarrier_envs if e in expected_envs)  # non-carrier + expected\n",
    "    d = sum(1 for e in noncarrier_envs if e not in expected_envs)  # non-carrier + not expected\n",
    "    \n",
    "    if a + c == 0:  # no genomes in expected environments at all\n",
    "        continue\n",
    "    \n",
    "    odds_ratio, p_val = stats.fisher_exact([[a, b], [c, d]])\n",
    "    \n",
    "    concordance_results.append({\n",
    "        'gene_cluster_id': cid,\n",
    "        'gtdb_species_clade_id': sid,\n",
    "        'orgId': row.get('orgId', ''),\n",
    "        'locusId': row.get('locusId', ''),\n",
    "        'desc': row.get('desc', ''),\n",
    "        'top_condition_class': cond_class,\n",
    "        'max_abs_fit': row.get('max_abs_fit', np.nan),\n",
    "        'expected_envs': '|'.join(expected_envs),\n",
    "        'n_carriers': len(carrier_envs),\n",
    "        'n_noncarriers': len(noncarrier_envs),\n",
    "        'carrier_in_expected': a,\n",
    "        'carrier_pct_expected': a / (a + b) * 100 if (a + b) > 0 else 0,\n",
    "        'noncarrier_in_expected': c,\n",
    "        'noncarrier_pct_expected': c / (c + d) * 100 if (c + d) > 0 else 0,\n",
    "        'odds_ratio': odds_ratio,\n",
    "        'p_value': p_val,\n",
    "        'is_concordant': a / (a + b) > c / (c + d) if (a + b) > 0 and (c + d) > 0 else False,\n",
    "    })\n",
    "\n",
    "conc_df = pd.DataFrame(concordance_results)\n",
    "print(f'Concordance tests completed: {len(conc_df)} clusters')\n",
    "print(f'  By condition class:')\n",
    "print(conc_df['top_condition_class'].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T17:35:38.354998Z",
     "iopub.status.busy": "2026-02-25T17:35:38.354883Z",
     "iopub.status.idle": "2026-02-25T17:35:38.365081Z",
     "shell.execute_reply": "2026-02-25T17:35:38.364293Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concordance summary:\n",
      "  Total tested: 47\n",
      "  Concordant (carrier enriched in expected env): 29 (61.7%)\n",
      "  Significant (FDR < 0.05): 1\n",
      "  Significant AND concordant: 0\n",
      "\n",
      "Per condition class:\n",
      "  carbon source: 23 tested, 15 concordant (65%), 0 sig\n",
      "  stress: 8 tested, 2 concordant (25%), 1 sig\n",
      "  nitrogen source: 9 tested, 7 concordant (78%), 0 sig\n",
      "  pH: 4 tested, 4 concordant (100%), 0 sig\n",
      "  anaerobic: 1 tested, 1 concordant (100%), 0 sig\n",
      "  motility: 2 tested, 0 concordant (0%), 0 sig\n"
     ]
    }
   ],
   "source": [
    "# Apply BH-FDR correction\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "if len(conc_df) > 0:\n",
    "    _, fdr, _, _ = multipletests(conc_df['p_value'], method='fdr_bh')\n",
    "    conc_df['fdr'] = fdr\n",
    "    \n",
    "    n_sig = (fdr < 0.05).sum()\n",
    "    n_concordant = conc_df['is_concordant'].sum()\n",
    "    n_sig_concordant = ((fdr < 0.05) & conc_df['is_concordant']).sum()\n",
    "    \n",
    "    print(f'Concordance summary:')\n",
    "    print(f'  Total tested: {len(conc_df)}')\n",
    "    print(f'  Concordant (carrier enriched in expected env): {n_concordant} ({n_concordant/len(conc_df)*100:.1f}%)')\n",
    "    print(f'  Significant (FDR < 0.05): {n_sig}')\n",
    "    print(f'  Significant AND concordant: {n_sig_concordant}')\n",
    "    \n",
    "    # Per condition class\n",
    "    print(f'\\nPer condition class:')\n",
    "    for cond in conc_df['top_condition_class'].unique():\n",
    "        sub = conc_df[conc_df['top_condition_class'] == cond]\n",
    "        n_c = sub['is_concordant'].sum()\n",
    "        n_s = (sub['fdr'] < 0.05).sum()\n",
    "        print(f'  {cond}: {len(sub)} tested, {n_c} concordant ({n_c/len(sub)*100:.0f}%), {n_s} sig')\n",
    "else:\n",
    "    print('No clusters could be tested for concordance')\n",
    "    conc_df['fdr'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T17:35:38.366999Z",
     "iopub.status.busy": "2026-02-25T17:35:38.366887Z",
     "iopub.status.idle": "2026-02-25T17:35:38.375862Z",
     "shell.execute_reply": "2026-02-25T17:35:38.375150Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Significant concordant clusters (FDR < 0.2): 6\n",
      "            gene_cluster_id           orgId        locusId                  desc top_condition_class  max_abs_fit                                   expected_envs  carrier_pct_expected  noncarrier_pct_expected  odds_ratio   p_value       fdr\n",
      "24  NZ_JACVAH010000015.1_20  pseudo5_N2C3_1    AO356_12450  hypothetical protein       carbon source     2.076086       soil_sediment|freshwater|plant_associated             62.500000                 0.000000         inf  0.009050  0.092663\n",
      "23  NZ_JACVAH010000006.1_10  pseudo5_N2C3_1    AO356_25185  hypothetical protein           anaerobic     2.723784      soil_sediment|freshwater|animal_associated             55.555556                 0.000000         inf  0.029412  0.178201\n",
      "38     NZ_PPRZ01000025.1_37  pseudo5_N2C3_1    AO356_24150  hypothetical protein     nitrogen source     3.012796  soil_sediment|freshwater|wastewater_engineered             55.555556                 0.000000         inf  0.029412  0.178201\n",
      "5        NZ_CP012831.1_2249  pseudo5_N2C3_1    AO356_11255  hypothetical protein     nitrogen source     3.359544  soil_sediment|freshwater|wastewater_engineered             80.000000                 8.333333   44.000000  0.009858  0.092663\n",
      "4   NZ_CABGWQ010000027.1_25            Koxy  BWI76_RS15525  hypothetical protein       carbon source     2.335444       soil_sediment|freshwater|plant_associated             14.285714                 2.472527    6.574074  0.004383  0.068662\n",
      "41     NZ_VNQT01000029.1_94            Koxy  BWI76_RS15535  hypothetical protein       carbon source     2.171705       soil_sediment|freshwater|plant_associated             14.285714                 2.472527    6.574074  0.004383  0.068662\n"
     ]
    }
   ],
   "source": [
    "# Show top concordant results\n",
    "if len(conc_df) > 0:\n",
    "    sig_conc = conc_df[(conc_df['fdr'] < 0.2) & conc_df['is_concordant']].sort_values('odds_ratio', ascending=False)\n",
    "    if len(sig_conc) > 0:\n",
    "        print(f'Significant concordant clusters (FDR < 0.2): {len(sig_conc)}')\n",
    "        cols = ['gene_cluster_id', 'orgId', 'locusId', 'desc', 'top_condition_class',\n",
    "                'max_abs_fit', 'expected_envs', 'carrier_pct_expected', 'noncarrier_pct_expected',\n",
    "                'odds_ratio', 'p_value', 'fdr']\n",
    "        print(sig_conc[cols].head(20).to_string())\n",
    "    else:\n",
    "        print('No significant concordant clusters at FDR < 0.2')\n",
    "        print('\\nTop 10 by odds ratio (concordant only):')\n",
    "        top = conc_df[conc_df['is_concordant']].sort_values('odds_ratio', ascending=False)\n",
    "        cols = ['gene_cluster_id', 'orgId', 'desc', 'top_condition_class',\n",
    "                'carrier_pct_expected', 'noncarrier_pct_expected', 'odds_ratio', 'p_value']\n",
    "        print(top[cols].head(10).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T17:35:38.377905Z",
     "iopub.status.busy": "2026-02-25T17:35:38.377793Z",
     "iopub.status.idle": "2026-02-25T17:35:38.663331Z",
     "shell.execute_reply": "2026-02-25T17:35:38.662420Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved fig11_concordance_matrix.png\n"
     ]
    }
   ],
   "source": [
    "# Figure 11: Concordance matrix — condition class vs observed enrichment direction\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Panel A: Concordance rate by condition class\n",
    "ax = axes[0]\n",
    "if len(conc_df) > 0:\n",
    "    cond_summary = conc_df.groupby('top_condition_class').agg(\n",
    "        n_tested=('gene_cluster_id', 'count'),\n",
    "        n_concordant=('is_concordant', 'sum'),\n",
    "        n_significant=('fdr', lambda x: (x < 0.05).sum()),\n",
    "        mean_carrier_pct=('carrier_pct_expected', 'mean'),\n",
    "        mean_noncarrier_pct=('noncarrier_pct_expected', 'mean'),\n",
    "    ).reset_index()\n",
    "    cond_summary['concordance_rate'] = cond_summary['n_concordant'] / cond_summary['n_tested'] * 100\n",
    "    cond_summary = cond_summary.sort_values('concordance_rate', ascending=True)\n",
    "    \n",
    "    y_pos = range(len(cond_summary))\n",
    "    bars = ax.barh(y_pos, cond_summary['concordance_rate'], color='steelblue', alpha=0.7)\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels([f\"{c} (n={n})\" for c, n in zip(cond_summary['top_condition_class'], cond_summary['n_tested'])])\n",
    "    ax.set_xlabel('Concordance rate (%)')\n",
    "    ax.set_title('Lab-Field Concordance Rate\\nby Condition Class')\n",
    "    ax.axvline(50, color='red', linestyle='--', alpha=0.5, label='Chance (50%)')\n",
    "    ax.legend()\n",
    "    ax.set_xlim(0, 100)\n",
    "\n",
    "# Panel B: Carrier vs non-carrier % in expected environment\n",
    "ax = axes[1]\n",
    "if len(conc_df) > 0:\n",
    "    for cond in conc_df['top_condition_class'].unique():\n",
    "        sub = conc_df[conc_df['top_condition_class'] == cond]\n",
    "        ax.scatter(sub['noncarrier_pct_expected'], sub['carrier_pct_expected'],\n",
    "                  label=cond, alpha=0.6, s=40)\n",
    "    \n",
    "    ax.plot([0, 100], [0, 100], 'k--', alpha=0.3, label='Equal')\n",
    "    ax.set_xlabel('Non-carrier % in expected environment')\n",
    "    ax.set_ylabel('Carrier % in expected environment')\n",
    "    ax.set_title('Carrier vs Non-Carrier\\nEnvironmental Enrichment')\n",
    "    ax.legend(fontsize=7, loc='upper left')\n",
    "    ax.set_xlim(-5, 105)\n",
    "    ax.set_ylim(-5, 105)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIG_DIR, 'fig11_concordance_matrix.png'), dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print('Saved fig11_concordance_matrix.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: NMDC Independent Validation\n",
    "\n",
    "Use NMDC metagenomic data as an independent check. For taxa that carry dark genes\n",
    "in the pangenome, we infer their abundance in NMDC samples via the taxonomy profile,\n",
    "then correlate with abiotic measurements.\n",
    "\n",
    "This is **community-level** validation at **genus** resolution — not gene-level.\n",
    "Results should be framed as \"consistent with\" or \"independently corroborated by\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T17:35:38.666003Z",
     "iopub.status.busy": "2026-02-25T17:35:38.665881Z",
     "iopub.status.idle": "2026-02-25T17:35:48.639552Z",
     "shell.execute_reply": "2026-02-25T17:35:48.638585Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading NMDC tables...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  taxonomy_features: 6365 samples x 3493 columns\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  abiotic_features: 13847 samples x 22 columns\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  taxonomy_dim: 2594787 taxa\n",
      "  Taxon columns: 3492\n",
      "  Samples with both taxonomy + abiotic: 6365\n"
     ]
    }
   ],
   "source": [
    "# Load NMDC tables\n",
    "print('Loading NMDC tables...')\n",
    "tax_features = spark.sql(\"SELECT * FROM nmdc_arkin.taxonomy_features\").toPandas()\n",
    "print(f'  taxonomy_features: {tax_features.shape[0]} samples x {tax_features.shape[1]} columns')\n",
    "\n",
    "abiotic = spark.sql(\"SELECT * FROM nmdc_arkin.abiotic_features\").toPandas()\n",
    "print(f'  abiotic_features: {abiotic.shape[0]} samples x {abiotic.shape[1]} columns')\n",
    "\n",
    "tax_dim = spark.sql(\"SELECT * FROM nmdc_arkin.taxonomy_dim\").toPandas()\n",
    "print(f'  taxonomy_dim: {tax_dim.shape[0]} taxa')\n",
    "\n",
    "# Identify taxon columns (numeric taxids as column names)\n",
    "taxon_cols = [c for c in tax_features.columns if c != 'sample_id' and c.isdigit()]\n",
    "print(f'  Taxon columns: {len(taxon_cols)}')\n",
    "\n",
    "# Overlap between taxonomy and abiotic samples\n",
    "shared_samples = set(tax_features['sample_id']) & set(abiotic['sample_id'])\n",
    "print(f'  Samples with both taxonomy + abiotic: {len(shared_samples)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T17:35:48.641711Z",
     "iopub.status.busy": "2026-02-25T17:35:48.641589Z",
     "iopub.status.idle": "2026-02-25T17:35:52.179118Z",
     "shell.execute_reply": "2026-02-25T17:35:52.177962Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carrier species -> genus mapping: 0\n",
      "Unique genera: 0\n",
      "\n",
      "Top genera:\n",
      "Series([], )\n"
     ]
    }
   ],
   "source": [
    "# Map dark gene carrier genera to NMDC taxon columns\n",
    "# Two-tier approach (following prophage_ecology pattern):\n",
    "# 1. Get GTDB genus for each dark gene carrier species\n",
    "# 2. Map NMDC taxid columns to genera via taxonomy_dim\n",
    "# 3. Match by genus name\n",
    "\n",
    "# Step 1: Extract genus from carrier species\n",
    "carrier_species = concordance_clusters['gtdb_species_clade_id'].unique()\n",
    "\n",
    "# Get GTDB genus from pangenome metadata\n",
    "sp_sdf = spark.createDataFrame(\n",
    "    [(str(s),) for s in carrier_species],\n",
    "    ['gtdb_species_clade_id']\n",
    ")\n",
    "sp_sdf.createOrReplaceTempView('carrier_species_view')\n",
    "\n",
    "genus_map = spark.sql(\"\"\"\n",
    "    SELECT DISTINCT\n",
    "        cs.gtdb_species_clade_id,\n",
    "        REGEXP_EXTRACT(t.genus, 'g__(.*)', 1) AS gtdb_genus,\n",
    "        CAST(m.ncbi_species_taxid AS INT) AS ncbi_species_taxid\n",
    "    FROM carrier_species_view cs\n",
    "    JOIN kbase_ke_pangenome.gtdb_taxonomy_r214v1 t\n",
    "        ON cs.gtdb_species_clade_id = t.species\n",
    "    JOIN kbase_ke_pangenome.gtdb_metadata m\n",
    "        ON t.genome_id = m.accession\n",
    "    WHERE t.genus IS NOT NULL\n",
    "\"\"\").toPandas()\n",
    "\n",
    "# Deduplicate — one genus per species\n",
    "genus_map = genus_map.drop_duplicates(subset='gtdb_species_clade_id')\n",
    "print(f'Carrier species -> genus mapping: {len(genus_map)}')\n",
    "print(f'Unique genera: {genus_map[\"gtdb_genus\"].nunique()}')\n",
    "print(f'\\nTop genera:')\n",
    "print(genus_map['gtdb_genus'].value_counts().head(10).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T17:35:52.181756Z",
     "iopub.status.busy": "2026-02-25T17:35:52.181500Z",
     "iopub.status.idle": "2026-02-25T17:38:42.335643Z",
     "shell.execute_reply": "2026-02-25T17:38:42.334557Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taxid -> genus mapping: 2594787 taxa\n",
      "Carrier genera to match: 0\n",
      "Matched genera in NMDC: 0 / 0\n",
      "Total matched taxon columns: 0\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Map NMDC taxon columns to genera via taxonomy_dim\n",
    "# taxonomy_dim has taxid -> genus mapping\n",
    "taxid_to_genus = {}\n",
    "for _, row in tax_dim.iterrows():\n",
    "    tid = str(int(row['taxid'])) if pd.notna(row.get('taxid')) else None\n",
    "    genus = row.get('genus', '')\n",
    "    if tid and genus and pd.notna(genus) and genus != '':\n",
    "        taxid_to_genus[tid] = genus\n",
    "\n",
    "print(f'Taxid -> genus mapping: {len(taxid_to_genus)} taxa')\n",
    "\n",
    "# Step 3: Find NMDC taxon columns matching carrier genera\n",
    "carrier_genera = set(genus_map['gtdb_genus'].dropna().unique())\n",
    "print(f'Carrier genera to match: {len(carrier_genera)}')\n",
    "\n",
    "matched_cols = {}  # genus -> list of taxon column names\n",
    "for col in taxon_cols:\n",
    "    genus = taxid_to_genus.get(col, '')\n",
    "    if genus in carrier_genera:\n",
    "        if genus not in matched_cols:\n",
    "            matched_cols[genus] = []\n",
    "        matched_cols[genus].append(col)\n",
    "\n",
    "print(f'Matched genera in NMDC: {len(matched_cols)} / {len(carrier_genera)}')\n",
    "n_matched_cols = sum(len(v) for v in matched_cols.values())\n",
    "print(f'Total matched taxon columns: {n_matched_cols}')\n",
    "if matched_cols:\n",
    "    print(f'\\nMatched genera: {\", \".join(sorted(matched_cols.keys())[:15])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T17:38:42.337953Z",
     "iopub.status.busy": "2026-02-25T17:38:42.337829Z",
     "iopub.status.idle": "2026-02-25T17:38:42.697911Z",
     "shell.execute_reply": "2026-02-25T17:38:42.696995Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genera with condition mapping: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shared samples for analysis: 6365\n",
      "No matched taxon columns — cannot compute carrier abundance\n"
     ]
    }
   ],
   "source": [
    "# Compute per-sample dark gene carrier score\n",
    "# For each NMDC sample, sum abundance of taxa whose genera carry dark genes\n",
    "# Weight by condition class to create condition-specific scores\n",
    "\n",
    "# Map genus -> condition classes (from our dark gene clusters)\n",
    "genus_conditions = defaultdict(set)\n",
    "for _, row in concordance_clusters.iterrows():\n",
    "    sid = row['gtdb_species_clade_id']\n",
    "    cond = row['top_condition_class']\n",
    "    gm = genus_map[genus_map['gtdb_species_clade_id'] == sid]\n",
    "    if len(gm) > 0:\n",
    "        genus = gm.iloc[0]['gtdb_genus']\n",
    "        if genus and pd.notna(genus):\n",
    "            genus_conditions[genus].add(cond)\n",
    "\n",
    "print(f'Genera with condition mapping: {len(genus_conditions)}')\n",
    "\n",
    "# Filter to shared samples\n",
    "tax_shared = tax_features[tax_features['sample_id'].isin(shared_samples)].copy()\n",
    "abiotic_shared = abiotic[abiotic['sample_id'].isin(shared_samples)].copy()\n",
    "print(f'Shared samples for analysis: {len(tax_shared)}')\n",
    "\n",
    "# Compute overall dark-gene-carrier abundance per sample\n",
    "all_matched_taxcols = []\n",
    "for genus, cols in matched_cols.items():\n",
    "    all_matched_taxcols.extend(cols)\n",
    "\n",
    "if all_matched_taxcols:\n",
    "    # Convert taxon columns to numeric\n",
    "    for col in all_matched_taxcols:\n",
    "        tax_shared[col] = pd.to_numeric(tax_shared[col], errors='coerce').fillna(0)\n",
    "    \n",
    "    tax_shared['dark_carrier_abundance'] = tax_shared[all_matched_taxcols].sum(axis=1)\n",
    "    print(f'Dark carrier abundance: median={tax_shared[\"dark_carrier_abundance\"].median():.4f}, '\n",
    "          f'mean={tax_shared[\"dark_carrier_abundance\"].mean():.4f}')\n",
    "    print(f'  Non-zero samples: {(tax_shared[\"dark_carrier_abundance\"] > 0).sum()} / {len(tax_shared)}')\n",
    "    \n",
    "    # Also compute condition-specific scores\n",
    "    for cond in mapped_conditions:\n",
    "        cond_genera = [g for g, conds in genus_conditions.items() if cond in conds]\n",
    "        cond_cols = []\n",
    "        for g in cond_genera:\n",
    "            cond_cols.extend(matched_cols.get(g, []))\n",
    "        if cond_cols:\n",
    "            score_col = f'score_{cond.replace(\" \", \"_\")}'\n",
    "            tax_shared[score_col] = tax_shared[cond_cols].sum(axis=1)\n",
    "            n_nonzero = (tax_shared[score_col] > 0).sum()\n",
    "            print(f'  {cond}: {len(cond_cols)} taxon cols, {n_nonzero} non-zero samples')\n",
    "else:\n",
    "    print('No matched taxon columns — cannot compute carrier abundance')\n",
    "    tax_shared['dark_carrier_abundance'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T17:38:42.700947Z",
     "iopub.status.busy": "2026-02-25T17:38:42.700825Z",
     "iopub.status.idle": "2026-02-25T17:38:42.723030Z",
     "shell.execute_reply": "2026-02-25T17:38:42.722261Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged samples for correlation: 6365\n",
      "\n",
      "NMDC correlation tests: 15\n",
      "  Significant (FDR < 0.05): 0\n",
      "\n",
      "Top 10 correlations by p-value:\n",
      "               score_type                 abiotic_variable  n_samples  spearman_rho  p_value\n",
      "0  dark_carrier_abundance                         ammonium         33           NaN      NaN\n",
      "1  dark_carrier_abundance                ammonium_nitrogen       1230           NaN      NaN\n",
      "2  dark_carrier_abundance                 carb_nitro_ratio        910           NaN      NaN\n",
      "3  dark_carrier_abundance                      chlorophyll         34           NaN      NaN\n",
      "4  dark_carrier_abundance                           conduc         70           NaN      NaN\n",
      "5  dark_carrier_abundance  depth_has_maximum_numeric_value       4973           NaN      NaN\n",
      "6  dark_carrier_abundance  depth_has_minimum_numeric_value        349           NaN      NaN\n",
      "7  dark_carrier_abundance                            depth        517           NaN      NaN\n",
      "8  dark_carrier_abundance                      diss_oxygen        272           NaN      NaN\n",
      "9  dark_carrier_abundance                               ph       4366           NaN      NaN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32096/4103744494.py:25: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  rho, p = stats.spearmanr(score_vals[valid], abiotic_vals[valid])\n"
     ]
    }
   ],
   "source": [
    "# Correlate dark carrier abundance with abiotic variables\n",
    "# Merge taxonomy scores with abiotic measurements\n",
    "merged = tax_shared[['sample_id', 'dark_carrier_abundance'] +\n",
    "                     [c for c in tax_shared.columns if c.startswith('score_')]].merge(\n",
    "    abiotic_shared, on='sample_id', how='inner'\n",
    ")\n",
    "print(f'Merged samples for correlation: {len(merged)}')\n",
    "\n",
    "# Get abiotic column names\n",
    "abiotic_cols = [c for c in abiotic_shared.columns if c.startswith('annotations_')]\n",
    "score_cols = ['dark_carrier_abundance'] + [c for c in tax_shared.columns if c.startswith('score_')]\n",
    "\n",
    "# Correlate each score with each abiotic variable\n",
    "nmdc_results = []\n",
    "\n",
    "for score_col in score_cols:\n",
    "    for abiotic_col in abiotic_cols:\n",
    "        score_vals = pd.to_numeric(merged[score_col], errors='coerce')\n",
    "        abiotic_vals = pd.to_numeric(merged[abiotic_col], errors='coerce')\n",
    "        \n",
    "        valid = score_vals.notna() & abiotic_vals.notna() & (abiotic_vals != 0)\n",
    "        n_valid = valid.sum()\n",
    "        \n",
    "        if n_valid >= 30:\n",
    "            rho, p = stats.spearmanr(score_vals[valid], abiotic_vals[valid])\n",
    "            nmdc_results.append({\n",
    "                'score_type': score_col,\n",
    "                'abiotic_variable': abiotic_col.replace('annotations_', '').replace('_has_numeric_value', ''),\n",
    "                'n_samples': n_valid,\n",
    "                'spearman_rho': rho,\n",
    "                'p_value': p,\n",
    "            })\n",
    "\n",
    "nmdc_df = pd.DataFrame(nmdc_results)\n",
    "print(f'\\nNMDC correlation tests: {len(nmdc_df)}')\n",
    "\n",
    "if len(nmdc_df) > 0:\n",
    "    _, nmdc_fdr, _, _ = multipletests(nmdc_df['p_value'], method='fdr_bh')\n",
    "    nmdc_df['fdr'] = nmdc_fdr\n",
    "    n_sig = (nmdc_fdr < 0.05).sum()\n",
    "    print(f'  Significant (FDR < 0.05): {n_sig}')\n",
    "    \n",
    "    # Show significant results\n",
    "    sig = nmdc_df[nmdc_df['fdr'] < 0.05].sort_values('p_value')\n",
    "    if len(sig) > 0:\n",
    "        print(f'\\nSignificant NMDC correlations:')\n",
    "        print(sig[['score_type', 'abiotic_variable', 'n_samples', 'spearman_rho', 'p_value', 'fdr']].to_string())\n",
    "    else:\n",
    "        print('\\nTop 10 correlations by p-value:')\n",
    "        print(nmdc_df.sort_values('p_value').head(10)[['score_type', 'abiotic_variable', 'n_samples', 'spearman_rho', 'p_value']].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T17:38:42.725354Z",
     "iopub.status.busy": "2026-02-25T17:38:42.725242Z",
     "iopub.status.idle": "2026-02-25T17:38:42.729357Z",
     "shell.execute_reply": "2026-02-25T17:38:42.728703Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-registered NMDC concordance check:\n",
      "============================================================\n",
      "\n",
      "stress: no score computed (no matched genera)\n",
      "\n",
      "carbon source: no score computed (no matched genera)\n",
      "\n",
      "nitrogen source: no score computed (no matched genera)\n",
      "\n",
      "pH: no score computed (no matched genera)\n",
      "\n",
      "motility: no score computed (no matched genera)\n",
      "\n",
      "anaerobic: no score computed (no matched genera)\n"
     ]
    }
   ],
   "source": [
    "# Check pre-registered concordance: do condition-specific scores correlate\n",
    "# with their expected abiotic variables?\n",
    "print('Pre-registered NMDC concordance check:')\n",
    "print('=' * 60)\n",
    "\n",
    "for cond, info in CONDITION_ENV_MAP.items():\n",
    "    score_col = f'score_{cond.replace(\" \", \"_\")}'\n",
    "    if score_col not in tax_shared.columns:\n",
    "        print(f'\\n{cond}: no score computed (no matched genera)')\n",
    "        continue\n",
    "    \n",
    "    expected_abiotics = info['nmdc_abiotic']\n",
    "    if not expected_abiotics:\n",
    "        print(f'\\n{cond}: no pre-registered abiotic variables')\n",
    "        continue\n",
    "    \n",
    "    print(f'\\n{cond}:')\n",
    "    for ab in expected_abiotics:\n",
    "        ab_short = ab.replace('annotations_', '').replace('_has_numeric_value', '')\n",
    "        match = nmdc_df[\n",
    "            (nmdc_df['score_type'] == score_col) &\n",
    "            (nmdc_df['abiotic_variable'] == ab_short)\n",
    "        ]\n",
    "        if len(match) > 0:\n",
    "            r = match.iloc[0]\n",
    "            sig_str = '*' if r['fdr'] < 0.05 else ''\n",
    "            print(f'  {ab_short}: rho={r[\"spearman_rho\"]:.3f}, p={r[\"p_value\"]:.2e}, '\n",
    "                  f'FDR={r[\"fdr\"]:.3f}, n={int(r[\"n_samples\"])} {sig_str}')\n",
    "        else:\n",
    "            print(f'  {ab_short}: insufficient data (< 30 valid samples)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T17:38:42.731052Z",
     "iopub.status.busy": "2026-02-25T17:38:42.730943Z",
     "iopub.status.idle": "2026-02-25T17:38:42.907638Z",
     "shell.execute_reply": "2026-02-25T17:38:42.906642Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved fig12_nmdc_correlations.png\n"
     ]
    }
   ],
   "source": [
    "# Figure 12: NMDC correlation results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Panel A: Heatmap of correlations (score_type x abiotic_variable)\n",
    "ax = axes[0]\n",
    "if len(nmdc_df) > 0:\n",
    "    pivot = nmdc_df.pivot_table(index='score_type', columns='abiotic_variable',\n",
    "                                values='spearman_rho', aggfunc='first')\n",
    "    # Show only variables with at least one |rho| > 0.05\n",
    "    strong_cols = pivot.columns[pivot.abs().max() > 0.05]\n",
    "    if len(strong_cols) > 0 and len(pivot) > 0:\n",
    "        pivot_sub = pivot[strong_cols]\n",
    "        # Clean labels\n",
    "        pivot_sub.index = [s.replace('score_', '').replace('dark_carrier_abundance', 'all_dark')\n",
    "                           for s in pivot_sub.index]\n",
    "        sns.heatmap(pivot_sub, cmap='RdBu_r', center=0, annot=True, fmt='.2f',\n",
    "                   ax=ax, cbar_kws={'label': 'Spearman rho'})\n",
    "        ax.set_title('NMDC: Dark Gene Carrier Abundance\\nvs Abiotic Variables')\n",
    "        ax.set_ylabel('Score type')\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'No strong correlations found', ha='center', va='center',\n",
    "                transform=ax.transAxes)\n",
    "        ax.set_title('NMDC Correlations')\n",
    "else:\n",
    "    ax.text(0.5, 0.5, 'No NMDC data available', ha='center', va='center',\n",
    "            transform=ax.transAxes)\n",
    "\n",
    "# Panel B: Volcano plot of NMDC correlations\n",
    "ax = axes[1]\n",
    "if len(nmdc_df) > 0:\n",
    "    colors = ['#E53935' if f < 0.05 else '#2196F3' if f < 0.2 else '#9E9E9E'\n",
    "              for f in nmdc_df['fdr']]\n",
    "    ax.scatter(nmdc_df['spearman_rho'],\n",
    "              -np.log10(nmdc_df['p_value'].clip(lower=1e-20)),\n",
    "              c=colors, alpha=0.6, s=30)\n",
    "    ax.axhline(-np.log10(0.05), color='gray', linestyle='--', alpha=0.5)\n",
    "    ax.axvline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax.set_xlabel('Spearman rho')\n",
    "    ax.set_ylabel('-log10(p-value)')\n",
    "    ax.set_title(f'NMDC Abiotic Correlations\\n({len(nmdc_df)} tests, {(nmdc_df[\"fdr\"] < 0.05).sum()} sig)')\n",
    "    \n",
    "    # Label significant points\n",
    "    sig_points = nmdc_df[nmdc_df['fdr'] < 0.05]\n",
    "    for _, r in sig_points.iterrows():\n",
    "        ax.annotate(f\"{r['score_type'].replace('score_', '').replace('dark_carrier_abundance', 'all')}\\n{r['abiotic_variable']}\",\n",
    "                   (r['spearman_rho'], -np.log10(max(r['p_value'], 1e-20))),\n",
    "                   fontsize=6, alpha=0.8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIG_DIR, 'fig12_nmdc_correlations.png'), dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print('Saved fig12_nmdc_correlations.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Save Outputs and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T17:38:42.909835Z",
     "iopub.status.busy": "2026-02-25T17:38:42.909717Z",
     "iopub.status.idle": "2026-02-25T17:38:42.916761Z",
     "shell.execute_reply": "2026-02-25T17:38:42.915960Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved lab_field_concordance.tsv: 47 clusters\n",
      "Saved nmdc_validation.tsv: 15 correlation tests\n"
     ]
    }
   ],
   "source": [
    "# Save results\n",
    "conc_df.to_csv(os.path.join(DATA_DIR, 'lab_field_concordance.tsv'), sep='\\t', index=False)\n",
    "print(f'Saved lab_field_concordance.tsv: {len(conc_df)} clusters')\n",
    "\n",
    "nmdc_df.to_csv(os.path.join(DATA_DIR, 'nmdc_validation.tsv'), sep='\\t', index=False)\n",
    "print(f'Saved nmdc_validation.tsv: {len(nmdc_df)} correlation tests')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-25T17:38:42.918660Z",
     "iopub.status.busy": "2026-02-25T17:38:42.918547Z",
     "iopub.status.idle": "2026-02-25T17:38:42.924117Z",
     "shell.execute_reply": "2026-02-25T17:38:42.923267Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "NB04: LAB-FIELD CONCORDANCE & NMDC VALIDATION — SUMMARY\n",
      "======================================================================\n",
      "\n",
      "--- Lab-Field Concordance ---\n",
      "Pre-registered condition classes: 6\n",
      "Clusters tested: 47\n",
      "  Concordant: 29/47 (61.7%)\n",
      "  Significant (FDR < 0.05): 1\n",
      "  Significant + concordant: 0\n",
      "\n",
      "--- NMDC Validation ---\n",
      "Samples with taxonomy + abiotic: 6365\n",
      "Matched genera in NMDC: 0\n",
      "Correlation tests: 15\n",
      "  Significant (FDR < 0.05): 0\n",
      "\n",
      "Output files:\n",
      "  lab_field_concordance.tsv: 12.0 KB\n",
      "  nmdc_validation.tsv: 0.7 KB\n",
      "\n",
      "Figures:\n",
      "  fig11_concordance_matrix.png\n",
      "  fig12_nmdc_correlations.png\n"
     ]
    }
   ],
   "source": [
    "# Final summary\n",
    "print('=' * 70)\n",
    "print('NB04: LAB-FIELD CONCORDANCE & NMDC VALIDATION — SUMMARY')\n",
    "print('=' * 70)\n",
    "\n",
    "print(f'\\n--- Lab-Field Concordance ---')\n",
    "print(f'Pre-registered condition classes: {len(CONDITION_ENV_MAP)}')\n",
    "print(f'Clusters tested: {len(conc_df)}')\n",
    "if len(conc_df) > 0:\n",
    "    n_conc = conc_df['is_concordant'].sum()\n",
    "    n_sig = (conc_df['fdr'] < 0.05).sum()\n",
    "    n_sig_conc = ((conc_df['fdr'] < 0.05) & conc_df['is_concordant']).sum()\n",
    "    print(f'  Concordant: {n_conc}/{len(conc_df)} ({n_conc/len(conc_df)*100:.1f}%)')\n",
    "    print(f'  Significant (FDR < 0.05): {n_sig}')\n",
    "    print(f'  Significant + concordant: {n_sig_conc}')\n",
    "\n",
    "print(f'\\n--- NMDC Validation ---')\n",
    "if len(nmdc_df) > 0:\n",
    "    print(f'Samples with taxonomy + abiotic: {len(shared_samples)}')\n",
    "    print(f'Matched genera in NMDC: {len(matched_cols)}')\n",
    "    print(f'Correlation tests: {len(nmdc_df)}')\n",
    "    n_nmdc_sig = (nmdc_df['fdr'] < 0.05).sum()\n",
    "    print(f'  Significant (FDR < 0.05): {n_nmdc_sig}')\n",
    "else:\n",
    "    print('No NMDC correlations could be computed')\n",
    "\n",
    "print(f'\\nOutput files:')\n",
    "for f in ['lab_field_concordance.tsv', 'nmdc_validation.tsv']:\n",
    "    fp = os.path.join(DATA_DIR, f)\n",
    "    if os.path.exists(fp):\n",
    "        size_kb = os.path.getsize(fp) / 1024\n",
    "        print(f'  {f}: {size_kb:.1f} KB')\n",
    "print(f'\\nFigures:')\n",
    "for f in ['fig11_concordance_matrix.png', 'fig12_nmdc_correlations.png']:\n",
    "    fp = os.path.join(FIG_DIR, f)\n",
    "    if os.path.exists(fp):\n",
    "        print(f'  {f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
