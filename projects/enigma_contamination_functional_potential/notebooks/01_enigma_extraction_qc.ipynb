{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB01: ENIGMA Extraction and QC\n",
    "\n",
    "Extract overlap-ready geochemistry, community, and metadata tables for downstream modeling.\n",
    "\n",
    "Requires BERDL JupyterHub with built-in `get_spark_session()`.\n",
    "\n",
    "Outputs:\n",
    "- `../data/geochemistry_sample_matrix.tsv`\n",
    "- `../data/community_taxon_counts.tsv`\n",
    "- `../data/sample_location_metadata.tsv`\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "DATA_DIR = Path('../data')\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "spark = get_spark_session()\n",
    "print('Spark session ready')\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "tables = spark.sql('SHOW TABLES IN enigma_coral').toPandas()\n",
    "table_names = set(tables['tableName'].tolist())\n",
    "print(f\"ENIGMA tables discovered: {len(table_names)}\")\n",
    "\n",
    "community_brick = 'ddt_brick0000476' if 'ddt_brick0000476' in table_names else 'ddt_brick0000459'\n",
    "print('Using community count table:', community_brick)\n",
    "\n",
    "for t in ['ddt_brick0000010', community_brick, 'ddt_brick0000454', 'sdt_sample', 'sdt_community', 'sdt_location']:\n",
    "    print(f\"\\n=== {t} ===\")\n",
    "    spark.sql(f'DESCRIBE enigma_coral.{t}').show(200, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "geochem_raw = spark.sql(\"\"\"\n",
    "SELECT\n",
    "  sdt_sample_name,\n",
    "  molecule_from_list_sys_oterm_name AS molecule,\n",
    "  CAST(concentration_micromolar AS DOUBLE) AS concentration_micromolar\n",
    "FROM enigma_coral.ddt_brick0000010\n",
    "WHERE concentration_micromolar IS NOT NULL\n",
    "\"\"\").toPandas()\n",
    "\n",
    "geochem_raw = geochem_raw.dropna(subset=['sdt_sample_name', 'molecule'])\n",
    "print(f\"Geochemistry raw rows: {len(geochem_raw):,}\")\n",
    "print(f\"Samples: {geochem_raw['sdt_sample_name'].nunique():,}\")\n",
    "print(f\"Molecules: {geochem_raw['molecule'].nunique():,}\")\n",
    "\n",
    "geochem = geochem_raw.pivot_table(\n",
    "    index='sdt_sample_name',\n",
    "    columns='molecule',\n",
    "    values='concentration_micromolar',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "geochem.columns = [re.sub(r'[^a-z0-9]+', '_', c.lower()).strip('_').replace('_atom', '') for c in geochem.columns]\n",
    "geochem = geochem.reset_index()\n",
    "print('Geochemistry matrix shape:', geochem.shape)\n",
    "\n",
    "target_cols = [c for c in geochem.columns if any(k in c for k in ['uranium', 'chromium', 'nickel', 'zinc', 'copper', 'cadmium', 'lead', 'arsenic', 'mercury'])]\n",
    "print('Contaminant columns:', target_cols[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "geochem_samples = geochem[['sdt_sample_name']].drop_duplicates()\n",
    "spark.createDataFrame(geochem_samples).createOrReplaceTempView('geochem_samples_tmp')\n",
    "\n",
    "overlap_communities = spark.sql(\"\"\"\n",
    "SELECT DISTINCT\n",
    "  c.sdt_community_name,\n",
    "  c.sdt_sample_name\n",
    "FROM enigma_coral.sdt_community c\n",
    "JOIN geochem_samples_tmp g ON c.sdt_sample_name = g.sdt_sample_name\n",
    "WHERE c.sdt_community_name IS NOT NULL\n",
    "\"\"\").toPandas()\n",
    "print('Overlap communities:', len(overlap_communities))\n",
    "print('Overlap samples:', overlap_communities['sdt_sample_name'].nunique())\n",
    "\n",
    "spark.createDataFrame(overlap_communities).createOrReplaceTempView('overlap_comms_tmp')\n",
    "\n",
    "asv_counts = spark.sql(f\"\"\"\n",
    "SELECT\n",
    "  b.sdt_asv_name,\n",
    "  b.sdt_community_name,\n",
    "  o.sdt_sample_name,\n",
    "  CAST(regexp_replace(CAST(b.count_count_unit AS STRING), '[^0-9.-]', '') AS DOUBLE) AS read_count\n",
    "FROM enigma_coral.{community_brick} b\n",
    "JOIN overlap_comms_tmp o ON b.sdt_community_name = o.sdt_community_name\n",
    "WHERE b.count_count_unit IS NOT NULL\n",
    "\"\"\").toPandas()\n",
    "\n",
    "asv_counts = asv_counts.dropna(subset=['sdt_asv_name', 'sdt_community_name', 'sdt_sample_name', 'read_count'])\n",
    "asv_counts = asv_counts[asv_counts['read_count'] > 0]\n",
    "asv_counts = asv_counts.groupby(['sdt_asv_name', 'sdt_community_name', 'sdt_sample_name'], as_index=False)['read_count'].sum()\n",
    "print(f\"ASV count rows after cleanup: {len(asv_counts):,}\")\n",
    "print(f\"Unique ASVs: {asv_counts['sdt_asv_name'].nunique():,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "asv_genus = spark.sql(\"\"\"\n",
    "SELECT DISTINCT\n",
    "  sdt_asv_name,\n",
    "  sdt_taxon_name AS genus\n",
    "FROM enigma_coral.ddt_brick0000454\n",
    "WHERE taxonomic_level_sys_oterm_name = 'Genus'\n",
    "\"\"\").toPandas()\n",
    "\n",
    "asv_phylum = spark.sql(\"\"\"\n",
    "SELECT DISTINCT\n",
    "  sdt_asv_name,\n",
    "  sdt_taxon_name AS phylum\n",
    "FROM enigma_coral.ddt_brick0000454\n",
    "WHERE taxonomic_level_sys_oterm_name = 'Phylum'\n",
    "\"\"\").toPandas()\n",
    "\n",
    "asv_tax = asv_genus.merge(asv_phylum, on='sdt_asv_name', how='left')\n",
    "community_taxon = asv_counts.merge(asv_tax, on='sdt_asv_name', how='left')\n",
    "\n",
    "community_taxon['genus'] = community_taxon['genus'].fillna('unclassified')\n",
    "community_taxon['phylum'] = community_taxon['phylum'].fillna('unclassified')\n",
    "print('Community+taxonomy rows:', len(community_taxon))\n",
    "print('Unique genera:', community_taxon['genus'].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample_meta = spark.sql(\"\"\"\n",
    "SELECT DISTINCT\n",
    "  s.sdt_sample_name,\n",
    "  s.sdt_location_name,\n",
    "  s.date,\n",
    "  s.depth_meter,\n",
    "  l.latitude_degree,\n",
    "  l.longitude_degree,\n",
    "  l.region\n",
    "FROM enigma_coral.sdt_sample s\n",
    "LEFT JOIN enigma_coral.sdt_location l\n",
    "  ON s.sdt_location_name = l.sdt_location_name\n",
    "\"\"\").toPandas()\n",
    "\n",
    "valid_samples = set(community_taxon['sdt_sample_name'])\n",
    "geochem_f = geochem[geochem['sdt_sample_name'].isin(valid_samples)].copy()\n",
    "community_f = community_taxon[community_taxon['sdt_sample_name'].isin(valid_samples)].copy()\n",
    "sample_meta_f = sample_meta[sample_meta['sdt_sample_name'].isin(valid_samples)].copy()\n",
    "\n",
    "print('Filtered geochemistry shape:', geochem_f.shape)\n",
    "print('Filtered community rows:', len(community_f))\n",
    "print('Filtered sample metadata rows:', len(sample_meta_f))\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "geochem_f.to_csv(DATA_DIR / 'geochemistry_sample_matrix.tsv', sep='\t', index=False)\n",
    "community_f.to_csv(DATA_DIR / 'community_taxon_counts.tsv', sep='\t', index=False)\n",
    "sample_meta_f.to_csv(DATA_DIR / 'sample_location_metadata.tsv', sep='\t', index=False)\n",
    "\n",
    "print('Saved outputs:')\n",
    "print(' -', (DATA_DIR / 'geochemistry_sample_matrix.tsv').resolve())\n",
    "print(' -', (DATA_DIR / 'community_taxon_counts.tsv').resolve())\n",
    "print(' -', (DATA_DIR / 'sample_location_metadata.tsv').resolve())\n",
    "\n",
    "print('\\nSummary')\n",
    "print('Samples with both geochem+community:', geochem_f['sdt_sample_name'].nunique())\n",
    "print('Communities:', community_f['sdt_community_name'].nunique())\n",
    "print('ASVs:', community_f['sdt_asv_name'].nunique())\n",
    "print('Genera:', community_f['genus'].nunique())\n"
   ]
  }
 ]
}