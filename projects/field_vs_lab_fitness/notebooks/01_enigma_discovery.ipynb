{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# NB 01: ENIGMA CORAL Discovery\n\nDiscover what data exists in the undocumented ENIGMA CORAL database tables.\nAssess whether ENIGMA adds TnSeq, DubSeq, community, or field sampling\ndata beyond what the Fitness Browser already provides for DvH.\n\n**Run via Spark Connect** — `get_spark_session()` from `berdl_notebook_utils`.\n\n**Outputs**: Summary of ENIGMA data availability, schema notes.\n\n### Execution Summary\n\n> Executed 2026-02-15 via Spark Connect. Key findings:\n>\n> - **47 tables** in `enigma_coral` (17 sdt_*, 23 ddt_brick*, 5 sys_*, 2 other)\n> - **No DvH data**: Zero DvH strains, genomes, or genes in the database. The single TnSeq library is for FW300-N2E2 (a *Pseudomonas* isolate), not DvH.\n> - **All 596 locations** are Oak Ridge Reservation sites\n> - **3 DubSeq libraries**: *E. coli* BW25113, *P. putida* KT2440, *B. thetaiotaomicron* VPI-5482 — none for DvH\n> - **6 Desulfovibrio taxa** in `sdt_taxon` but only at genus/family/order level — no DvH Hildenborough\n> - **6,705 genomes, 15,015 genes** — these are environmental isolates from ORR, not DvH\n> - **4,346 field samples** with geochemistry data in brick tables (uranium, metals)\n> - **213,044 ASVs** for community composition (16S amplicon data)\n> - **Assessment**: ENIGMA CORAL does NOT add TnSeq/fitness data for DvH beyond the Fitness Browser. It provides complementary environmental/community data (field samples, geochemistry, community composition) that could be used for future analyses linking DvH ecology to field conditions, but not for the current gene-level fitness analysis."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "spark = get_spark_session()\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "\n",
    "DATA_DIR = Path('../data')\n",
    "DATA_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. List All ENIGMA Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all tables in enigma_coral\n",
    "tables = spark.sql(\"SHOW TABLES IN enigma_coral\").toPandas()\n",
    "print(f\"Total ENIGMA tables: {len(tables)}\")\n",
    "print()\n",
    "\n",
    "# Categorize tables\n",
    "sdt_tables = tables[tables['tableName'].str.startswith('sdt_')]\n",
    "ddt_tables = tables[tables['tableName'].str.startswith('ddt_')]\n",
    "sys_tables = tables[tables['tableName'].str.startswith('sys_')]\n",
    "other_tables = tables[~tables['tableName'].str.startswith(('sdt_', 'ddt_', 'sys_'))]\n",
    "\n",
    "print(f\"Scientific data tables (sdt_*): {len(sdt_tables)}\")\n",
    "print(f\"Data brick tables (ddt_*): {len(ddt_tables)}\")\n",
    "print(f\"System tables (sys_*): {len(sys_tables)}\")\n",
    "print(f\"Other tables: {len(other_tables)}\")\n",
    "print()\n",
    "print(\"sdt_ tables:\")\n",
    "for t in sorted(sdt_tables['tableName'].tolist()):\n",
    "    print(f\"  {t}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Row Counts for Key Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get row counts for all sdt_ tables\n",
    "row_counts = {}\n",
    "for t in sorted(sdt_tables['tableName'].tolist()):\n",
    "    try:\n",
    "        n = spark.sql(f\"SELECT COUNT(*) as n FROM enigma_coral.{t}\").toPandas()['n'].iloc[0]\n",
    "        row_counts[t] = n\n",
    "        print(f\"  {t}: {n:,} rows\")\n",
    "    except Exception as e:\n",
    "        print(f\"  {t}: ERROR - {e}\")\n",
    "        row_counts[t] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 3. TnSeq Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What TnSeq libraries exist?\n",
    "tnseq = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM enigma_coral.sdt_tnseq_library\n",
    "    LIMIT 50\n",
    "\"\"\").toPandas()\n",
    "\n",
    "print(f\"TnSeq library rows (sample): {len(tnseq)}\")\n",
    "print(f\"Columns: {tnseq.columns.tolist()}\")\n",
    "print()\n",
    "tnseq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 4. DubSeq Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What DubSeq data is available?\n",
    "dubseq = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM enigma_coral.sdt_dubseq_library\n",
    "    LIMIT 50\n",
    "\"\"\").toPandas()\n",
    "\n",
    "print(f\"DubSeq library rows (sample): {len(dubseq)}\")\n",
    "print(f\"Columns: {dubseq.columns.tolist()}\")\n",
    "print()\n",
    "dubseq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 5. Strains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What strains are recorded?\n",
    "strains = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM enigma_coral.sdt_strain\n",
    "    LIMIT 100\n",
    "\"\"\").toPandas()\n",
    "\n",
    "print(f\"Strain rows (sample): {len(strains)}\")\n",
    "print(f\"Columns: {strains.columns.tolist()}\")\n",
    "print()\n",
    "\n",
    "# Check for DvH strains\n",
    "if 'sdt_strain_name' in strains.columns:\n",
    "    dvh_strains = strains[strains['sdt_strain_name'].str.contains('Desulfovibrio|DvH|vulgaris', case=False, na=False)]\n",
    "    print(f\"DvH-related strains in sample: {len(dvh_strains)}\")\n",
    "    if len(dvh_strains) > 0:\n",
    "        print(dvh_strains)\n",
    "print()\n",
    "strains.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 6. Experimental Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What experimental conditions were tested?\n",
    "conditions = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM enigma_coral.sdt_condition\n",
    "    LIMIT 100\n",
    "\"\"\").toPandas()\n",
    "\n",
    "print(f\"Condition rows (sample): {len(conditions)}\")\n",
    "print(f\"Columns: {conditions.columns.tolist()}\")\n",
    "print()\n",
    "conditions.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 7. Field Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What field samples exist?\n",
    "samples = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM enigma_coral.sdt_sample\n",
    "    LIMIT 50\n",
    "\"\"\").toPandas()\n",
    "\n",
    "print(f\"Sample rows (sample): {len(samples)}\")\n",
    "print(f\"Columns: {samples.columns.tolist()}\")\n",
    "print()\n",
    "samples.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 8. Community Composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Community composition data\n",
    "community = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM enigma_coral.sdt_community\n",
    "    LIMIT 50\n",
    "\"\"\").toPandas()\n",
    "\n",
    "print(f\"Community rows (sample): {len(community)}\")\n",
    "print(f\"Columns: {community.columns.tolist()}\")\n",
    "print()\n",
    "community.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 9. Sampling Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling locations\n",
    "locations = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM enigma_coral.sdt_location\n",
    "    LIMIT 50\n",
    "\"\"\").toPandas()\n",
    "\n",
    "print(f\"Location rows (sample): {len(locations)}\")\n",
    "print(f\"Columns: {locations.columns.tolist()}\")\n",
    "print()\n",
    "locations.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 10. Desulfovibrio Presence in Taxon Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for Desulfovibrio in taxon table\n",
    "dvh_taxa = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM enigma_coral.sdt_taxon\n",
    "    WHERE sdt_taxon_name LIKE '%Desulfovibrio%'\n",
    "       OR sdt_taxon_name LIKE '%vulgaris%'\n",
    "\"\"\").toPandas()\n",
    "\n",
    "print(f\"Desulfovibrio/vulgaris taxa: {len(dvh_taxa)}\")\n",
    "print(f\"Columns: {dvh_taxa.columns.tolist()}\")\n",
    "print()\n",
    "dvh_taxa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## 11. Data Brick Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample ddt_brick* tables to understand numerical data format\n",
    "brick_tables = ddt_tables[ddt_tables['tableName'].str.startswith('ddt_brick')]\n",
    "print(f\"Brick tables found: {len(brick_tables)}\")\n",
    "\n",
    "for t in sorted(brick_tables['tableName'].tolist())[:5]:\n",
    "    print(f\"\\n--- {t} ---\")\n",
    "    try:\n",
    "        schema = spark.sql(f\"DESCRIBE enigma_coral.{t}\").toPandas()\n",
    "        print(f\"Columns: {schema['col_name'].tolist()}\")\n",
    "        sample = spark.sql(f\"SELECT * FROM enigma_coral.{t} LIMIT 3\").toPandas()\n",
    "        print(f\"Sample rows: {len(sample)}\")\n",
    "        print(sample)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## 12. Summary & Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"ENIGMA CORAL DISCOVERY SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"Table row counts:\")\n",
    "for t, n in sorted(row_counts.items()):\n",
    "    status = f\"{n:,}\" if n >= 0 else \"ERROR\"\n",
    "    print(f\"  {t}: {status}\")\n",
    "print()\n",
    "print(\"Key questions:\")\n",
    "print(f\"  TnSeq libraries found: {len(tnseq)}\")\n",
    "print(f\"  DubSeq libraries found: {len(dubseq)}\")\n",
    "print(f\"  Desulfovibrio taxa: {len(dvh_taxa)}\")\n",
    "print(f\"  Sampling locations: {len(locations)}\")\n",
    "print()\n",
    "print(\"Assessment:\")\n",
    "print(\"  [Fill in after running: does ENIGMA CORAL add data beyond\")\n",
    "print(\"   the Fitness Browser for this project's analyses?]\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}