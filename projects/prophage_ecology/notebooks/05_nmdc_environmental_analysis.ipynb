{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {
    "papermill": {
     "duration": 0.004777,
     "end_time": "2026-02-21T00:11:09.469294",
     "exception": false,
     "start_time": "2026-02-21T00:11:09.464517",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# NB05: NMDC Environmental Gradient Analysis\n",
    "\n",
    "**Project**: Prophage Ecology Across Bacterial Phylogeny and Environmental Gradients\n",
    "\n",
    "**Goal**: Test H1d — infer per-sample prophage burden from NMDC metagenomic taxonomic profiles, weighted by pangenome-derived prophage prevalence per genus. Correlate with abiotic environmental features.\n",
    "\n",
    "**Dependencies**: NB01 outputs (`data/species_module_summary.tsv`)\n",
    "\n",
    "**Environment**: Requires BERDL JupyterHub (Spark SQL) for NMDC data access\n",
    "\n",
    "**Outputs**:\n",
    "- `data/nmdc_prophage_prevalence.tsv` — per-sample prophage inference scores\n",
    "- `data/nmdc_module_by_environment.tsv` — per-module abiotic correlations\n",
    "- `figures/nmdc_prophage_vs_abiotic.png`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T00:11:09.479450Z",
     "iopub.status.busy": "2026-02-21T00:11:09.479062Z",
     "iopub.status.idle": "2026-02-21T00:11:10.231967Z",
     "shell.execute_reply": "2026-02-21T00:11:10.230937Z"
    },
    "papermill": {
     "duration": 0.759534,
     "end_time": "2026-02-21T00:11:10.233155",
     "exception": false,
     "start_time": "2026-02-21T00:11:09.473621",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Species with prophage data: 27,702\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "spark = get_spark_session()\n",
    "\n",
    "sys.path.insert(0, '../src')\n",
    "from prophage_utils import MODULES\n",
    "\n",
    "os.makedirs('../data', exist_ok=True)\n",
    "os.makedirs('../figures', exist_ok=True)\n",
    "\n",
    "# Load NB01 species module summary\n",
    "species_summary = pd.read_csv('../data/species_module_summary.tsv', sep='\\t')\n",
    "print(f'Species with prophage data: {len(species_summary):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {
    "papermill": {
     "duration": 0.003846,
     "end_time": "2026-02-21T00:11:10.241770",
     "exception": false,
     "start_time": "2026-02-21T00:11:10.237924",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. NMDC Data Overview\n",
    "\n",
    "NMDC has no per-sample gene-level functional annotations — we use taxonomy-based inference (same approach as PHB NB04)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T00:11:10.250862Z",
     "iopub.status.busy": "2026-02-21T00:11:10.250710Z",
     "iopub.status.idle": "2026-02-21T00:11:21.882145Z",
     "shell.execute_reply": "2026-02-21T00:11:21.881102Z"
    },
    "papermill": {
     "duration": 11.637729,
     "end_time": "2026-02-21T00:11:21.883326",
     "exception": false,
     "start_time": "2026-02-21T00:11:10.245597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMDC samples with taxonomy: 6,365\n",
      "NMDC samples with abiotic data: 13,847\n",
      "NMDC studies: 48\n",
      "Taxonomy dimension table: 2,594,787 taxa\n",
      "Taxon columns: 3492\n"
     ]
    }
   ],
   "source": [
    "# Load NMDC taxonomy features and study metadata\n",
    "tax_features = spark.sql(\"SELECT * FROM nmdc_arkin.taxonomy_features\").toPandas()\n",
    "studies = spark.sql(\"SELECT * FROM nmdc_arkin.study_table\").toPandas()\n",
    "abiotic_all = spark.sql(\"SELECT * FROM nmdc_arkin.abiotic_features\").toPandas()\n",
    "tax_dim = spark.sql(\"SELECT * FROM nmdc_arkin.taxonomy_dim\").toPandas()\n",
    "\n",
    "print(f'NMDC samples with taxonomy: {len(tax_features):,}')\n",
    "print(f'NMDC samples with abiotic data: {len(abiotic_all):,}')\n",
    "print(f'NMDC studies: {len(studies)}')\n",
    "print(f'Taxonomy dimension table: {len(tax_dim):,} taxa')\n",
    "\n",
    "taxon_cols = [c for c in tax_features.columns if c != 'sample_id']\n",
    "print(f'Taxon columns: {len(taxon_cols)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T00:11:21.893750Z",
     "iopub.status.busy": "2026-02-21T00:11:21.893623Z",
     "iopub.status.idle": "2026-02-21T00:11:21.905906Z",
     "shell.execute_reply": "2026-02-21T00:11:21.905091Z"
    },
    "papermill": {
     "duration": 0.018248,
     "end_time": "2026-02-21T00:11:21.906655",
     "exception": false,
     "start_time": "2026-02-21T00:11:21.888407",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMDC studies by ecosystem:\n",
      "  nmdc:sty-11-8fb6t785: Terrestrial/Deep subsurface — Deep subsurface shale carbon reservoir microbial communities\n",
      "  nmdc:sty-11-33fbta56: Aquatic/Freshwater — Peatland microbial communities from Minnesota, USA, analyzin\n",
      "  nmdc:sty-11-aygzgv51: Aquatic/Freshwater — Riverbed sediment microbial communities from the Columbia Ri\n",
      "  nmdc:sty-11-34xj1150: nan/nan — National Ecological Observatory Network: soil metagenomes (D\n",
      "  nmdc:sty-11-076c9980: Terrestrial/Soil — Lab enrichment of tropical soil microbial communities from L\n",
      "  nmdc:sty-11-t91cwb40: nan/nan — Determining the genomic basis for interactions between gut f\n",
      "  nmdc:sty-11-5bgrvr62: nan/nan — Freshwater microbial communities from Lake Mendota, Crystal \n",
      "  nmdc:sty-11-5tgfr349: Aquatic/Freshwater — Freshwater microbial communities from rivers from various lo\n",
      "  nmdc:sty-11-dcqce727: Terrestrial/Soil — Bulk soil microbial communities from the East River watershe\n",
      "  nmdc:sty-11-1t150432: Plants/Unclassified — Populus root and rhizosphere microbial communities from Tenn\n",
      "  nmdc:sty-11-zs2syx06: nan/nan — Meadow Soil Metagenomes from the Angelo Coast Range Reserve\n",
      "  nmdc:sty-11-r2h77870: Terrestrial/Soil — Roots, rhizosphere and bulk soil microbial communities from \n",
      "  nmdc:sty-11-28tm5d36: nan/nan — 1000 Soils Research Campaign\n",
      "  nmdc:sty-11-547rwq94: nan/nan — nan\n",
      "  nmdc:sty-11-hht5sb92: nan/nan — National Ecological Observatory Network: surface water metag\n",
      "  nmdc:sty-11-pzmd0x14: nan/nan — National Ecological Observatory Network: benthic metagenomes\n",
      "  nmdc:sty-11-db67n062: nan/nan — Geochemical controls on soil resiliency to carbon loss follo\n",
      "  nmdc:sty-11-8xdqsn54: nan/nan — Coupling spectral techniques; Molecular characterization of \n",
      "  nmdc:sty-11-hdd4bf83: nan/nan — Colonization resistance against Candida\n",
      "  nmdc:sty-11-2zhqs261: nan/nan — nan\n",
      "  nmdc:sty-11-xcbexm97: nan/nan — nan\n",
      "  nmdc:sty-11-x4aawf73: nan/nan — nan\n",
      "  nmdc:sty-11-f1he1955: nan/nan — nan\n",
      "  nmdc:sty-11-cytnjc39: nan/nan — nan\n",
      "  nmdc:sty-11-msexsy29: nan/nan — nan\n",
      "  nmdc:sty-11-nxrz9m96: nan/nan — nan\n",
      "  nmdc:sty-11-e4yb9z58: nan/nan — Seasonal activities of the phyllosphere microbiome of perenn\n",
      "  nmdc:sty-11-abkzcd11: nan/nan — nan\n",
      "  nmdc:sty-11-ev70y104: nan/nan — EcoFAB 2.0 Root Microbiome Ring Trial\n",
      "  nmdc:sty-11-8ws97026: nan/nan — Molecular mechanisms underlying changes in the temperature s\n",
      "  nmdc:sty-11-fkbnah04: nan/nan — Freshwater microbial communities from oligotrophic, dystroph\n",
      "  nmdc:sty-11-prtb4s28: nan/nan — Arabidopsis, maize, boechera and miscanthus rhizosphere micr\n",
      "  nmdc:sty-11-dwsv7q78: nan/nan — Microbial regulation of soil water repellency to control soi\n",
      "  nmdc:sty-11-y1kdd163: nan/nan — Great Lakes Bioenergy Research Center (GLBRC)\n",
      "  nmdc:sty-11-3cmn1g53: nan/nan — Center for Advanced Bioenergy and Bioproducts Innovation (CA\n",
      "  nmdc:sty-11-cssvjy19: nan/nan — Center for Bioenergy Innovation (CBI)\n",
      "  nmdc:sty-11-ggfd7z76: nan/nan — Joint BioEnergy Institute (JBEI)\n",
      "  nmdc:sty-11-srtxhh77: nan/nan — MONet\n",
      "  nmdc:sty-11-kjs8av65: nan/nan — COMPASS - Field, Measurements, and Experiments\n",
      "  nmdc:sty-11-wbc14h22: nan/nan — Switchgrass cropping systems affect soil carbon and nitrogen\n",
      "  nmdc:sty-11-vh2hty57: nan/nan — The Impact of Stand Age and Fertilization on the Soil Microb\n",
      "  nmdc:sty-11-kfvmk798: nan/nan — Chronic drought differentially alters the belowground microb\n",
      "  nmdc:sty-11-n7mtj961: nan/nan — Impact of modulating bioenergy traits on the sorghum microbi\n",
      "  nmdc:sty-11-46aje659: nan/nan — Panicgrass rhizosphere soil microbial communities from growt\n",
      "  nmdc:sty-11-h1m9nj62: nan/nan — Seawater and ice microbial communities from Arctic Ocean\n",
      "  nmdc:sty-11-rh9tya90: nan/nan — Effects of warming and drought on the microbial communities \n",
      "  nmdc:sty-11-b0ykqz91: nan/nan — 2014 Lake Erie Harmful Algal Bloom\n",
      "  nmdc:sty-11-j05pc998: nan/nan — Soil microbial communities from watershed of Upper East Rive\n"
     ]
    }
   ],
   "source": [
    "# Study ecosystem overview\n",
    "study_eco = studies[['study_id', 'name', 'ecosystem', 'ecosystem_category',\n",
    "                      'ecosystem_type', 'ecosystem_subtype']].copy()\n",
    "print('NMDC studies by ecosystem:')\n",
    "for _, row in study_eco.iterrows():\n",
    "    eco = f\"{row['ecosystem_category'] or '?'}/{row['ecosystem_type'] or '?'}\"\n",
    "    print(f\"  {row['study_id']}: {eco} — {str(row['name'])[:60]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {
    "papermill": {
     "duration": 0.004001,
     "end_time": "2026-02-21T00:11:21.914861",
     "exception": false,
     "start_time": "2026-02-21T00:11:21.910860",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Build Genus-Level Prophage Burden Scores\n",
    "\n",
    "From the pangenome, compute the mean prophage module count per genus. This is our reference for weighting NMDC taxonomic profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T00:11:21.924341Z",
     "iopub.status.busy": "2026-02-21T00:11:21.924210Z",
     "iopub.status.idle": "2026-02-21T00:11:25.854062Z",
     "shell.execute_reply": "2026-02-21T00:11:25.853163Z"
    },
    "papermill": {
     "duration": 3.936306,
     "end_time": "2026-02-21T00:11:25.855218",
     "exception": false,
     "start_time": "2026-02-21T00:11:21.918912",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genera with prophage burden scores: 8,419\n",
      "\n",
      "Top 10 genera by prophage burden:\n",
      "gtdb_genus_name  n_species  prophage_burden  mean_modules\n",
      "     Salmonella          5       2044.00000           7.0\n",
      "     Klebsiella         16       1976.25000           7.0\n",
      "    Citrobacter         10       1403.30000           7.0\n",
      "          Dorea          1       1373.00000           7.0\n",
      "  Citrobacter_B          1       1195.00000           7.0\n",
      "   Enterobacter         32        979.78125           7.0\n",
      "    Escherichia          8        970.00000           7.0\n",
      "       Serratia         10        881.40000           7.0\n",
      " Enterococcus_D          4        809.75000           7.0\n",
      "     Hungatella          2        802.50000           7.0\n"
     ]
    }
   ],
   "source": [
    "# Get genus-level taxonomy from GTDB\n",
    "# Note: gtdb_taxonomy_r214v1 JOIN intermittently returns 0 rows;\n",
    "# use gtdb_metadata.gtdb_taxonomy string instead (reliable)\n",
    "taxonomy = spark.sql(\"\"\"\n",
    "    SELECT DISTINCT g.gtdb_species_clade_id,\n",
    "           REGEXP_EXTRACT(m.gtdb_taxonomy, 'g__([^;]+)', 1) AS gtdb_genus_name\n",
    "    FROM kbase_ke_pangenome.genome g\n",
    "    JOIN kbase_ke_pangenome.gtdb_metadata m ON g.genome_id = m.accession\n",
    "    WHERE m.gtdb_taxonomy IS NOT NULL\n",
    "      AND REGEXP_EXTRACT(m.gtdb_taxonomy, 'g__([^;]+)', 1) != ''\n",
    "\"\"\").toPandas()\n",
    "\n",
    "# Merge with species module summary\n",
    "species_with_genus = species_summary.merge(taxonomy, on='gtdb_species_clade_id', how='left')\n",
    "\n",
    "# Build genus-level prophage burden scores\n",
    "module_ids = sorted(MODULES.keys())\n",
    "\n",
    "# Per-genus: mean prophage prevalence and per-module prevalence\n",
    "genus_scores = species_with_genus.groupby('gtdb_genus_name').agg(\n",
    "    n_species=('gtdb_species_clade_id', 'count'),\n",
    "    mean_prophage_clusters=('n_prophage_clusters', 'mean'),\n",
    "    mean_modules=('n_modules_present', 'mean'),\n",
    "    **{f'pct_{m}': (f'has_{m}', 'mean') for m in module_ids},\n",
    ").reset_index()\n",
    "\n",
    "# Overall prophage burden score = mean prophage cluster count per species\n",
    "genus_scores['prophage_burden'] = genus_scores['mean_prophage_clusters']\n",
    "\n",
    "print(f'Genera with prophage burden scores: {len(genus_scores):,}')\n",
    "print(f'\\nTop 10 genera by prophage burden:')\n",
    "print(genus_scores.nlargest(10, 'prophage_burden')[['gtdb_genus_name', 'n_species',\n",
    "      'prophage_burden', 'mean_modules']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {
    "papermill": {
     "duration": 0.004131,
     "end_time": "2026-02-21T00:11:25.864502",
     "exception": false,
     "start_time": "2026-02-21T00:11:25.860371",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. Two-Tier Taxonomy Mapping\n",
    "\n",
    "Map NMDC taxon IDs to pangenome genera via:\n",
    "- **Tier 1**: gtdb_metadata NCBI taxid → GTDB genus (handles renames)\n",
    "- **Tier 2**: Direct genus name matching via taxonomy_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T00:11:25.874192Z",
     "iopub.status.busy": "2026-02-21T00:11:25.874048Z",
     "iopub.status.idle": "2026-02-21T00:13:47.426438Z",
     "shell.execute_reply": "2026-02-21T00:13:47.425652Z"
    },
    "papermill": {
     "duration": 141.563119,
     "end_time": "2026-02-21T00:13:47.431760",
     "exception": false,
     "start_time": "2026-02-21T00:11:25.868641",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Tier 1: Mapping via gtdb_metadata ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gtdb_metadata mappings: 72,819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tier 1 matches: 2336/3492\n",
      "\n",
      "=== Tier 2: Fallback via taxonomy_dim ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tier 2 matches: 678 additional\n",
      "\n",
      "Total mapped: 3014/3492 taxon columns\n"
     ]
    }
   ],
   "source": [
    "# Tier 1: gtdb_metadata bridge (NCBI taxid → GTDB genus)\n",
    "print('=== Tier 1: Mapping via gtdb_metadata ===')\n",
    "ncbi_to_gtdb = spark.sql(\"\"\"\n",
    "    SELECT DISTINCT\n",
    "        CAST(m.ncbi_species_taxid AS INT) as ncbi_taxid,\n",
    "        REGEXP_EXTRACT(m.gtdb_taxonomy, 'g__([^;]+)', 1) AS gtdb_genus\n",
    "    FROM kbase_ke_pangenome.gtdb_metadata m\n",
    "    WHERE m.gtdb_taxonomy IS NOT NULL\n",
    "      AND REGEXP_EXTRACT(m.gtdb_taxonomy, 'g__([^;]+)', 1) != ''\n",
    "    UNION\n",
    "    SELECT DISTINCT\n",
    "        CAST(m.ncbi_taxid AS INT) as ncbi_taxid,\n",
    "        REGEXP_EXTRACT(m.gtdb_taxonomy, 'g__([^;]+)', 1) AS gtdb_genus\n",
    "    FROM kbase_ke_pangenome.gtdb_metadata m\n",
    "    WHERE m.gtdb_taxonomy IS NOT NULL\n",
    "      AND REGEXP_EXTRACT(m.gtdb_taxonomy, 'g__([^;]+)', 1) != ''\n",
    "\"\"\").toPandas()\n",
    "\n",
    "print(f'gtdb_metadata mappings: {len(ncbi_to_gtdb):,}')\n",
    "\n",
    "# For taxids mapping to multiple genera, take the most common\n",
    "ncbi_genus_map = ncbi_to_gtdb.groupby('ncbi_taxid')['gtdb_genus'].agg(\n",
    "    lambda x: x.value_counts().index[0]\n",
    ").to_dict()\n",
    "\n",
    "# Build Tier 1 mapping\n",
    "taxid_to_genus = {}\n",
    "tier1_hits = 0\n",
    "for col_id in taxon_cols:\n",
    "    try:\n",
    "        tid = int(col_id)\n",
    "    except (ValueError, TypeError):\n",
    "        continue\n",
    "    if tid in ncbi_genus_map:\n",
    "        taxid_to_genus[col_id] = ncbi_genus_map[tid]\n",
    "        tier1_hits += 1\n",
    "\n",
    "print(f'Tier 1 matches: {tier1_hits}/{len(taxon_cols)}')\n",
    "\n",
    "# Tier 2: Direct genus name matching via taxonomy_dim\n",
    "print('\\n=== Tier 2: Fallback via taxonomy_dim ===')\n",
    "gtdb_genus_set = set(genus_scores['gtdb_genus_name'].dropna().str.strip().str.lower())\n",
    "\n",
    "tier2_hits = 0\n",
    "for col_id in taxon_cols:\n",
    "    if col_id in taxid_to_genus:\n",
    "        continue\n",
    "    try:\n",
    "        tid = int(col_id)\n",
    "    except (ValueError, TypeError):\n",
    "        continue\n",
    "    matches = tax_dim[tax_dim['taxid'] == tid]\n",
    "    if len(matches) > 0:\n",
    "        genus = str(matches.iloc[0]['genus']).strip()\n",
    "        if genus and genus.lower() not in ('unclassified', 'nan', ''):\n",
    "            if genus.lower() in gtdb_genus_set:\n",
    "                taxid_to_genus[col_id] = genus\n",
    "                tier2_hits += 1\n",
    "\n",
    "print(f'Tier 2 matches: {tier2_hits} additional')\n",
    "print(f'\\nTotal mapped: {len(taxid_to_genus)}/{len(taxon_cols)} taxon columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T00:13:47.442668Z",
     "iopub.status.busy": "2026-02-21T00:13:47.442536Z",
     "iopub.status.idle": "2026-02-21T00:13:47.491239Z",
     "shell.execute_reply": "2026-02-21T00:13:47.490386Z"
    },
    "papermill": {
     "duration": 0.055328,
     "end_time": "2026-02-21T00:13:47.491961",
     "exception": false,
     "start_time": "2026-02-21T00:13:47.436633",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taxon IDs matched to pangenome genera with prophage scores: 3014/3492\n"
     ]
    }
   ],
   "source": [
    "# Build genus → prophage score lookups\n",
    "genus_burden_lookup = dict(zip(\n",
    "    genus_scores['gtdb_genus_name'].str.lower(),\n",
    "    genus_scores['prophage_burden']\n",
    "))\n",
    "\n",
    "# Per-module burden lookups\n",
    "genus_module_lookups = {}\n",
    "for module_id in module_ids:\n",
    "    genus_module_lookups[module_id] = dict(zip(\n",
    "        genus_scores['gtdb_genus_name'].str.lower(),\n",
    "        genus_scores[f'pct_{module_id}']\n",
    "    ))\n",
    "\n",
    "# Build matched taxon list\n",
    "matched = []\n",
    "for col_id in taxon_cols:\n",
    "    genus = taxid_to_genus.get(col_id, None)\n",
    "    if genus and genus.lower() in genus_burden_lookup:\n",
    "        matched.append((col_id, genus.lower()))\n",
    "\n",
    "print(f'Taxon IDs matched to pangenome genera with prophage scores: {len(matched)}/{len(taxon_cols)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {
    "papermill": {
     "duration": 0.004519,
     "end_time": "2026-02-21T00:13:47.501253",
     "exception": false,
     "start_time": "2026-02-21T00:13:47.496734",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. Compute Per-Sample Prophage Inference Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T00:13:47.511729Z",
     "iopub.status.busy": "2026-02-21T00:13:47.511605Z",
     "iopub.status.idle": "2026-02-21T00:25:54.545481Z",
     "shell.execute_reply": "2026-02-21T00:25:54.544521Z"
    },
    "papermill": {
     "duration": 727.146515,
     "end_time": "2026-02-21T00:25:54.652323",
     "exception": false,
     "start_time": "2026-02-21T00:13:47.505808",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed prophage scores for 6,365 samples\n",
      "\n",
      "Overall prophage score distribution:\n",
      "count    6.365000e+03\n",
      "mean     8.503221e+04\n",
      "std      8.695364e+04\n",
      "min      0.000000e+00\n",
      "25%      3.755687e+04\n",
      "50%      6.141617e+04\n",
      "75%      1.028432e+05\n",
      "max      2.435216e+06\n",
      "Name: prophage_score, dtype: float64\n",
      "\n",
      "Taxon matching coverage:\n",
      "  Median % abundance matched: 87.2%\n"
     ]
    }
   ],
   "source": [
    "# Compute per-sample prophage inference scores\n",
    "# Overall score + per-module scores\n",
    "sample_scores = []\n",
    "\n",
    "for _, row in tax_features.iterrows():\n",
    "    sample_id = row['sample_id']\n",
    "    overall_score = 0.0\n",
    "    module_scores = {m: 0.0 for m in module_ids}\n",
    "    matched_abundance = 0.0\n",
    "    total_abundance = 0.0\n",
    "    \n",
    "    for col_id, genus in matched:\n",
    "        abundance = pd.to_numeric(row.get(col_id, 0), errors='coerce')\n",
    "        if pd.notna(abundance) and abundance > 0:\n",
    "            overall_score += abundance * genus_burden_lookup.get(genus, 0)\n",
    "            matched_abundance += abundance\n",
    "            for module_id in module_ids:\n",
    "                module_scores[module_id] += abundance * genus_module_lookups[module_id].get(genus, 0)\n",
    "    \n",
    "    for col in taxon_cols:\n",
    "        val = pd.to_numeric(row.get(col, 0), errors='coerce')\n",
    "        if pd.notna(val) and val > 0:\n",
    "            total_abundance += val\n",
    "    \n",
    "    result = {\n",
    "        'sample_id': sample_id,\n",
    "        'prophage_score': overall_score,\n",
    "        'matched_abundance': matched_abundance,\n",
    "        'total_abundance': total_abundance,\n",
    "        'pct_matched': matched_abundance / total_abundance * 100 if total_abundance > 0 else 0,\n",
    "    }\n",
    "    for module_id in module_ids:\n",
    "        result[f'score_{module_id}'] = module_scores[module_id]\n",
    "    \n",
    "    sample_scores.append(result)\n",
    "\n",
    "sample_prophage = pd.DataFrame(sample_scores)\n",
    "print(f'Computed prophage scores for {len(sample_prophage):,} samples')\n",
    "print(f'\\nOverall prophage score distribution:')\n",
    "print(sample_prophage['prophage_score'].describe())\n",
    "print(f'\\nTaxon matching coverage:')\n",
    "print(f'  Median % abundance matched: {sample_prophage[\"pct_matched\"].median():.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T00:25:54.847735Z",
     "iopub.status.busy": "2026-02-21T00:25:54.847449Z",
     "iopub.status.idle": "2026-02-21T00:25:55.047421Z",
     "shell.execute_reply": "2026-02-21T00:25:55.046916Z"
    },
    "papermill": {
     "duration": 0.499491,
     "end_time": "2026-02-21T00:25:55.245429",
     "exception": false,
     "start_time": "2026-02-21T00:25:54.745938",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-module score summary:\n",
      "  A_packaging: median=288.820, mean=393.206, max=12283.852\n",
      "  B_head_morphogenesis: median=185.908, mean=266.546, max=7748.568\n",
      "  C_tail: median=211.695, mean=295.847, max=9260.947\n",
      "  D_lysis: median=288.820, mean=393.221, max=12283.852\n",
      "  E_integration: median=288.428, mean=392.756, max=12161.048\n",
      "  F_lysogenic_regulation: median=288.820, mean=393.221, max=12283.852\n",
      "  G_anti_defense: median=248.646, mean=336.430, max=11289.522\n"
     ]
    }
   ],
   "source": [
    "# Per-module score distributions\n",
    "print('Per-module score summary:')\n",
    "for module_id in module_ids:\n",
    "    col = f'score_{module_id}'\n",
    "    print(f'  {module_id}: median={sample_prophage[col].median():.3f}, '\n",
    "          f'mean={sample_prophage[col].mean():.3f}, '\n",
    "          f'max={sample_prophage[col].max():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {
    "papermill": {
     "duration": 0.00536,
     "end_time": "2026-02-21T00:25:55.350423",
     "exception": false,
     "start_time": "2026-02-21T00:25:55.345063",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5. Correlate with Abiotic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T00:25:55.647353Z",
     "iopub.status.busy": "2026-02-21T00:25:55.646948Z",
     "iopub.status.idle": "2026-02-21T00:25:57.244036Z",
     "shell.execute_reply": "2026-02-21T00:25:57.150539Z"
    },
    "papermill": {
     "duration": 1.699933,
     "end_time": "2026-02-21T00:25:57.244886",
     "exception": false,
     "start_time": "2026-02-21T00:25:55.544953",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples with both prophage scores and abiotic data: 6,365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall prophage score vs abiotic variables:\n",
      "                                 abiotic_variable score_type    n  spearman_rho       p_value\n",
      "                                   annotations_ph    overall 4366      0.474357 6.985993e-244\n",
      "               annotations_temp_has_numeric_value    overall 4587      0.399498 2.264122e-175\n",
      "      annotations_depth_has_maximum_numeric_value    overall 4973      0.352461 2.078767e-145\n",
      "  annotations_tot_nitro_content_has_numeric_value    overall 1231      0.314422  1.190946e-29\n",
      "   annotations_carb_nitro_ratio_has_numeric_value    overall  910     -0.215925  4.626414e-11\n",
      "      annotations_depth_has_minimum_numeric_value    overall  349      0.281673  8.700601e-08\n",
      "              annotations_depth_has_numeric_value    overall  517     -0.170558  9.727513e-05\n",
      "  annotations_ammonium_nitrogen_has_numeric_value    overall 1230      0.100235  4.304370e-04\n",
      "          annotations_samp_size_has_numeric_value    overall  148     -0.218260  7.700167e-03\n",
      "        annotations_diss_oxygen_has_numeric_value    overall  272      0.113607  6.133263e-02\n",
      "annotations_soluble_react_phosp_has_numeric_value    overall   32      0.199615  2.733682e-01\n",
      "             annotations_conduc_has_numeric_value    overall   70      0.080360  5.084146e-01\n",
      "          annotations_tot_phosp_has_numeric_value    overall   32      0.112609  5.394632e-01\n",
      "           annotations_ammonium_has_numeric_value    overall   33      0.109324  5.447657e-01\n",
      "        annotations_chlorophyll_has_numeric_value    overall   34     -0.055165  7.566605e-01\n"
     ]
    }
   ],
   "source": [
    "# Merge prophage scores with abiotic features\n",
    "prophage_abiotic = sample_prophage.merge(abiotic_all, on='sample_id', how='inner')\n",
    "print(f'Samples with both prophage scores and abiotic data: {len(prophage_abiotic):,}')\n",
    "\n",
    "# Identify abiotic columns\n",
    "abiotic_cols = [c for c in abiotic_all.columns if c != 'sample_id']\n",
    "\n",
    "# Correlate overall prophage score with abiotic variables\n",
    "overall_corr = []\n",
    "for col in abiotic_cols:\n",
    "    vals = pd.to_numeric(prophage_abiotic[col], errors='coerce')\n",
    "    valid = vals.notna() & (vals != 0) & prophage_abiotic['prophage_score'].notna()\n",
    "    if valid.sum() >= 30:\n",
    "        rho, p = stats.spearmanr(prophage_abiotic.loc[valid, 'prophage_score'], vals[valid])\n",
    "        overall_corr.append({\n",
    "            'abiotic_variable': col,\n",
    "            'score_type': 'overall',\n",
    "            'n': valid.sum(),\n",
    "            'spearman_rho': rho,\n",
    "            'p_value': p,\n",
    "        })\n",
    "\n",
    "overall_corr_df = pd.DataFrame(overall_corr).sort_values('p_value')\n",
    "print('\\nOverall prophage score vs abiotic variables:')\n",
    "print(overall_corr_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T00:25:57.346582Z",
     "iopub.status.busy": "2026-02-21T00:25:57.346284Z",
     "iopub.status.idle": "2026-02-21T00:26:00.747168Z",
     "shell.execute_reply": "2026-02-21T00:26:00.746428Z"
    },
    "papermill": {
     "duration": 3.500329,
     "end_time": "2026-02-21T00:26:00.748039",
     "exception": false,
     "start_time": "2026-02-21T00:25:57.247710",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Significant module-abiotic correlations (FDR < 0.05): 57\n",
      "                module_name                                abiotic_variable  spearman_rho         p_fdr\n",
      "           Packaging Module                                  annotations_ph      0.518946 2.869697e-298\n",
      "               Lysis Module                                  annotations_ph      0.518946 2.869697e-298\n",
      "Lysogenic Regulation Module                                  annotations_ph      0.518946 2.869697e-298\n",
      "         Integration Module                                  annotations_ph      0.518959 2.869697e-298\n",
      "                Tail Module                                  annotations_ph      0.497890 5.588501e-271\n",
      "        Anti-Defense Module                                  annotations_ph      0.497110 4.427506e-270\n",
      "                Tail Module     annotations_depth_has_maximum_numeric_value      0.382007 2.368981e-171\n",
      "  Head Morphogenesis Module                                  annotations_ph      0.400735 5.129427e-167\n",
      "Lysogenic Regulation Module     annotations_depth_has_maximum_numeric_value      0.360718 9.576379e-152\n",
      "               Lysis Module     annotations_depth_has_maximum_numeric_value      0.360718 9.576379e-152\n",
      "           Packaging Module     annotations_depth_has_maximum_numeric_value      0.360720 9.576379e-152\n",
      "         Integration Module     annotations_depth_has_maximum_numeric_value      0.360702 9.576379e-152\n",
      "        Anti-Defense Module     annotations_depth_has_maximum_numeric_value      0.356371 6.288947e-148\n",
      "        Anti-Defense Module              annotations_temp_has_numeric_value      0.351715 8.453988e-133\n",
      "                Tail Module              annotations_temp_has_numeric_value      0.351417 1.367282e-132\n",
      "  Head Morphogenesis Module     annotations_depth_has_maximum_numeric_value      0.336709 2.836186e-131\n",
      "         Integration Module              annotations_temp_has_numeric_value      0.331672 2.043343e-117\n",
      "           Packaging Module              annotations_temp_has_numeric_value      0.331531 2.210564e-117\n",
      "               Lysis Module              annotations_temp_has_numeric_value      0.331531 2.210564e-117\n",
      "Lysogenic Regulation Module              annotations_temp_has_numeric_value      0.331531 2.210564e-117\n",
      "  Head Morphogenesis Module              annotations_temp_has_numeric_value      0.311518 4.388531e-103\n",
      "               Lysis Module annotations_tot_nitro_content_has_numeric_value      0.333463  9.892882e-33\n",
      "Lysogenic Regulation Module annotations_tot_nitro_content_has_numeric_value      0.333463  9.892882e-33\n",
      "         Integration Module annotations_tot_nitro_content_has_numeric_value      0.333463  9.892882e-33\n",
      "           Packaging Module annotations_tot_nitro_content_has_numeric_value      0.333463  9.892882e-33\n",
      "                Tail Module annotations_tot_nitro_content_has_numeric_value      0.324806  4.771820e-31\n",
      "        Anti-Defense Module annotations_tot_nitro_content_has_numeric_value      0.324838  4.771820e-31\n",
      "  Head Morphogenesis Module annotations_tot_nitro_content_has_numeric_value      0.270635  1.565975e-21\n",
      "                Tail Module  annotations_carb_nitro_ratio_has_numeric_value     -0.278034  4.664831e-17\n",
      "           Packaging Module  annotations_carb_nitro_ratio_has_numeric_value     -0.249175  7.648686e-14\n"
     ]
    }
   ],
   "source": [
    "# Per-module correlations with abiotic variables\n",
    "module_corr_results = []\n",
    "\n",
    "for module_id in module_ids:\n",
    "    score_col = f'score_{module_id}'\n",
    "    for abiotic_col in abiotic_cols:\n",
    "        vals = pd.to_numeric(prophage_abiotic[abiotic_col], errors='coerce')\n",
    "        valid = vals.notna() & (vals != 0) & prophage_abiotic[score_col].notna()\n",
    "        if valid.sum() >= 30:\n",
    "            rho, p = stats.spearmanr(prophage_abiotic.loc[valid, score_col], vals[valid])\n",
    "            module_corr_results.append({\n",
    "                'module': module_id,\n",
    "                'module_name': MODULES[module_id]['full_name'],\n",
    "                'abiotic_variable': abiotic_col,\n",
    "                'n': valid.sum(),\n",
    "                'spearman_rho': rho,\n",
    "                'p_value': p,\n",
    "            })\n",
    "\n",
    "module_corr_df = pd.DataFrame(module_corr_results)\n",
    "\n",
    "# Multiple testing correction (FDR)\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "if len(module_corr_df) > 0:\n",
    "    reject, pvals_corrected, _, _ = multipletests(module_corr_df['p_value'], method='fdr_bh')\n",
    "    module_corr_df['p_fdr'] = pvals_corrected\n",
    "    module_corr_df['significant_fdr'] = reject\n",
    "\n",
    "# Show significant per-module correlations\n",
    "sig_module = module_corr_df[module_corr_df['significant_fdr'] == True].sort_values('p_fdr')\n",
    "print(f'\\nSignificant module-abiotic correlations (FDR < 0.05): {len(sig_module)}')\n",
    "if len(sig_module) > 0:\n",
    "    print(sig_module[['module_name', 'abiotic_variable', 'spearman_rho', 'p_fdr']].head(30).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {
    "papermill": {
     "duration": 0.194797,
     "end_time": "2026-02-21T00:26:00.948629",
     "exception": false,
     "start_time": "2026-02-21T00:26:00.753832",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6. Study-Level Analysis\n",
    "\n",
    "Compare prophage burden across NMDC study ecosystem types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T00:26:01.353107Z",
     "iopub.status.busy": "2026-02-21T00:26:01.352710Z",
     "iopub.status.idle": "2026-02-21T00:26:04.045214Z",
     "shell.execute_reply": "2026-02-21T00:26:04.043999Z"
    },
    "papermill": {
     "duration": 2.799836,
     "end_time": "2026-02-21T00:26:04.145614",
     "exception": false,
     "start_time": "2026-02-21T00:26:01.345778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"ts\": \"2026-02-21 00:26:02.693\", \"level\": \"ERROR\", \"logger\": \"SQLQueryContextLogger\", \"msg\": \"[TABLE_OR_VIEW_NOT_FOUND] The table or view `nmdc_arkin`.`biosample_set` cannot be found. Verify the spelling and correctness of the schema and catalog.\\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 3 pos 13;\\n'Distinct\\n+- 'Project ['sample_id, 'study_id]\\n   +- 'UnresolvedRelation [nmdc_arkin, biosample_set], [], false\\n\\n\\nJVM stacktrace:\\norg.apache.spark.sql.catalyst.ExtendedAnalysisException\\n\\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:91)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:306)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:299)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:299)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\n\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\n\\tat scala.util.Try$.apply(Try.scala:217)\\n\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\n\\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\\n\\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\\n\\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$5(Dataset.scala:139)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:136)\\n\\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$4(SparkSession.scala:499)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:490)\\n\\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:2764)\\n\\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:2608)\\n\\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:2499)\\n\\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:322)\\n\\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:224)\\n\\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:196)\\n\\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:341)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:341)\\n\\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\\n\\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\\n\\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:186)\\n\\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:102)\\n\\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\\n\\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:340)\\n\\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:196)\\n\\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:125)\\n\\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:347)\", \"context\": {\"errorClass\": \"TABLE_OR_VIEW_NOT_FOUND\"}, \"exception\": {\"class\": \"_MultiThreadedRendezvous\", \"msg\": \"<_MultiThreadedRendezvous of RPC that terminated with:\\n\\tstatus = StatusCode.INTERNAL\\n\\tdetails = \\\"[TABLE_OR_VIEW_NOT_FOUND] The table or view `nmdc_arkin`.`biosample_set` cannot be found. Verify the spelling and correctness of the schema and catalog.\\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 3 pos 13;\\n'Distinct\\n+- 'Project ['sample_id, 'study_id]\\n   +- 'UnresolvedRelation [nmdc_arkin, biosample_set], [], false\\n\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {grpc_message:\\\"[TABLE_OR_VIEW_NOT_FOUND] The table or view `nmdc_arkin`.`biosample_set` cannot be found. Verify the spelling and correctness of the schema and catalog.\\\\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\\\\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 3 pos 13;\\\\n\\\\'Distinct\\\\n+- \\\\'Project [\\\\'sample_id, \\\\'study_id]\\\\n   +- \\\\'UnresolvedRelation [nmdc_arkin, biosample_set], [], false\\\\n\\\", grpc_status:13}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"_execute_and_fetch_as_iterator\", \"file\": \"/usr/local/spark/python/pyspark/sql/connect/client/core.py\", \"line\": \"1523\"}, {\"class\": null, \"method\": \"__next__\", \"file\": \"<frozen _collections_abc>\", \"line\": \"360\"}, {\"class\": null, \"method\": \"send\", \"file\": \"/usr/local/spark/python/pyspark/sql/connect/client/reattach.py\", \"line\": \"138\"}, {\"class\": null, \"method\": \"_has_next\", \"file\": \"/usr/local/spark/python/pyspark/sql/connect/client/reattach.py\", \"line\": \"190\"}, {\"class\": null, \"method\": \"_has_next\", \"file\": \"/usr/local/spark/python/pyspark/sql/connect/client/reattach.py\", \"line\": \"162\"}, {\"class\": null, \"method\": \"_call_iter\", \"file\": \"/usr/local/spark/python/pyspark/sql/connect/client/reattach.py\", \"line\": \"281\"}, {\"class\": null, \"method\": \"_call_iter\", \"file\": \"/usr/local/spark/python/pyspark/sql/connect/client/reattach.py\", \"line\": \"261\"}, {\"class\": null, \"method\": \"<lambda>\", \"file\": \"/usr/local/spark/python/pyspark/sql/connect/client/reattach.py\", \"line\": \"163\"}, {\"class\": null, \"method\": \"__next__\", \"file\": \"/opt/conda/lib/python3.13/site-packages/grpc/_channel.py\", \"line\": \"538\"}, {\"class\": null, \"method\": \"_next\", \"file\": \"/opt/conda/lib/python3.13/site-packages/grpc/_channel.py\", \"line\": \"956\"}]}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biosample_set query failed: [TABLE_OR_VIEW_NOT_FOUND] The table or view `nmdc_arkin`.`biosample_set` cannot be found. Verify the spelling and correctness of the schema and catalog.\n",
      "If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\n",
      "To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 3 pos 13;\n",
      "'Distinct\n",
      "+- 'Project ['sample_id, 'study_id]\n",
      "   +- 'UnresolvedRelation [nmdc_arkin, biosample_set], [], false\n",
      "\n",
      "\n",
      "JVM stacktrace:\n",
      "org.apache.spark.sql.catalyst.ExtendedAnalysisException\n",
      "\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:91)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:306)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\n",
      "\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\n",
      "\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:299)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:299)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$5(Dataset.scala:139)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:136)\n",
      "\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$4(SparkSession.scala:499)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:490)\n",
      "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:2764)\n",
      "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:2608)\n",
      "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:2499)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:322)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:224)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:196)\n",
      "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:341)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:341)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n",
      "\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:186)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:102)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n",
      "\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:340)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:196)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:125)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:347)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"ts\": \"2026-02-21 00:26:03.945\", \"level\": \"ERROR\", \"logger\": \"SQLQueryContextLogger\", \"msg\": \"[TABLE_OR_VIEW_NOT_FOUND] The table or view `nmdc_arkin`.`omics_processing_set` cannot be found. Verify the spelling and correctness of the schema and catalog.\\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 3 pos 17;\\n'Distinct\\n+- 'Project ['sample_id, 'study_id]\\n   +- 'UnresolvedRelation [nmdc_arkin, omics_processing_set], [], false\\n\\n\\nJVM stacktrace:\\norg.apache.spark.sql.catalyst.ExtendedAnalysisException\\n\\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:91)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:306)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:299)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:299)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\n\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\n\\tat scala.util.Try$.apply(Try.scala:217)\\n\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\n\\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\\n\\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\\n\\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$5(Dataset.scala:139)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:136)\\n\\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$4(SparkSession.scala:499)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:490)\\n\\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:2764)\\n\\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:2608)\\n\\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:2499)\\n\\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:322)\\n\\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:224)\\n\\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:196)\\n\\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:341)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:341)\\n\\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\\n\\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\\n\\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:186)\\n\\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:102)\\n\\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\\n\\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:340)\\n\\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:196)\\n\\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:125)\\n\\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:347)\", \"context\": {\"errorClass\": \"TABLE_OR_VIEW_NOT_FOUND\"}, \"exception\": {\"class\": \"_MultiThreadedRendezvous\", \"msg\": \"<_MultiThreadedRendezvous of RPC that terminated with:\\n\\tstatus = StatusCode.INTERNAL\\n\\tdetails = \\\"[TABLE_OR_VIEW_NOT_FOUND] The table or view `nmdc_arkin`.`omics_processing_set` cannot be found. Verify the spelling and correctness of the schema and catalog.\\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 3 pos 17;\\n'Distinct\\n+- 'Project ['sample_id, 'study_id]\\n   +- 'UnresolvedRelation [nmdc_arkin, omics_processing_set], [], false\\n\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {grpc_message:\\\"[TABLE_OR_VIEW_NOT_FOUND] The table or view `nmdc_arkin`.`omics_processing_set` cannot be found. Verify the spelling and correctness of the schema and catalog.\\\\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\\\\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 3 pos 17;\\\\n\\\\'Distinct\\\\n+- \\\\'Project [\\\\'sample_id, \\\\'study_id]\\\\n   +- \\\\'UnresolvedRelation [nmdc_arkin, omics_processing_set], [], false\\\\n\\\", grpc_status:13}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"_execute_and_fetch_as_iterator\", \"file\": \"/usr/local/spark/python/pyspark/sql/connect/client/core.py\", \"line\": \"1523\"}, {\"class\": null, \"method\": \"__next__\", \"file\": \"<frozen _collections_abc>\", \"line\": \"360\"}, {\"class\": null, \"method\": \"send\", \"file\": \"/usr/local/spark/python/pyspark/sql/connect/client/reattach.py\", \"line\": \"138\"}, {\"class\": null, \"method\": \"_has_next\", \"file\": \"/usr/local/spark/python/pyspark/sql/connect/client/reattach.py\", \"line\": \"190\"}, {\"class\": null, \"method\": \"_has_next\", \"file\": \"/usr/local/spark/python/pyspark/sql/connect/client/reattach.py\", \"line\": \"162\"}, {\"class\": null, \"method\": \"_call_iter\", \"file\": \"/usr/local/spark/python/pyspark/sql/connect/client/reattach.py\", \"line\": \"281\"}, {\"class\": null, \"method\": \"_call_iter\", \"file\": \"/usr/local/spark/python/pyspark/sql/connect/client/reattach.py\", \"line\": \"261\"}, {\"class\": null, \"method\": \"<lambda>\", \"file\": \"/usr/local/spark/python/pyspark/sql/connect/client/reattach.py\", \"line\": \"163\"}, {\"class\": null, \"method\": \"__next__\", \"file\": \"/opt/conda/lib/python3.13/site-packages/grpc/_channel.py\", \"line\": \"538\"}, {\"class\": null, \"method\": \"_next\", \"file\": \"/opt/conda/lib/python3.13/site-packages/grpc/_channel.py\", \"line\": \"956\"}]}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "omics_processing_set also failed: [TABLE_OR_VIEW_NOT_FOUND] The table or view `nmdc_arkin`.`omics_processing_set` cannot be found. Verify the spelling and correctness of the schema and catalog.\n",
      "If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\n",
      "To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 3 pos 17;\n",
      "'Distinct\n",
      "+- 'Project ['sample_id, 'study_id]\n",
      "   +- 'UnresolvedRelation [nmdc_arkin, omics_processing_set], [], false\n",
      "\n",
      "\n",
      "JVM stacktrace:\n",
      "org.apache.spark.sql.catalyst.ExtendedAnalysisException\n",
      "\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:91)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:306)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\n",
      "\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\n",
      "\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:299)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:299)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$5(Dataset.scala:139)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:136)\n",
      "\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$4(SparkSession.scala:499)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:490)\n",
      "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:2764)\n",
      "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:2608)\n",
      "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:2499)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:322)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:224)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:196)\n",
      "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:341)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:341)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n",
      "\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:186)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:102)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n",
      "\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:340)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:196)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:125)\n",
      "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:347)\n",
      "Could not link samples to studies — skipping study-level analysis\n"
     ]
    }
   ],
   "source": [
    "# Try to link samples to studies\n",
    "# The biosample_set or omics_processing tables should have study links\n",
    "try:\n",
    "    sample_study = spark.sql(\"\"\"\n",
    "        SELECT DISTINCT sample_id, study_id\n",
    "        FROM nmdc_arkin.biosample_set\n",
    "    \"\"\").toPandas()\n",
    "    print(f'Sample-study links from biosample_set: {len(sample_study):,}')\n",
    "except Exception as e:\n",
    "    print(f'biosample_set query failed: {e}')\n",
    "    # Try alternative approach\n",
    "    try:\n",
    "        sample_study = spark.sql(\"\"\"\n",
    "            SELECT DISTINCT sample_id, study_id\n",
    "            FROM nmdc_arkin.omics_processing_set\n",
    "        \"\"\").toPandas()\n",
    "        print(f'Sample-study links from omics_processing_set: {len(sample_study):,}')\n",
    "    except Exception as e2:\n",
    "        print(f'omics_processing_set also failed: {e2}')\n",
    "        sample_study = None\n",
    "\n",
    "if sample_study is not None and len(sample_study) > 0:\n",
    "    # Merge with prophage scores and study metadata\n",
    "    study_prophage = sample_prophage.merge(sample_study, on='sample_id', how='inner')\n",
    "    study_prophage = study_prophage.merge(\n",
    "        studies[['study_id', 'name', 'ecosystem_category', 'ecosystem_type']],\n",
    "        on='study_id', how='left'\n",
    "    )\n",
    "    \n",
    "    print(f'\\nSamples linked to studies: {len(study_prophage):,}')\n",
    "    \n",
    "    # Prophage score by ecosystem type\n",
    "    eco_stats = study_prophage.groupby('ecosystem_type').agg(\n",
    "        n_samples=('sample_id', 'count'),\n",
    "        mean_prophage_score=('prophage_score', 'mean'),\n",
    "        median_prophage_score=('prophage_score', 'median'),\n",
    "    ).sort_values('median_prophage_score', ascending=False)\n",
    "    \n",
    "    print('\\nProphage score by ecosystem type:')\n",
    "    print(eco_stats.to_string())\n",
    "else:\n",
    "    print('Could not link samples to studies — skipping study-level analysis')\n",
    "    study_prophage = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {
    "papermill": {
     "duration": 0.096181,
     "end_time": "2026-02-21T00:26:04.344370",
     "exception": false,
     "start_time": "2026-02-21T00:26:04.248189",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 7. Save Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T00:26:04.745804Z",
     "iopub.status.busy": "2026-02-21T00:26:04.745439Z",
     "iopub.status.idle": "2026-02-21T00:26:08.548473Z",
     "shell.execute_reply": "2026-02-21T00:26:08.547638Z"
    },
    "papermill": {
     "duration": 4.30006,
     "end_time": "2026-02-21T00:26:08.746008",
     "exception": false,
     "start_time": "2026-02-21T00:26:04.445948",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data/nmdc_prophage_prevalence.tsv: 6,365 rows\n",
      "Saved data/nmdc_module_by_environment.tsv: 105 rows\n"
     ]
    }
   ],
   "source": [
    "# Save per-sample prophage scores\n",
    "sample_prophage.to_csv('../data/nmdc_prophage_prevalence.tsv', sep='\\t', index=False)\n",
    "print(f'Saved data/nmdc_prophage_prevalence.tsv: {len(sample_prophage):,} rows')\n",
    "\n",
    "# Save module × abiotic correlations\n",
    "module_corr_df.to_csv('../data/nmdc_module_by_environment.tsv', sep='\\t', index=False)\n",
    "print(f'Saved data/nmdc_module_by_environment.tsv: {len(module_corr_df):,} rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {
    "papermill": {
     "duration": 0.005375,
     "end_time": "2026-02-21T00:26:08.851718",
     "exception": false,
     "start_time": "2026-02-21T00:26:08.846343",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 8. Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T00:26:09.145869Z",
     "iopub.status.busy": "2026-02-21T00:26:09.145667Z",
     "iopub.status.idle": "2026-02-21T00:26:29.045025Z",
     "shell.execute_reply": "2026-02-21T00:26:28.948329Z"
    },
    "papermill": {
     "duration": 20.09876,
     "end_time": "2026-02-21T00:26:29.046145",
     "exception": false,
     "start_time": "2026-02-21T00:26:08.947385",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved figures/nmdc_prophage_vs_abiotic.png\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Figure: multi-panel prophage vs abiotic\n",
    "# Top correlations (by p-value) for overall score\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Panel 1: Distribution of prophage scores\n",
    "ax = axes[0, 0]\n",
    "ax.hist(sample_prophage['prophage_score'], bins=50, color='#E91E63', alpha=0.8, edgecolor='white')\n",
    "ax.set_xlabel('Prophage inference score')\n",
    "ax.set_ylabel('Number of samples')\n",
    "ax.set_title('Distribution of Prophage Scores')\n",
    "ax.axvline(sample_prophage['prophage_score'].median(), color='black', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Panel 2: Matching coverage\n",
    "ax = axes[0, 1]\n",
    "ax.hist(sample_prophage['pct_matched'], bins=50, color='#4CAF50', alpha=0.8, edgecolor='white')\n",
    "ax.set_xlabel('% abundance matched')\n",
    "ax.set_ylabel('Number of samples')\n",
    "ax.set_title('Pangenome Matching Coverage')\n",
    "\n",
    "# Panels 3-6: Top 4 abiotic correlations\n",
    "if len(overall_corr_df) > 0:\n",
    "    top_corr = overall_corr_df.head(4)\n",
    "    panel_positions = [(0, 2), (1, 0), (1, 1), (1, 2)]\n",
    "    \n",
    "    for i, (_, corr_row) in enumerate(top_corr.iterrows()):\n",
    "        if i >= len(panel_positions):\n",
    "            break\n",
    "        r, c = panel_positions[i]\n",
    "        ax = axes[r, c]\n",
    "        col = corr_row['abiotic_variable']\n",
    "        vals = pd.to_numeric(prophage_abiotic[col], errors='coerce')\n",
    "        valid = vals.notna() & (vals != 0) & prophage_abiotic['prophage_score'].notna()\n",
    "        \n",
    "        ax.scatter(vals[valid], prophage_abiotic.loc[valid, 'prophage_score'],\n",
    "                   alpha=0.2, s=10, color='#E91E63')\n",
    "        clean_name = col.replace('annotations_', '').replace('_has_numeric_value', '')\n",
    "        ax.set_xlabel(clean_name)\n",
    "        ax.set_ylabel('Prophage score')\n",
    "        ax.set_title(f'rho={corr_row[\"spearman_rho\"]:.3f}, p={corr_row[\"p_value\"]:.1e}')\n",
    "\n",
    "plt.suptitle('NMDC: Prophage Burden vs Environmental Variables', fontsize=13, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/nmdc_prophage_vs_abiotic.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved figures/nmdc_prophage_vs_abiotic.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T00:26:29.246082Z",
     "iopub.status.busy": "2026-02-21T00:26:29.245347Z",
     "iopub.status.idle": "2026-02-21T00:26:38.244631Z",
     "shell.execute_reply": "2026-02-21T00:26:38.147070Z"
    },
    "papermill": {
     "duration": 9.292036,
     "end_time": "2026-02-21T00:26:38.345015",
     "exception": false,
     "start_time": "2026-02-21T00:26:29.052979",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved figures/nmdc_module_abiotic_heatmap.png\n"
     ]
    }
   ],
   "source": [
    "# Figure: Per-module abiotic correlation heatmap\n",
    "if len(sig_module) > 0:\n",
    "    # Pivot: module × abiotic variable → rho\n",
    "    pivot = module_corr_df.pivot_table(\n",
    "        index='module_name', columns='abiotic_variable', values='spearman_rho'\n",
    "    )\n",
    "    \n",
    "    # Clean column names\n",
    "    pivot.columns = [c.replace('annotations_', '').replace('_has_numeric_value', '')\n",
    "                     for c in pivot.columns]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    sns.heatmap(pivot, cmap='RdBu_r', center=0, annot=True, fmt='.2f', ax=ax,\n",
    "                cbar_kws={'label': 'Spearman rho'})\n",
    "    ax.set_title('Per-Module Correlation with Abiotic Variables')\n",
    "    ax.set_ylabel('')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../figures/nmdc_module_abiotic_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print('Saved figures/nmdc_module_abiotic_heatmap.png')\n",
    "else:\n",
    "    print('No significant module-abiotic correlations to visualize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cell-23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T00:26:38.945847Z",
     "iopub.status.busy": "2026-02-21T00:26:38.945529Z",
     "iopub.status.idle": "2026-02-21T00:26:41.744978Z",
     "shell.execute_reply": "2026-02-21T00:26:41.448435Z"
    },
    "papermill": {
     "duration": 3.498701,
     "end_time": "2026-02-21T00:26:42.144830",
     "exception": false,
     "start_time": "2026-02-21T00:26:38.646129",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "NB05 SUMMARY\n",
      "============================================================\n",
      "NMDC samples scored: 6,365\n",
      "Median matching coverage: 87.2%\n",
      "Genera matched to pangenome: 3014/3492\n",
      "\n",
      "Overall prophage score significant correlations (p<0.05): 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ph: rho=0.474, p=6.99e-244\n",
      "  temp: rho=0.399, p=2.26e-175\n",
      "  depth_has_maximum_numeric_value: rho=0.352, p=2.08e-145\n",
      "  tot_nitro_content: rho=0.314, p=1.19e-29\n",
      "  carb_nitro_ratio: rho=-0.216, p=4.63e-11\n",
      "\n",
      "Per-module significant correlations (FDR<0.05): 57\n",
      "\n",
      "Files saved:\n",
      "  data/nmdc_prophage_prevalence.tsv (6,365 rows)\n",
      "  data/nmdc_module_by_environment.tsv (105 rows)\n",
      "  figures/nmdc_prophage_vs_abiotic.png\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "print('='*60)\n",
    "print('NB05 SUMMARY')\n",
    "print('='*60)\n",
    "print(f'NMDC samples scored: {len(sample_prophage):,}')\n",
    "print(f'Median matching coverage: {sample_prophage[\"pct_matched\"].median():.1f}%')\n",
    "print(f'Genera matched to pangenome: {len(matched)}/{len(taxon_cols)}')\n",
    "\n",
    "n_sig_overall = (overall_corr_df['p_value'] < 0.05).sum() if len(overall_corr_df) > 0 else 0\n",
    "print(f'\\nOverall prophage score significant correlations (p<0.05): {n_sig_overall}')\n",
    "if len(overall_corr_df) > 0:\n",
    "    for _, row in overall_corr_df.head(5).iterrows():\n",
    "        clean = row['abiotic_variable'].replace('annotations_', '').replace('_has_numeric_value', '')\n",
    "        print(f'  {clean}: rho={row[\"spearman_rho\"]:.3f}, p={row[\"p_value\"]:.2e}')\n",
    "\n",
    "n_sig_module = len(sig_module) if len(module_corr_df) > 0 else 0\n",
    "print(f'\\nPer-module significant correlations (FDR<0.05): {n_sig_module}')\n",
    "\n",
    "print(f'\\nFiles saved:')\n",
    "print(f'  data/nmdc_prophage_prevalence.tsv ({len(sample_prophage):,} rows)')\n",
    "print(f'  data/nmdc_module_by_environment.tsv ({len(module_corr_df):,} rows)')\n",
    "print(f'  figures/nmdc_prophage_vs_abiotic.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 946.521918,
   "end_time": "2026-02-21T00:26:51.345489",
   "environment_variables": {},
   "exception": null,
   "input_path": "05_nmdc_environmental_analysis.ipynb",
   "output_path": "05_nmdc_environmental_analysis.ipynb",
   "parameters": {},
   "start_time": "2026-02-21T00:11:04.823571",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}