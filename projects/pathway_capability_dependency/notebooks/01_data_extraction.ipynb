{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# NB01: Data Extraction\n\n**Requires BERDL JupyterHub** \u2014 `get_spark_session()` only available there.\n\n**Purpose**: Extract and cache all data needed for the Metabolic Capability vs Dependency analysis.\nLeverages upstream project data where available (essential_metabolome, essential_genome).\n\n**v2 changes** (2026-02-19):\n- Added GapMind-FB organism coverage verification (essential_metabolome found only 7/45 covered)\n- Added condition-type fitness extraction (per field_vs_lab_fitness methodology)\n- Added KEGG KO-based annotations (more precise than KEGG pathway maps)\n- Fixed: large extracts use Spark `.write.csv()` instead of `.toPandas()` to avoid OOM\n- Upstream data check: reuse fb_genome_mapping from essential_metabolome\n\n**Outputs**:\n- `data/gapmind_genome_pathway_status.csv` \u2014 Per-genome pathway completeness (best score, scope=all)\n- `data/gapmind_core_pathway_status.csv` \u2014 Per-genome pathway completeness (core genes only)\n- `data/species_pathway_summary/` \u2014 Species-level pathway completion fractions (partitioned Spark output)\n- `data/pangenome_stats.csv` \u2014 Pangenome openness metrics\n- `data/fb_fitness_summary.csv` \u2014 Per-gene fitness summary for FB organisms\n- `data/fb_fitness_by_condition_type.csv` \u2014 Per-gene fitness stratified by condition type\n- `data/fb_kegg_annotations.csv` \u2014 KEGG KO + pathway annotations for metabolic gene clusters\n- `data/fb_essential_genes.csv` \u2014 Putative essential genes (no transposon insertions)\n- `data/gapmind_fb_coverage.csv` \u2014 GapMind coverage for each FB organism"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize Spark session\nspark = get_spark_session()\n\nimport os\nimport pandas as pd\n\n# Output directory\nDATA_DIR = '../data'\nos.makedirs(DATA_DIR, exist_ok=True)\n\n# Upstream project data paths\nUPSTREAM = {\n    'essential_metabolome': '../../essential_metabolome/data',\n    'essential_genome': '../../essential_genome/data',\n    'conservation_vs_fitness': '../../conservation_vs_fitness/data',\n    'fitness_effects_conservation': '../../fitness_effects_conservation/data',\n}\n\nprint('Spark session ready')\nprint(f'Output directory: {os.path.abspath(DATA_DIR)}')\nprint()\n\n# Check upstream data availability\nfor project, path in UPSTREAM.items():\n    if os.path.exists(path):\n        files = os.listdir(path)\n        print(f'  {project}: {len(files)} files available')\n        for f in sorted(files):\n            size = os.path.getsize(os.path.join(path, f)) / 1e6\n            print(f'    {f} ({size:.1f} MB)')\n    else:\n        print(f'  {project}: NOT FOUND locally (may need JupyterHub extraction)')\nprint()\n\n# Load FB-genome mapping from essential_metabolome if available\nfb_genome_map_path = os.path.join(UPSTREAM['essential_metabolome'], 'fb_genome_mapping_manual.tsv')\nif os.path.exists(fb_genome_map_path):\n    fb_genome_map = pd.read_csv(fb_genome_map_path, sep='\\t')\n    print(f'Loaded FB-genome mapping: {len(fb_genome_map)} organisms')\n    print(fb_genome_map[['orgId', 'genome_id']].head())\nelse:\n    fb_genome_map = None\n    print('FB-genome mapping not found \u2014 will query from BERDL')"
  },
  {
   "cell_type": "markdown",
   "source": "## 1a. GapMind-FB Organism Coverage Check\n\n**Critical**: essential_metabolome found only 7 of 45 FB organisms had GapMind data.\nVerify coverage for all 48 FB organisms before designing the analysis.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "output_path = f'{DATA_DIR}/gapmind_fb_coverage.csv'\n\nif os.path.exists(output_path):\n    print(f'Already exists: {output_path}, loading...')\n    coverage = pd.read_csv(output_path)\nelse:\n    print('Checking GapMind coverage for FB organisms...')\n    \n    # Get all FB organisms\n    fb_orgs = spark.sql(\"\"\"\n        SELECT DISTINCT orgId FROM kescience_fitnessbrowser.gene\n        WHERE CAST(type AS INT) = 1\n    \"\"\").toPandas()\n    \n    # Get all distinct clade_names in GapMind\n    gapmind_clades = spark.sql(\"\"\"\n        SELECT DISTINCT clade_name FROM kbase_ke_pangenome.gapmind_pathways\n    \"\"\").toPandas()\n    \n    # Get organism -> species mapping from FB gene table + pangenome\n    # FB organisms are mapped to GTDB species clades via the link table\n    # For now, check GapMind clade_names against known FB species\n    \n    # Also check: how many distinct genomes per clade in GapMind?\n    gapmind_genome_counts = spark.sql(\"\"\"\n        SELECT clade_name, COUNT(DISTINCT genome_id) as n_genomes,\n               COUNT(DISTINCT pathway) as n_pathways\n        FROM kbase_ke_pangenome.gapmind_pathways\n        WHERE sequence_scope = 'all'\n        GROUP BY clade_name\n    \"\"\").toPandas()\n    \n    print(f'Total FB organisms: {len(fb_orgs)}')\n    print(f'Total GapMind clades: {len(gapmind_clades)}')\n    print(f'GapMind clades with data: {len(gapmind_genome_counts)}')\n    \n    # If we have the FB-genome mapping, check coverage directly\n    if fb_genome_map is not None:\n        # Strip RS_/GB_ prefix from genome_id to match GapMind format\n        fb_genome_map['gapmind_genome_id'] = fb_genome_map['genome_id'].str.replace(r'^(RS_|GB_)', '', regex=True)\n        \n        # Check which FB genomes appear in GapMind\n        gapmind_genomes = spark.sql(\"\"\"\n            SELECT DISTINCT genome_id FROM kbase_ke_pangenome.gapmind_pathways\n        \"\"\").toPandas()\n        \n        fb_genome_map['in_gapmind'] = fb_genome_map['gapmind_genome_id'].isin(gapmind_genomes['genome_id'])\n        coverage = fb_genome_map[['orgId', 'genome_id', 'gapmind_genome_id', 'in_gapmind']]\n        \n        n_covered = coverage['in_gapmind'].sum()\n        print(f'\\nFB organisms with GapMind data: {n_covered}/{len(coverage)}')\n        print(f'Coverage rate: {n_covered/len(coverage)*100:.1f}%')\n        print('\\nCovered:')\n        print(coverage[coverage['in_gapmind']])\n        print('\\nNOT covered:')\n        print(coverage[~coverage['in_gapmind']])\n    else:\n        print('No FB-genome mapping available \u2014 cannot check per-organism coverage')\n        coverage = gapmind_genome_counts\n    \n    coverage.to_csv(output_path, index=False)\n    print(f'\\nSaved: {output_path}')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. GapMind Pathway Status (All Genes)\n",
    "\n",
    "Aggregate `gapmind_pathways` to get the best pathway score per genome \u00d7 pathway.\n",
    "The table has multiple rows per pair (different sequence scopes and alternative routes).\n",
    "We take the maximum score across all routes within each sequence scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = f'{DATA_DIR}/gapmind_genome_pathway_status.csv'\n",
    "\n",
    "if os.path.exists(output_path):\n",
    "    print(f'Already exists: {output_path}, skipping')\n",
    "else:\n",
    "    print('Extracting GapMind pathway status (scope=all)...')\n",
    "    gapmind_all = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            genome_id,\n",
    "            clade_name,\n",
    "            pathway,\n",
    "            metabolic_category,\n",
    "            MAX(score) as best_score,\n",
    "            MAX(score_simplified) as is_complete,\n",
    "            MAX(nHi) as max_nHi,\n",
    "            MAX(CASE WHEN score_category = 'complete' THEN 1 ELSE 0 END) as is_fully_complete,\n",
    "            MAX(CASE WHEN score_category = 'likely_complete' THEN 1 ELSE 0 END) as is_likely_complete\n",
    "        FROM kbase_ke_pangenome.gapmind_pathways\n",
    "        WHERE sequence_scope = 'all'\n",
    "        GROUP BY genome_id, clade_name, pathway, metabolic_category\n",
    "    \"\"\")\n",
    "    \n",
    "    n_rows = gapmind_all.count()\n",
    "    print(f'Aggregated rows: {n_rows:,}')\n",
    "    \n",
    "    # This should be ~293K genomes \u00d7 80 pathways = ~23.4M rows\n",
    "    # Safe to collect to pandas for CSV export\n",
    "    gapmind_all_pd = gapmind_all.toPandas()\n",
    "    gapmind_all_pd.to_csv(output_path, index=False)\n",
    "    print(f'Saved: {output_path} ({len(gapmind_all_pd):,} rows)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GapMind Pathway Status (Core Genes Only)\n",
    "\n",
    "Same aggregation but for `sequence_scope = 'core'` only.\n",
    "Pathways that are complete with all genes but incomplete with core genes only\n",
    "indicate that accessory genes are required \u2014 these are interesting for the BQH analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = f'{DATA_DIR}/gapmind_core_pathway_status.csv'\n",
    "\n",
    "if os.path.exists(output_path):\n",
    "    print(f'Already exists: {output_path}, skipping')\n",
    "else:\n",
    "    print('Extracting GapMind pathway status (scope=core)...')\n",
    "    gapmind_core = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            genome_id,\n",
    "            clade_name,\n",
    "            pathway,\n",
    "            metabolic_category,\n",
    "            MAX(score) as best_score_core,\n",
    "            MAX(score_simplified) as is_complete_core\n",
    "        FROM kbase_ke_pangenome.gapmind_pathways\n",
    "        WHERE sequence_scope = 'core'\n",
    "        GROUP BY genome_id, clade_name, pathway, metabolic_category\n",
    "    \"\"\")\n",
    "    \n",
    "    gapmind_core_pd = gapmind_core.toPandas()\n",
    "    gapmind_core_pd.to_csv(output_path, index=False)\n",
    "    print(f'Saved: {output_path} ({len(gapmind_core_pd):,} rows)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Species-Level Pathway Summary\n",
    "\n",
    "Aggregate pathway completeness to species level: for each species \u00d7 pathway,\n",
    "what fraction of genomes have the pathway complete?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "output_path = f'{DATA_DIR}/species_pathway_summary'\noutput_csv = f'{DATA_DIR}/species_pathway_summary.csv'\n\nif os.path.exists(output_csv):\n    print(f'Already exists: {output_csv}, skipping')\nelse:\n    print('Computing species-level pathway summary...')\n    print('NOTE: This produces ~27K species \u00d7 80 pathways = ~2M rows. Using Spark write.')\n    species_summary = spark.sql(\"\"\"\n        WITH genome_pathway AS (\n            SELECT \n                genome_id,\n                clade_name,\n                pathway,\n                metabolic_category,\n                MAX(score_simplified) as is_complete\n            FROM kbase_ke_pangenome.gapmind_pathways\n            WHERE sequence_scope = 'all'\n            GROUP BY genome_id, clade_name, pathway, metabolic_category\n        )\n        SELECT \n            clade_name,\n            pathway,\n            metabolic_category,\n            COUNT(*) as n_genomes,\n            SUM(CAST(is_complete AS INT)) as n_complete,\n            AVG(is_complete) as frac_complete\n        FROM genome_pathway\n        GROUP BY clade_name, pathway, metabolic_category\n    \"\"\")\n    \n    n_rows = species_summary.count()\n    print(f'Aggregated rows: {n_rows:,}')\n    \n    # This is ~2M rows \u2014 safe for toPandas but write as Spark first for safety\n    species_summary_pd = species_summary.toPandas()\n    species_summary_pd.to_csv(output_csv, index=False)\n    print(f'Saved: {output_csv} ({len(species_summary_pd):,} rows)')\n    \n    n_species = species_summary_pd['clade_name'].nunique()\n    n_pathways = species_summary_pd['pathway'].nunique()\n    print(f'Species: {n_species:,}, Pathways: {n_pathways}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pangenome Stats\n",
    "\n",
    "Extract pangenome openness metrics for all species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = f'{DATA_DIR}/pangenome_stats.csv'\n",
    "\n",
    "if os.path.exists(output_path):\n",
    "    print(f'Already exists: {output_path}, skipping')\n",
    "else:\n",
    "    print('Extracting pangenome stats...')\n",
    "    pangenome = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            gtdb_species_clade_id,\n",
    "            no_genomes,\n",
    "            no_gene_clusters,\n",
    "            no_core,\n",
    "            no_aux_genome,\n",
    "            no_singleton_gene_clusters\n",
    "        FROM kbase_ke_pangenome.pangenome\n",
    "    \"\"\")\n",
    "    \n",
    "    pangenome_pd = pangenome.toPandas()\n",
    "    \n",
    "    # Convert to numeric (may come as strings)\n",
    "    for col in ['no_genomes', 'no_gene_clusters', 'no_core', 'no_aux_genome', 'no_singleton_gene_clusters']:\n",
    "        pangenome_pd[col] = pd.to_numeric(pangenome_pd[col], errors='coerce')\n",
    "    \n",
    "    # Calculate openness metrics\n",
    "    pangenome_pd['frac_singleton'] = pangenome_pd['no_singleton_gene_clusters'] / pangenome_pd['no_gene_clusters']\n",
    "    pangenome_pd['frac_core'] = pangenome_pd['no_core'] / pangenome_pd['no_gene_clusters']\n",
    "    pangenome_pd['frac_accessory'] = pangenome_pd['no_aux_genome'] / pangenome_pd['no_gene_clusters']\n",
    "    \n",
    "    pangenome_pd.to_csv(output_path, index=False)\n",
    "    print(f'Saved: {output_path} ({len(pangenome_pd):,} rows)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Fitness Browser \u2014 Gene Fitness Summary\n\nFor each FB organism, extract per-gene fitness summary statistics across all conditions.\n**v2**: Added fitness breadth metrics per fitness_effects_conservation methodology:\nbreadth = fraction of conditions where gene has |fitness| > 1."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "output_path = f'{DATA_DIR}/fb_fitness_summary.csv'\n\nif os.path.exists(output_path):\n    print(f'Already exists: {output_path}, skipping')\nelse:\n    print('Extracting FB fitness summary...')\n    # v2: Added fitness_breadth (fraction of conditions with strong phenotype)\n    # per fitness_effects_conservation methodology\n    fb_fitness = spark.sql(\"\"\"\n        SELECT \n            orgId,\n            locusId,\n            COUNT(*) as n_conditions,\n            AVG(CAST(fit AS FLOAT)) as mean_fitness,\n            PERCENTILE_APPROX(CAST(fit AS FLOAT), 0.5) as median_fitness,\n            MIN(CAST(fit AS FLOAT)) as min_fitness,\n            MAX(CAST(fit AS FLOAT)) as max_fitness,\n            STDDEV(CAST(fit AS FLOAT)) as std_fitness,\n            SUM(CASE WHEN ABS(CAST(fit AS FLOAT)) > 1.0 THEN 1 ELSE 0 END) as n_strong_phenotype,\n            SUM(CASE WHEN CAST(fit AS FLOAT) < -1.0 THEN 1 ELSE 0 END) as n_sick,\n            SUM(CASE WHEN CAST(fit AS FLOAT) > 1.0 THEN 1 ELSE 0 END) as n_beneficial,\n            SUM(CASE WHEN ABS(CAST(fit AS FLOAT)) > 2.0 THEN 1 ELSE 0 END) as n_very_strong,\n            -- v2: explicit breadth metrics\n            SUM(CASE WHEN ABS(CAST(fit AS FLOAT)) > 1.0 THEN 1 ELSE 0 END) / COUNT(*) as fitness_breadth,\n            SUM(CASE WHEN CAST(fit AS FLOAT) < -1.0 THEN 1 ELSE 0 END) / COUNT(*) as sick_breadth,\n            SUM(CASE WHEN CAST(fit AS FLOAT) < -2.0 THEN 1 ELSE 0 END) / COUNT(*) as severe_sick_breadth\n        FROM kescience_fitnessbrowser.genefitness\n        GROUP BY orgId, locusId\n    \"\"\")\n    \n    fb_fitness_pd = fb_fitness.toPandas()\n    fb_fitness_pd.to_csv(output_path, index=False)\n    print(f'Saved: {output_path} ({len(fb_fitness_pd):,} rows)')\n    print(f'Organisms: {fb_fitness_pd[\"orgId\"].nunique()}')\n    print(f'\\nFitness breadth distribution:')\n    print(fb_fitness_pd['fitness_breadth'].describe())"
  },
  {
   "cell_type": "markdown",
   "source": "## 5b. Condition-Type Fitness (v2)\n\n**New in v2**: Per field_vs_lab_fitness and core_gene_tradeoffs, condition type matters.\nGenes important under stress may appear neutral in rich media.\nExtract experiment metadata and compute per-gene fitness by condition type.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "output_path = f'{DATA_DIR}/fb_fitness_by_condition_type.csv'\n",
    "\n",
    "if os.path.exists(output_path):\n",
    "    print(f'Already exists: {output_path}, skipping')\n",
    "else:\n",
    "    print('Extracting condition-type fitness...')\n",
    "    \n",
    "    # Classify experiments by condition type using the experiment table\n",
    "    # Note: table is 'experiment' (not 'exps'), columns are expName, expGroup, condition_1\n",
    "    # See docs/pitfalls.md [pathway_capability_dependency] for details\n",
    "    # Classification logic adapted from field_vs_lab_fitness project\n",
    "    cond_fitness = spark.sql(\"\"\"\n",
    "        WITH exp_types AS (\n",
    "            SELECT\n",
    "                expName,\n",
    "                orgId,\n",
    "                CASE\n",
    "                    WHEN LOWER(condition_1) LIKE '%carbon%'\n",
    "                         OR LOWER(expGroup) LIKE '%carbon%'\n",
    "                         OR LOWER(condition_1) LIKE '%glucose%'\n",
    "                         OR LOWER(condition_1) LIKE '%sugar%'\n",
    "                         OR LOWER(condition_1) LIKE '%acetate%'\n",
    "                         OR LOWER(condition_1) LIKE '%lactate%'\n",
    "                         OR LOWER(condition_1) LIKE '%glycerol%'\n",
    "                         THEN 'carbon_source'\n",
    "                    WHEN LOWER(condition_1) LIKE '%nitrogen%'\n",
    "                         OR LOWER(expGroup) LIKE '%nitrogen%'\n",
    "                         OR LOWER(condition_1) LIKE '%ammonium%'\n",
    "                         OR LOWER(condition_1) LIKE '%amino acid%'\n",
    "                         THEN 'nitrogen_source'\n",
    "                    WHEN LOWER(condition_1) LIKE '%stress%'\n",
    "                         OR LOWER(condition_1) LIKE '%metal%'\n",
    "                         OR LOWER(condition_1) LIKE '%antibiotic%'\n",
    "                         OR LOWER(condition_1) LIKE '%salt%'\n",
    "                         OR LOWER(condition_1) LIKE '%pH%'\n",
    "                         OR LOWER(condition_1) LIKE '%temperature%'\n",
    "                         OR LOWER(condition_1) LIKE '%oxidative%'\n",
    "                         OR LOWER(condition_1) LIKE '%osmotic%'\n",
    "                         THEN 'stress'\n",
    "                    ELSE 'other'\n",
    "                END as condition_type\n",
    "            FROM kescience_fitnessbrowser.experiment\n",
    "        )\n",
    "        SELECT \n",
    "            gf.orgId,\n",
    "            gf.locusId,\n",
    "            et.condition_type,\n",
    "            COUNT(*) as n_conditions,\n",
    "            AVG(CAST(gf.fit AS FLOAT)) as mean_fitness,\n",
    "            PERCENTILE_APPROX(CAST(gf.fit AS FLOAT), 0.5) as median_fitness,\n",
    "            MIN(CAST(gf.fit AS FLOAT)) as min_fitness,\n",
    "            SUM(CASE WHEN CAST(gf.fit AS FLOAT) < -1.0 THEN 1 ELSE 0 END) as n_sick,\n",
    "            SUM(CASE WHEN ABS(CAST(gf.fit AS FLOAT)) > 1.0 THEN 1 ELSE 0 END) as n_strong\n",
    "        FROM kescience_fitnessbrowser.genefitness gf\n",
    "        JOIN exp_types et ON gf.expName = et.expName AND gf.orgId = et.orgId\n",
    "        GROUP BY gf.orgId, gf.locusId, et.condition_type\n",
    "    \"\"\")\n",
    "    \n",
    "    cond_fitness_pd = cond_fitness.toPandas()\n",
    "    cond_fitness_pd.to_csv(output_path, index=False)\n",
    "    print(f'Saved: {output_path} ({len(cond_fitness_pd):,} rows)')\n",
    "    print(f'\\nCondition type distribution:')\n",
    "    print(cond_fitness_pd.groupby('condition_type')['n_conditions'].sum())\n",
    "    print(f'\\nOrganism \u00d7 condition type pairs: {cond_fitness_pd.groupby([\"orgId\", \"condition_type\"]).ngroups}')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. FB-Pangenome Link Table\n",
    "\n",
    "Check if the link table exists from the `conservation_vs_fitness` project.\n",
    "If not available locally, we'll need to rebuild it or copy from JupyterHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for existing link table from conservation_vs_fitness\n",
    "link_paths = [\n",
    "    '../data/fb_pangenome_link.tsv',\n",
    "    '../../conservation_vs_fitness/data/fb_pangenome_link.tsv',\n",
    "    '../../essential_genome/data/all_ortholog_groups.csv',\n",
    "]\n",
    "\n",
    "found = None\n",
    "for p in link_paths:\n",
    "    if os.path.exists(p):\n",
    "        found = p\n",
    "        print(f'Found link data: {p}')\n",
    "        df = pd.read_csv(p, sep='\\t' if p.endswith('.tsv') else ',')\n",
    "        print(f'  Rows: {len(df):,}')\n",
    "        print(f'  Columns: {list(df.columns)}')\n",
    "        print(f'  Organisms: {df.iloc[:, 1].nunique() if len(df.columns) > 1 else \"?\"}')\n",
    "        break\n",
    "\n",
    "if not found:\n",
    "    print('No link table found locally.')\n",
    "    print('The link table must be rebuilt or copied from the conservation_vs_fitness project.')\n",
    "    print('For now, we will use the ortholog groups from essential_genome as a starting point.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. KEGG Annotations for Metabolic Gene Clusters\n\nExtract eggNOG annotations (**KEGG orthologs/KOs** and EC numbers) for gene clusters.\n\n**v2 change**: Extract KEGG_ko (KO numbers) in addition to KEGG_Pathway map IDs.\nKOs provide more precise functional assignment than broad KEGG pathway maps.\nThis addresses the lossy KEGG map \u2192 GapMind pathway mapping from v1."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "output_path = f'{DATA_DIR}/fb_kegg_annotations.csv'\n\nif os.path.exists(output_path):\n    print(f'Already exists: {output_path}, skipping')\nelse:\n    # v2: Extract KEGG_ko (orthologs) + EC + KEGG_Pathway for metabolic genes\n    # Use broader filter: any gene with KEGG_ko or EC annotation in metabolic categories\n    # This captures more genes than filtering on KEGG_Pathway alone\n    \n    print('Extracting KEGG KO + pathway annotations for metabolic genes...')\n    print('This queries the 93M-row eggnog_mapper_annotations table...')\n    \n    kegg_annot = spark.sql(\"\"\"\n        SELECT \n            query_name as gene_cluster_id,\n            EC,\n            KEGG_ko,\n            KEGG_Pathway,\n            COG_category,\n            Description\n        FROM kbase_ke_pangenome.eggnog_mapper_annotations\n        WHERE (\n            -- Amino acid biosynthesis/metabolism KEGG pathways\n            KEGG_Pathway LIKE '%map00220%'   -- Arginine biosynthesis\n            OR KEGG_Pathway LIKE '%map00260%' -- Glycine, serine, threonine\n            OR KEGG_Pathway LIKE '%map00270%' -- Cysteine and methionine\n            OR KEGG_Pathway LIKE '%map00290%' -- Valine, leucine, isoleucine biosynthesis\n            OR KEGG_Pathway LIKE '%map00300%' -- Lysine biosynthesis\n            OR KEGG_Pathway LIKE '%map00330%' -- Arginine and proline metabolism\n            OR KEGG_Pathway LIKE '%map00340%' -- Histidine metabolism\n            OR KEGG_Pathway LIKE '%map00350%' -- Tyrosine metabolism\n            OR KEGG_Pathway LIKE '%map00360%' -- Phenylalanine metabolism\n            OR KEGG_Pathway LIKE '%map00380%' -- Tryptophan metabolism\n            OR KEGG_Pathway LIKE '%map00400%' -- Phe, Tyr, Trp biosynthesis\n            OR KEGG_Pathway LIKE '%map00250%' -- Alanine, aspartate, glutamate\n            -- Carbon source utilization KEGG pathways\n            OR KEGG_Pathway LIKE '%map00010%' -- Glycolysis\n            OR KEGG_Pathway LIKE '%map00020%' -- TCA cycle\n            OR KEGG_Pathway LIKE '%map00030%' -- Pentose phosphate\n            OR KEGG_Pathway LIKE '%map00040%' -- Pentose and glucuronate\n            OR KEGG_Pathway LIKE '%map00051%' -- Fructose and mannose\n            OR KEGG_Pathway LIKE '%map00052%' -- Galactose\n            OR KEGG_Pathway LIKE '%map00500%' -- Starch and sucrose\n            OR KEGG_Pathway LIKE '%map00520%' -- Amino sugar and nucleotide sugar\n            OR KEGG_Pathway LIKE '%map00620%' -- Pyruvate metabolism\n            OR KEGG_Pathway LIKE '%map00630%' -- Glyoxylate and dicarboxylate\n            OR KEGG_Pathway LIKE '%map00640%' -- Propanoate metabolism\n            OR KEGG_Pathway LIKE '%map00650%' -- Butanoate metabolism\n            OR KEGG_Pathway LIKE '%map00660%' -- C5-Branched dibasic acid\n            OR KEGG_Pathway LIKE '%map00562%' -- Inositol phosphate\n            -- v2: Also capture genes with metabolic KO annotations\n            -- even if they lack KEGG_Pathway assignment\n            OR COG_category LIKE '%E%'  -- Amino acid transport and metabolism\n            OR COG_category LIKE '%G%'  -- Carbohydrate transport and metabolism\n            OR COG_category LIKE '%C%'  -- Energy production and conversion\n        )\n        AND (KEGG_ko != '-' OR EC != '-' OR KEGG_Pathway != '-')\n    \"\"\")\n    \n    kegg_pd = kegg_annot.toPandas()\n    kegg_pd.to_csv(output_path, index=False)\n    print(f'Saved: {output_path} ({len(kegg_pd):,} rows)')\n    print(f'Unique gene clusters: {kegg_pd[\"gene_cluster_id\"].nunique():,}')\n    print(f'\\nWith KEGG_ko: {(kegg_pd[\"KEGG_ko\"] != \"-\").sum():,}')\n    print(f'With EC: {(kegg_pd[\"EC\"] != \"-\").sum():,}')\n    print(f'With KEGG_Pathway: {(kegg_pd[\"KEGG_Pathway\"] != \"-\").sum():,}')\n    print(f'\\nCOG category distribution:')\n    print(kegg_pd['COG_category'].value_counts().head(10))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Identify Essential Genes (No Fitness Data = Putative Essential)\n",
    "\n",
    "Genes present in the FB gene table but absent from genefitness are putative essentials\n",
    "(no viable transposon insertions). These represent the strongest form of dependency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = f'{DATA_DIR}/fb_essential_genes.csv'\n",
    "\n",
    "if os.path.exists(output_path):\n",
    "    print(f'Already exists: {output_path}, skipping')\n",
    "else:\n",
    "    print('Identifying putative essential genes...')\n",
    "    essentials = spark.sql(\"\"\"\n",
    "        SELECT g.orgId, g.locusId, g.sysName, g.gene as gene_name, g.desc as gene_desc\n",
    "        FROM kescience_fitnessbrowser.gene g\n",
    "        LEFT JOIN (\n",
    "            SELECT DISTINCT orgId, locusId FROM kescience_fitnessbrowser.genefitness\n",
    "        ) gf ON g.orgId = gf.orgId AND g.locusId = gf.locusId\n",
    "        WHERE CAST(g.type AS INT) = 1  -- protein-coding genes only\n",
    "          AND gf.locusId IS NULL  -- no fitness data = putative essential\n",
    "    \"\"\")\n",
    "    \n",
    "    essentials_pd = essentials.toPandas()\n",
    "    essentials_pd.to_csv(output_path, index=False)\n",
    "    print(f'Saved: {output_path} ({len(essentials_pd):,} putative essential genes)')\n",
    "    print(f'Per organism:')\n",
    "    print(essentials_pd.groupby('orgId').size().describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "List all generated files and their sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "print('Generated data files:')\n",
    "print('=' * 60)\n",
    "for f in sorted(glob.glob(f'{DATA_DIR}/*.csv')):\n",
    "    size_mb = os.path.getsize(f) / 1e6\n",
    "    df = pd.read_csv(f, nrows=0)\n",
    "    print(f'  {os.path.basename(f):45s} {size_mb:8.1f} MB  cols={len(df.columns)}')\n",
    "\n",
    "print('\\nReady for NB02 (Tier 1 classification) and NB03 (Tier 2 conservation).')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}