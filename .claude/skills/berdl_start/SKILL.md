---
name: berdl_start
description: Get started with the BERIL Research Observatory. Use when a user is new, wants orientation, or asks what they can do.
allowed-tools: Read, Bash
user-invocable: true
---

# BERIL Research Observatory - Onboarding

Welcome the user and orient them to the system, then route them to the right context based on their goal.

## Phase 1: System Overview

Present this information directly (no file reads needed):

### What is BERDL?

The **KBase BER Data Lakehouse (BERDL)** is an on-prem Delta Lakehouse (Spark SQL) hosting **35 databases across 9 tenants**. Key collections:

| Collection | Scale | What it contains |
|------------|-------|------------------|
| `kbase_ke_pangenome` | 293K genomes, 1B genes, 27K species | Species-level pangenomes from GTDB r214: gene clusters, ANI, functional annotations (eggNOG), pathway predictions (GapMind), environmental embeddings (AlphaEarth) |
| `kbase_genomes` | 293K genomes, 253M proteins | Structural genomics (contigs, features, protein sequences) in CDM format |
| `kbase_msd_biochemistry` | 56K reactions, 46K molecules | ModelSEED biochemical reactions and compounds for metabolic modeling |
| `kescience_fitnessbrowser` | 48 organisms, 27M fitness scores | Genome-wide mutant fitness from RB-TnSeq experiments |
| `enigma_coral` | 3K taxa, 7K genomes | ENIGMA SFA environmental microbiology |
| `nmdc_arkin` | 48 studies, 3M+ metabolomics | NMDC multi-omics (annotations, embeddings, metabolomics, proteomics) |
| PhageFoundry (5 DBs) | Various | Species-specific genome browsers for phage-host research |
| `planetmicrobe_planetmicrobe` | 2K samples, 6K experiments | Marine microbial ecology |

### Repo Structure

```
projects/           # Science projects (each has README.md + notebooks/ + data/)
docs/               # Shared knowledge base
  collections.md    # Full database inventory
  schemas/          # Per-collection schema docs
  pitfalls.md       # SQL gotchas, data sparsity, common errors
  performance.md    # Query strategies for large tables
  research_ideas.md # Future research directions
  overview.md       # Scientific context and data generation workflow
  discoveries.md    # Running log of insights
.claude/skills/     # Agent skills
data/               # Shared data extracts reusable across projects
```

### Available Skills

| Skill | What it does |
|-------|-------------|
| `/berdl` | Query BERDL databases via REST API or Spark SQL |
| `/berdl-discover` | Explore and document a new BERDL database |
| `/literature-review` | Search PubMed, Europe PMC, and other sources for relevant biological literature |
| `/synthesize` | Read analysis outputs, compare against literature, and draft findings |
| `/submit` | Submit a project for automated review |
| `/cts` | Run batch compute jobs on the CTS cluster |

> **Note**: Hypothesis generation, research planning, and notebook creation are handled automatically as part of the research workflow (Path 1 below). You don't need to invoke them separately.

### Existing Projects

Discover projects dynamically — run `ls projects/` to list them. Read the first line of each `projects/*/README.md` to get titles. Present the list to the user so they can see what's been done.

### How Projects Work

Each project lives in `projects/<name>/` with a three-file structure plus supporting directories:

```
projects/<name>/
├── README.md            — Project overview, reproduction, authors
├── RESEARCH_PLAN.md     — Hypothesis, approach, query strategy, revision history
├── REPORT.md            — Findings, interpretation, supporting evidence
├── REVIEW.md            — Automated review (generated by /submit)
├── notebooks/           — Analysis notebooks with saved outputs
├── data/                — Extracted/processed data
├── figures/             — Key visualizations
└── requirements.txt     — Python dependencies
```

**Reproducibility is required**: notebooks must be committed with outputs, figures must be saved to `figures/`, and README must include a `## Reproduction` section. See PROJECT.md for full standards.

---

## Phase 2: Interactive Routing

Ask the user which of these they want to do:

1. **Start a new research project**
2. **Explore BERDL data**
3. **Review published literature**
4. **Continue an existing project**
5. **Understand the system**

Then follow the appropriate path below.

---

### Path 1: Start a New Research Project (Orchestrated Workflow)

When the user wants to start a new research project, the agent drives the entire process — from ideation through review — checking in with the user at natural decision points rather than requiring manual skill invocations.

#### Phase A: Orientation & Ideation

**Required reading before anything else:**
1. Read `PROJECT.md` — understand dual goals (science + knowledge capture), project structure requirements, reproducibility standards, JupyterHub workflow, Spark notebook patterns
2. Read `docs/overview.md` — understand the data architecture, key tables, data generation workflow, known limitations
3. Read `docs/collections.md` — full database inventory (35 databases, 9 tenants), what data is actually available
4. Read `docs/pitfalls.md` and `docs/performance.md` — **critical: read these before designing any queries or analysis**
5. Read `docs/research_ideas.md` — check for existing ideas, avoid duplicating work

**Environment check (on first run or when issues arise):**
6. Verify `.env` exists and contains `KBASE_AUTH_TOKEN` — if missing, ask the user for their KBase token
7. Check `gh auth status` — needed for creating branches, PRs, and pushing code. If not authenticated, prompt the user to run `gh auth login`
8. Note the user's ORCID and affiliation for the Authors section — ask once, remember for future projects

**Then engage with the user:**
9. Chat with the user about their research interest
10. Explore BERDL data (use `/berdl` queries) to check data availability, row counts, column types
11. Check existing projects (`ls projects/`) — read READMEs of related projects to understand what's been done
12. Develop 2-3 testable hypotheses with H0/H1
13. Search literature (use `/literature-review` internally) for context

#### Phase B: Research Plan

14. Write `RESEARCH_PLAN.md` with: Research Question, Hypothesis, Literature Context, Approach, Data Sources, Query Strategy, Analysis Plan, Expected Outcomes, Revision History (v1)
15. Write slim `README.md` with: Title, Research Question, Status (In Progress), Overview, Quick Links, Reproduction placeholder, Authors
16. Create project directory structure: `notebooks/`, `data/`, `figures/`
17. **Ask user**: "Here's the research plan. Should I create a new branch and check in before starting the analysis?"
18. If yes: create branch `projects/{project_id}`, commit README + RESEARCH_PLAN

#### Phase C: Analysis (Notebooks)

19. Write numbered notebooks (`01_data_exploration.ipynb`, `02_analysis.ipynb`, etc.) following the analysis plan
20. Notebooks are the primary audit trail — do as much work as possible in notebooks so humans can inspect intermediate results
21. When parallel execution or complex pipelines are needed, write scripts in `src/` but call them from notebooks
22. **Run notebooks** — execute cells, inspect outputs, iterate
23. As new information emerges, update `RESEARCH_PLAN.md` with a revision tag: `- **v2** ({date}): {what changed and why}`
24. **Check in code frequently** — commit after each major milestone (plan written, notebooks created, data extracted, analysis complete)
25. Re-read `docs/pitfalls.md` when something doesn't work as expected

#### Phase D: Synthesis & Writeup

26. Chat with the user about results — discuss interpretation, identify gaps, add analysis for clarity
27. Run `/synthesize` to create `REPORT.md` with findings, interpretation, supporting evidence
28. Commit the report
29. Chat with user about the report — revise if needed

#### Phase E: Review & Submission

30. Run `/submit` to validate documentation and generate `REVIEW.md`
31. Fix any issues flagged by the review
32. Commit fixes
33. Chat with user about next steps

#### Throughout the Entire Workflow:
- **Check in code often** — don't let work accumulate uncommitted
- **Update `docs/discoveries.md`** when you find something interesting (tag with `[project_id]`)
- **Update `docs/pitfalls.md`** when you hit a gotcha (follow pitfall-capture protocol from `.claude/skills/pitfall-capture/SKILL.md`)
- **Update `docs/performance.md`** when you learn a query optimization
- **Re-read `docs/pitfalls.md`** when debugging failures — the answer may already be documented
- **Re-read `docs/performance.md`** when queries are slow — check for existing optimization patterns
- **Follow `PROJECT.md` standards** — notebooks with saved outputs, figures as standalone PNGs, requirements.txt, Reproduction section in README

### Path 2: Explore BERDL Data

Read these files:
- `docs/collections.md` — full database inventory
- `docs/pitfalls.md` — critical gotchas before querying

Then:
- Summarize what databases are available and their scale
- Highlight cross-collection relationships (pangenome <-> genomes <-> biochemistry <-> fitness)
- Suggest using `/berdl` to start querying
- Suggest using `/berdl-discover` if they want to explore a database not yet documented in `docs/schemas/`
- Warn about the key pitfalls (see Critical Pitfalls below)

### Path 3: Review Published Literature

Suggest using `/literature-review` to search biological databases. This is useful for:
- Checking what's already known about an organism or pathway before querying BERDL
- Finding published pangenome analyses to compare against BERDL data
- Supporting a hypothesis with existing citations
- Discovering methods and approaches used in similar studies

**MCP setup check**: The `pubmed-search` MCP server is configured in `.claude/settings.json`. It runs via `uvx pubmed-search-mcp`. If it's not working:
1. Ensure `uv` is installed: `curl -LsSf https://astral.sh/uv/install.sh | sh`
2. Optionally add `NCBI_EMAIL` and `NCBI_API_KEY` to `.env` for faster PubMed access (3→10 requests/sec)
3. The skill falls back to WebSearch if the MCP server is unavailable

### Path 4: Continue an Existing Project

Steps:
1. Run `ls projects/` and list all projects for the user to choose from
2. Read the chosen project's `README.md`
3. Check if a `REVIEW.md` exists in that project directory (read it if so)
4. Summarize where the project stands: what's done, what's next
5. Suggest using `/submit` when the project is ready for review

### Path 5: Understand the System

Read these files:
- `PROJECT.md` — high-level goals and structure
- `docs/collections.md` — database inventory
- `docs/overview.md` — scientific context and data workflow

Then:
- Walk through the dual goals (science + knowledge capture)
- Explain the documentation workflow (tag discoveries, update pitfalls)
- Mention the UI can be browsed at the BERDL JupyterHub
- List the available skills and what each does
- Point to `docs/research_ideas.md` for future directions

---

## Key Principles (for the agent)

1. **Read the docs first** — `PROJECT.md`, `docs/overview.md`, `docs/collections.md`, `docs/pitfalls.md`, and `docs/performance.md` before designing anything. Check existing `projects/` to avoid duplicating work.
2. **Notebooks are the audit trail** — numbered sequentially (01, 02, 03...), each self-contained with a clear purpose. Commit with saved outputs per `PROJECT.md` reproducibility standards.
3. **Commit early and often** — after plan, after notebooks, after data extraction, after analysis, after synthesis.
4. **Ask before branching** — always ask the user before creating a new branch.
5. **Update the plan** — when the analysis reveals something that changes the approach, update RESEARCH_PLAN.md with a dated revision tag explaining what changed and why.
6. **Don't stop and wait** — drive the process forward, checking in with the user at decision points rather than stopping after each step.
7. **Document as you go** — discoveries go in `docs/discoveries.md`, pitfalls in `docs/pitfalls.md`, performance tips in `docs/performance.md` — captured in real-time, tagged with `[project_id]`.
8. **Use Spark patterns from PROJECT.md** — `get_spark_session()`, PySpark-first, `.toPandas()` only for final small results.

---

## Critical Pitfalls (always mention)

Regardless of path chosen, surface these early:

1. **Species IDs contain `--`** — This is fine inside quoted strings in SQL. Use exact equality (`WHERE id = 's__Escherichia_coli--RS_GCF_000005845.2'`), not LIKE patterns.
2. **Large tables need filters** — Never full-scan `gene` (1B rows) or `genome_ani` (420M rows). Always filter by species or genome ID.
3. **AlphaEarth embeddings cover only 28%** of genomes (83K/293K) — check coverage before relying on them.
4. **Notebooks must run on BERDL JupyterHub** — `get_spark_session()` is only available in JupyterHub kernels. Develop locally, upload and run on the hub.
5. **Auth token** — stored in `.env` as `KBASE_AUTH_TOKEN` (not `KB_AUTH_TOKEN`).
6. **String-typed numeric columns** — Many databases store numbers as strings. Always CAST before comparisons.
7. **Gene clusters are species-specific** — Cannot compare cluster IDs across species. Use COG/KEGG/PFAM for cross-species comparisons.
8. **Avoid unnecessary `.toPandas()`** — `.toPandas()` pulls all data to the driver node and can be very slow or cause OOM errors. Use PySpark DataFrame operations for filtering, joins, and aggregations. Only convert to pandas for final small results (plotting, CSV export).

---

## Templates

### RESEARCH_PLAN.md

```markdown
# Research Plan: {Title}

## Research Question
{Refined question after literature review}

## Hypothesis
- **H0**: {Null hypothesis}
- **H1**: {Alternative hypothesis}

## Literature Context
{Summary of what's known, key references, identified gaps}

## Query Strategy

### Tables Required
| Table | Purpose | Estimated Rows | Filter Strategy |
|---|---|---|---|
| {table} | {why needed} | {count} | {how to filter} |

### Key Queries
1. **{Description}**:
\```sql
{query}
\```

### Performance Plan
- **Tier**: {REST API / JupyterHub}
- **Estimated complexity**: {simple / moderate / complex}
- **Known pitfalls**: {list from pitfalls.md}

## Analysis Plan

### Notebook 1: Data Exploration
- **Goal**: {what to verify/explore}
- **Expected output**: {CSV/figures}

### Notebook 2: Main Analysis
- **Goal**: {core analysis}
- **Expected output**: {CSV/figures}

### Notebook 3: Visualization (if needed)
- **Goal**: {figures for findings}

## Expected Outcomes
- **If H1 supported**: {interpretation}
- **If H0 not rejected**: {interpretation}
- **Potential confounders**: {list}

## Revision History
- **v1** ({date}): Initial plan

## Authors
{ORCID, affiliation}
```

### README.md

```markdown
# {Title}

## Research Question
{Refined question}

## Status
In Progress — research plan created, awaiting analysis.

## Overview
{One-paragraph summary of the hypothesis and approach}

## Quick Links
- [Research Plan](RESEARCH_PLAN.md) — hypothesis, approach, query strategy
- [Report](REPORT.md) — findings, interpretation, supporting evidence

## Reproduction
*TBD — add prerequisites and step-by-step instructions after analysis is complete.*

## Authors
{Authors}
```
