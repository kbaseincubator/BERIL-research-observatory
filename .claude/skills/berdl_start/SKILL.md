---
name: berdl_start
description: Get started with the BERIL Research Observatory. Use when a user is new, wants orientation, or asks what they can do.
allowed-tools: Read, Bash
user-invocable: true
---

# BERIL Research Observatory - Onboarding

Welcome the user and orient them to the system, then route them to the right context based on their goal.

## Phase 1: System Overview

Present this information directly (no file reads needed):

### What is BERDL?

The **KBase BER Data Lakehouse (BERDL)** is an on-prem Delta Lakehouse (Spark SQL) hosting **35 databases across 9 tenants**. Key collections:

| Collection | Scale | What it contains |
|------------|-------|------------------|
| `kbase_ke_pangenome` | 293K genomes, 1B genes, 27K species | Species-level pangenomes from GTDB r214: gene clusters, ANI, functional annotations (eggNOG), pathway predictions (GapMind), environmental embeddings (AlphaEarth) |
| `kbase_genomes` | 293K genomes, 253M proteins | Structural genomics (contigs, features, protein sequences) in CDM format |
| `kbase_msd_biochemistry` | 56K reactions, 46K molecules | ModelSEED biochemical reactions and compounds for metabolic modeling |
| `kescience_fitnessbrowser` | 48 organisms, 27M fitness scores | Genome-wide mutant fitness from RB-TnSeq experiments |
| `enigma_coral` | 3K taxa, 7K genomes | ENIGMA SFA environmental microbiology |
| `nmdc_arkin` | 48 studies, 3M+ metabolomics | NMDC multi-omics (annotations, embeddings, metabolomics, proteomics) |
| PhageFoundry (5 DBs) | Various | Species-specific genome browsers for phage-host research |
| `planetmicrobe_planetmicrobe` | 2K samples, 6K experiments | Marine microbial ecology |

### Repo Structure

```
projects/           # Science projects (each has README.md + notebooks/ + data/)
docs/               # Shared knowledge base
  collections.md    # Full database inventory
  schemas/          # Per-collection schema docs
  pitfalls.md       # SQL gotchas, data sparsity, common errors
  performance.md    # Query strategies for large tables
  research_ideas.md # Future research directions
  overview.md       # Scientific context and data generation workflow
  discoveries.md    # Running log of insights
.claude/skills/     # Agent skills
data/               # Shared data extracts reusable across projects
```

### Available Skills

| Skill | What it does |
|-------|-------------|
| `/berdl` | Query BERDL databases via REST API or Spark SQL |
| `/berdl-discover` | Explore and document a new BERDL database |
| `/hypothesis` | Generate testable research questions from BERDL data |
| `/submit` | Submit a project for automated review |

### Existing Projects

Discover projects dynamically — run `ls projects/` to list them. Read the first line of each `projects/*/README.md` to get titles. Present the list to the user so they can see what's been done.

### How Projects Work

Each project lives in `projects/<name>/` with:
- `README.md` — Research question, hypothesis, approach, key findings
- `notebooks/` — Analysis notebooks (designed to run on BERDL JupyterHub with Spark)
- `data/` — Extracted/processed data (large files gitignored)
- `REVIEW.md` — Automated review (generated by `/submit`)

---

## Phase 2: Interactive Routing

Ask the user which of these they want to do:

1. **Start a new research project**
2. **Explore BERDL data**
3. **Continue an existing project**
4. **Understand the system**

Then follow the appropriate path below.

---

### Path 1: Start a New Research Project

Read these files:
- `docs/research_ideas.md` — existing ideas and their status
- `docs/collections.md` — what data is available
- `projects/cog_analysis/README.md` — example of a well-structured project

Then:
- Summarize the high-priority research ideas that are still PROPOSED
- Mention cross-project integration opportunities
- Suggest using `/hypothesis` to develop a research question
- Once the user has a question, help them create the project structure:
  ```
  projects/<name>/
    README.md      # Question, hypothesis, approach
    notebooks/     # Analysis notebooks
    data/          # Output data
  ```

### Path 2: Explore BERDL Data

Read these files:
- `docs/collections.md` — full database inventory
- `docs/pitfalls.md` — critical gotchas before querying

Then:
- Summarize what databases are available and their scale
- Highlight cross-collection relationships (pangenome <-> genomes <-> biochemistry <-> fitness)
- Suggest using `/berdl` to start querying
- Suggest using `/berdl-discover` if they want to explore a database not yet documented in `docs/schemas/`
- Warn about the key pitfalls (see Critical Pitfalls below)

### Path 3: Continue an Existing Project

Steps:
1. Run `ls projects/` and list all projects for the user to choose from
2. Read the chosen project's `README.md`
3. Check if a `REVIEW.md` exists in that project directory (read it if so)
4. Summarize where the project stands: what's done, what's next
5. Suggest using `/submit` when the project is ready for review

### Path 4: Understand the System

Read these files:
- `PROJECT.md` — high-level goals and structure
- `docs/collections.md` — database inventory
- `docs/overview.md` — scientific context and data workflow

Then:
- Walk through the dual goals (science + knowledge capture)
- Explain the documentation workflow (tag discoveries, update pitfalls)
- Mention the UI can be browsed at the BERDL JupyterHub
- List the available skills and what each does
- Point to `docs/research_ideas.md` for future directions

---

## Critical Pitfalls (always mention)

Regardless of path chosen, surface these early:

1. **Species IDs contain `--`** — This is fine inside quoted strings in SQL. Use exact equality (`WHERE id = 's__Escherichia_coli--RS_GCF_000005845.2'`), not LIKE patterns.
2. **Large tables need filters** — Never full-scan `gene` (1B rows) or `genome_ani` (420M rows). Always filter by species or genome ID.
3. **AlphaEarth embeddings cover only 28%** of genomes (83K/293K) — check coverage before relying on them.
4. **Notebooks must run on BERDL JupyterHub** — `get_spark_session()` is only available in JupyterHub kernels. Develop locally, upload and run on the hub.
5. **Auth token** — stored in `.env` as `KBASE_AUTH_TOKEN` (not `KB_AUTH_TOKEN`).
6. **String-typed numeric columns** — Many databases store numbers as strings. Always CAST before comparisons.
7. **Gene clusters are species-specific** — Cannot compare cluster IDs across species. Use COG/KEGG/PFAM for cross-species comparisons.
