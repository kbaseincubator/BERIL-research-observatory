{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0000001-0000-0000-0000-000000000001",
   "metadata": {},
   "source": [
    "# BERDL Lakehouse — Ingest: {TENANT}_{DATASET}\n",
    "\n",
    "Points at a directory containing source files and ingests them into the BERDL Lakehouse.\n",
    "\n",
    "**Supported source layouts:**\n",
    "- `<dir>/<name>.db` + `<dir>/<name>.sql` — SQLite database with schema file\n",
    "- `<dir>/*.tsv` + `<dir>/*.sql` — pre-exported TSV files with schema file\n",
    "- `<dir>/*.csv` + `<dir>/*.sql` — pre-exported CSV files with schema file\n",
    "\n",
    "Set `DATA_DIR`, `TENANT`, `DATASET`, and `MODE` in the **Configuration** cell, then run all cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0000002-0000-0000-0000-000000000002",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0000003-0000-0000-0000-000000000003",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# ── USER CONFIGURATION ───────────────────────────────────────────────────────\n",
    "DATA_DIR = Path(\"{DATA_DIR}\")   # directory containing source files\n",
    "TENANT   = \"{TENANT}\"           # Lakehouse tenant name\n",
    "DATASET  = \"{DATASET}\"          # dataset name (or None to use DATA_DIR.name)\n",
    "BUCKET   = \"cdm-lake\"\n",
    "MODE     = \"{MODE}\"             # \"overwrite\" or \"append\"\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "DATASET = DATASET or DATA_DIR.name\n",
    "BRONZE_PREFIX = f\"tenant-general-warehouse/{TENANT}/datasets/{DATASET}\"\n",
    "CONFIG_KEY    = f\"{BRONZE_PREFIX}/{DATASET}.json\"\n",
    "\n",
    "print(f\"Tenant  : {TENANT}\")\n",
    "print(f\"Dataset : {DATASET}\")\n",
    "print(f\"Mode    : {MODE}\")\n",
    "print(f\"Source  : {DATA_DIR.resolve()}\")\n",
    "print(f\"Bronze  : s3a://{BUCKET}/{BRONZE_PREFIX}/\")\n",
    "print(f\"Silver  : s3a://{BUCKET}/tenant-sql-warehouse/{TENANT}/{TENANT}_{DATASET}.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0000004-0000-0000-0000-000000000004",
   "metadata": {},
   "source": [
    "### Imports and `berdl_notebook_utils` stubs\n",
    "\n",
    "The installed `berdl_notebook_utils` package imports JupyterHub-only dependencies at module\n",
    "load time. We replace every submodule with a lightweight stub **before** importing\n",
    "`data_lakehouse_ingest`, then wire in real implementations once clients are built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0000005-0000-0000-0000-000000000005",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import io\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "import sqlite3\n",
    "import sys\n",
    "from types import ModuleType\n",
    "\n",
    "_STUB_MODULES = [\n",
    "    \"berdl_notebook_utils\",\n",
    "    \"berdl_notebook_utils.berdl_settings\",\n",
    "    \"berdl_notebook_utils.clients\",\n",
    "    \"berdl_notebook_utils.setup_spark_session\",\n",
    "    \"berdl_notebook_utils.spark\",\n",
    "    \"berdl_notebook_utils.spark.database\",\n",
    "    \"berdl_notebook_utils.spark.cluster\",\n",
    "    \"berdl_notebook_utils.spark.dataframe\",\n",
    "    \"berdl_notebook_utils.minio_governance\",\n",
    "]\n",
    "for _name in _STUB_MODULES:\n",
    "    sys.modules[_name] = ModuleType(_name)\n",
    "\n",
    "def _create_namespace_if_not_exists(spark, namespace=None, append_target=True, tenant_name=None):\n",
    "    ns = f\"{tenant_name}_{namespace}\" if tenant_name else namespace\n",
    "    location = f\"s3a://cdm-lake/tenant-sql-warehouse/{tenant_name}/{ns}.db\"\n",
    "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS `{ns}` LOCATION '{location}'\")\n",
    "    print(f\"Namespace {ns} ready at {location}\")\n",
    "    return ns\n",
    "\n",
    "sys.modules[\"berdl_notebook_utils.spark.database\"].create_namespace_if_not_exists = (\n",
    "    _create_namespace_if_not_exists\n",
    ")\n",
    "sys.modules[\"berdl_notebook_utils.setup_spark_session\"].get_spark_session = None\n",
    "sys.modules[\"berdl_notebook_utils.clients\"].get_minio_client = None\n",
    "\n",
    "from minio import Minio\n",
    "from data_lakehouse_ingest import ingest\n",
    "from get_spark_session import get_spark_session\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "print(\"Imports OK.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0000006-0000-0000-0000-000000000006",
   "metadata": {},
   "source": [
    "### Initialize Spark and MinIO clients\n",
    "\n",
    "MinIO credentials are read from `~/.mc/config.json`. Spark connects via pproxy on port 8123."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0000007-0000-0000-0000-000000000007",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3\n",
    "\n",
    "_mc_cfg = json.loads(Path.home().joinpath(\".mc/config.json\").read_text())\n",
    "_berdl  = _mc_cfg[\"aliases\"][\"berdl-minio\"]\n",
    "\n",
    "minio_client = Minio(\n",
    "    endpoint=_berdl[\"url\"].replace(\"https://\", \"\").replace(\"http://\", \"\"),\n",
    "    access_key=_berdl[\"accessKey\"],\n",
    "    secret_key=_berdl[\"secretKey\"],\n",
    "    secure=_berdl[\"url\"].startswith(\"https\"),\n",
    "    http_client=urllib3.ProxyManager(\"http://127.0.0.1:8123\"),\n",
    ")\n",
    "\n",
    "spark = get_spark_session()\n",
    "\n",
    "sys.modules[\"berdl_notebook_utils.setup_spark_session\"].get_spark_session = lambda **kw: spark\n",
    "sys.modules[\"berdl_notebook_utils.clients\"].get_minio_client = lambda **kw: minio_client\n",
    "\n",
    "print(\"Spark and MinIO clients ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0000008-0000-0000-0000-000000000008",
   "metadata": {},
   "source": [
    "### Detect source format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0000009-0000-0000-0000-000000000009",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DATA_DIR.exists():\n",
    "    raise FileNotFoundError(f\"DATA_DIR not found: {DATA_DIR}\")\n",
    "\n",
    "db_files  = sorted(DATA_DIR.glob(\"*.db\")) + sorted(DATA_DIR.glob(\"*.sqlite\")) + sorted(DATA_DIR.glob(\"*.sqlite3\"))\n",
    "sql_files = sorted(DATA_DIR.glob(\"*.sql\"))\n",
    "tsv_files = sorted(DATA_DIR.glob(\"*.tsv\"))\n",
    "csv_files = sorted(DATA_DIR.glob(\"*.csv\"))\n",
    "\n",
    "print(f\"SQLite databases : {[f.name for f in db_files]}\")\n",
    "print(f\"SQL schema files : {[f.name for f in sql_files]}\")\n",
    "print(f\"TSV files        : {[f.name for f in tsv_files]}\")\n",
    "print(f\"CSV files        : {[f.name for f in csv_files]}\")\n",
    "\n",
    "if db_files:\n",
    "    SOURCE_MODE = \"sqlite\"\n",
    "    SOURCE_DB   = db_files[0]\n",
    "    print(f\"\\nMode: SQLite → TSV  (source: {SOURCE_DB.name})\")\n",
    "elif tsv_files:\n",
    "    SOURCE_MODE = \"tsv\"\n",
    "    print(f\"\\nMode: TSV files ({len(tsv_files)} found)\")\n",
    "elif csv_files:\n",
    "    SOURCE_MODE = \"csv\"\n",
    "    print(f\"\\nMode: CSV files ({len(csv_files)} found)\")\n",
    "else:\n",
    "    raise ValueError(f\"No recognised source files found in {DATA_DIR}\")\n",
    "\n",
    "SQL_SCHEMA = sql_files[0] if sql_files else None\n",
    "print(f\"Schema file      : {SQL_SCHEMA.name if SQL_SCHEMA else 'none — all columns default to STRING'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0000010-0000-0000-0000-000000000010",
   "metadata": {},
   "source": [
    "### Parse schema from SQL file\n",
    "\n",
    "Extracts `CREATE TABLE` statements and maps SQL types to Spark SQL types.\n",
    "Any table or column without a match defaults to `STRING`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0000011-0000-0000-0000-000000000011",
   "metadata": {},
   "outputs": [],
   "source": [
    "_TYPE_MAP = {\n",
    "    \"TEXT\": \"STRING\",    \"VARCHAR\": \"STRING\",  \"CHAR\": \"STRING\",   \"CLOB\": \"STRING\",\n",
    "    \"INTEGER\": \"INT\",    \"INT\": \"INT\",          \"SMALLINT\": \"INT\",  \"TINYINT\": \"INT\",\n",
    "    \"MEDIUMINT\": \"INT\",  \"BIGINT\": \"BIGINT\",\n",
    "    \"REAL\": \"DOUBLE\",    \"FLOAT\": \"DOUBLE\",     \"DOUBLE\": \"DOUBLE\",\n",
    "    \"NUMERIC\": \"DOUBLE\", \"DECIMAL\": \"DOUBLE\",   \"NUMBER\": \"DOUBLE\",\n",
    "    \"BLOB\": \"BINARY\",    \"BOOLEAN\": \"BOOLEAN\",  \"BOOL\": \"BOOLEAN\",\n",
    "}\n",
    "\n",
    "def parse_sql_schema(sql_path):\n",
    "    \"\"\"Return {table_name: spark_schema_sql} parsed from CREATE TABLE statements.\"\"\"\n",
    "    text = sql_path.read_text(encoding=\"utf-8\", errors=\"replace\")\n",
    "    schemas = {}\n",
    "    pattern = re.compile(\n",
    "        r'CREATE\\s+TABLE\\s+(?:IF\\s+NOT\\s+EXISTS\\s+)?[`\"\\[]?(\\w+)[`\"\\]]?\\s*\\(([^;]+?)\\)\\s*;',\n",
    "        re.IGNORECASE | re.DOTALL,\n",
    "    )\n",
    "    for m in pattern.finditer(text):\n",
    "        table_name = m.group(1)\n",
    "        cols = []\n",
    "        for line in m.group(2).splitlines():\n",
    "            line = line.strip().rstrip(\",\")\n",
    "            if not line:\n",
    "                continue\n",
    "            if re.match(r'(PRIMARY\\s+KEY|UNIQUE|INDEX|FOREIGN\\s+KEY|CHECK|CONSTRAINT)\\b', line, re.I):\n",
    "                continue\n",
    "            tokens = re.split(r'\\s+', line, maxsplit=2)\n",
    "            if len(tokens) < 2:\n",
    "                continue\n",
    "            col_name  = re.sub(r'[`\"\\[\\]]', '', tokens[0])\n",
    "            raw_type  = re.sub(r'\\(.*', '', tokens[1]).upper()\n",
    "            spark_type = _TYPE_MAP.get(raw_type, \"STRING\")\n",
    "            cols.append(f\"{col_name} {spark_type}\")\n",
    "        if cols:\n",
    "            schemas[table_name] = \", \".join(cols)\n",
    "    return schemas\n",
    "\n",
    "\n",
    "if SQL_SCHEMA:\n",
    "    SCHEMAS = parse_sql_schema(SQL_SCHEMA)\n",
    "    print(f\"Parsed {len(SCHEMAS)} tables from {SQL_SCHEMA.name}:\")\n",
    "    for name, schema in SCHEMAS.items():\n",
    "        print(f\"  {name}: {schema[:100]}{'…' if len(schema) > 100 else ''}\")\n",
    "else:\n",
    "    SCHEMAS = {}\n",
    "    print(\"No .sql schema file — all columns will default to STRING\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0000012-0000-0000-0000-000000000012",
   "metadata": {},
   "source": [
    "### Prepare data files\n",
    "\n",
    "- **SQLite**: exports each table to TSV in `/tmp/<dataset>_tsv/`. Embedded tabs and newlines\n",
    "  in text fields are replaced with a space to prevent TSV corruption.\n",
    "- **TSV / CSV**: files are used directly from `DATA_DIR`; no conversion needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0000013-0000-0000-0000-000000000013",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SOURCE_MODE == \"sqlite\":\n",
    "    WORK_DIR  = Path(f\"/tmp/{DATASET}_tsv\")\n",
    "    FILE_EXT  = \".tsv\"\n",
    "    DELIMITER = \"\\t\"\n",
    "    WORK_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "    def _clean(v):\n",
    "        if v is None: return \"\"\n",
    "        if isinstance(v, str):\n",
    "            return v.replace(\"\\t\", \" \").replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "        return v\n",
    "\n",
    "    conn = sqlite3.connect(SOURCE_DB)\n",
    "    cur  = conn.cursor()\n",
    "    cur.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
    "    TABLES = [r[0] for r in cur.fetchall()]\n",
    "\n",
    "    for table in TABLES:\n",
    "        out = WORK_DIR / f\"{table}.tsv\"\n",
    "        cur.execute(f'SELECT * FROM \"{table}\"')\n",
    "        cols = [d[0] for d in cur.description]\n",
    "        if table not in SCHEMAS:\n",
    "            SCHEMAS[table] = \", \".join(f\"{c} STRING\" for c in cols)\n",
    "        with open(out, \"w\", newline=\"\", encoding=\"utf-8\") as fh:\n",
    "            w = csv.writer(fh, delimiter=\"\\t\", quoting=csv.QUOTE_MINIMAL)\n",
    "            w.writerow(cols)\n",
    "            for row in cur:\n",
    "                w.writerow([_clean(v) for v in row])\n",
    "        rows = cur.execute(f'SELECT count(*) FROM \"{table}\"').fetchone()[0]\n",
    "        print(f\"  {table:30s}: {rows:>9,} rows  {out.stat().st_size / 1e6:.1f} MB\")\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "elif SOURCE_MODE in (\"tsv\", \"csv\"):\n",
    "    source_files = tsv_files if SOURCE_MODE == \"tsv\" else csv_files\n",
    "    WORK_DIR  = DATA_DIR\n",
    "    FILE_EXT  = \".tsv\" if SOURCE_MODE == \"tsv\" else \".csv\"\n",
    "    DELIMITER = \"\\t\"  if SOURCE_MODE == \"tsv\" else \",\"\n",
    "    TABLES    = [f.stem for f in source_files]\n",
    "\n",
    "    for f in source_files:\n",
    "        if f.stem not in SCHEMAS:\n",
    "            with open(f, newline=\"\") as fh:\n",
    "                cols = next(csv.reader(fh, delimiter=DELIMITER))\n",
    "            SCHEMAS[f.stem] = \", \".join(f\"{c} STRING\" for c in cols)\n",
    "\n",
    "    print(f\"{len(TABLES)} {SOURCE_MODE.upper()} files ready:\")\n",
    "    for f in source_files:\n",
    "        print(f\"  {f.name:35s}: {f.stat().st_size / 1e6:.1f} MB\")\n",
    "\n",
    "print(f\"\\nTables ({len(TABLES)}): {TABLES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0000014-0000-0000-0000-000000000014",
   "metadata": {},
   "source": [
    "### Build ingestion config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0000015-0000-0000-0000-000000000015",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"tenant\":   TENANT,\n",
    "    \"dataset\":  DATASET,\n",
    "    \"is_tenant\": True,\n",
    "    \"paths\": {\n",
    "        \"data_plane\":  f\"s3a://{BUCKET}/tenant-general-warehouse/{TENANT}/\",\n",
    "        \"bronze_base\": f\"s3a://{BUCKET}/{BRONZE_PREFIX}/\",\n",
    "        \"silver_base\": f\"s3a://{BUCKET}/tenant-sql-warehouse/{TENANT}/{DATASET}.db\",\n",
    "    },\n",
    "    \"defaults\": {\n",
    "        \"csv\": {\"header\": True, \"delimiter\": DELIMITER, \"inferSchema\": False}\n",
    "    },\n",
    "    \"tables\": [\n",
    "        {\n",
    "            \"name\":        table,\n",
    "            \"enabled\":     True,\n",
    "            \"schema_sql\":  SCHEMAS.get(table, \"\"),\n",
    "            \"partition_by\": None,\n",
    "            \"mode\":        MODE,\n",
    "            \"bronze_path\": f\"s3a://{BUCKET}/{BRONZE_PREFIX}/{table}{FILE_EXT}\",\n",
    "        }\n",
    "        for table in TABLES\n",
    "    ],\n",
    "}\n",
    "\n",
    "print(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0000016-0000-0000-0000-000000000016",
   "metadata": {},
   "source": [
    "### Upload config and data files to MinIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0000017-0000-0000-0000-000000000017",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_bytes = json.dumps(config, indent=2).encode(\"utf-8\")\n",
    "minio_client.put_object(\n",
    "    BUCKET, CONFIG_KEY, io.BytesIO(config_bytes), len(config_bytes),\n",
    "    content_type=\"application/json\",\n",
    ")\n",
    "print(f\"Config → s3a://{BUCKET}/{CONFIG_KEY}\")\n",
    "\n",
    "for table in TABLES:\n",
    "    src = WORK_DIR / f\"{table}{FILE_EXT}\"\n",
    "    key = f\"{BRONZE_PREFIX}/{table}{FILE_EXT}\"\n",
    "    minio_client.fput_object(BUCKET, key, str(src))\n",
    "    print(f\"  {table:30s}: {src.stat().st_size / 1e6:.1f} MB → s3a://{BUCKET}/{key}\")\n",
    "\n",
    "print(\"\\nAll files uploaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0000018-0000-0000-0000-000000000018",
   "metadata": {},
   "source": [
    "### Run ingestion pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0000019-0000-0000-0000-000000000019",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_path = f\"s3a://{BUCKET}/{CONFIG_KEY}\"\n",
    "report = ingest(cfg_path, spark=spark, minio_client=minio_client)\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0000020-0000-0000-0000-000000000020",
   "metadata": {},
   "source": [
    "### Verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0000021-0000-0000-0000-000000000021",
   "metadata": {},
   "outputs": [],
   "source": [
    "namespace = f\"{TENANT}_{DATASET}\"\n",
    "spark.sql(f\"SHOW TABLES IN {namespace}\").show()\n",
    "\n",
    "print(\"Row counts:\")\n",
    "for table in TABLES:\n",
    "    count = spark.sql(f\"SELECT count(*) FROM {namespace}.{table}\").collect()[0][0]\n",
    "    print(f\"  {table:30s}: {count:>9,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0000022-0000-0000-0000-000000000022",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
