{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0000001-0000-0000-0000-000000000001",
   "metadata": {},
   "source": [
    "# BERDL Lakehouse — Ingest: {TENANT}_{DATASET}\n",
    "\n",
    "Two-phase ingest: **(1) upload source files to MinIO bronze**, then **(2) ingest into Delta silver**.\n",
    "Tables larger than `CHUNK_TARGET_GB` are ingested in chunks to avoid Spark session timeouts.\n",
    "A progress log is kept in MinIO so interrupted jobs can resume where they left off.\n",
    "\n",
    "**Workflow:**\n",
    "1. Set `DATA_DIR`, `TENANT`, `DATASET`, `MODE` in the **Configuration** cell.\n",
    "2. Run all cells through **Pre-flight plan** and review the printed plan.\n",
    "3. Set `CONFIRMED = True` in Configuration, then re-run from the Pre-flight cell onward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0000002-0000-0000-0000-000000000002",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "id": "b0000003-0000-0000-0000-000000000003",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# ── USER CONFIGURATION ───────────────────────────────────────────────────────\n",
    "DATA_DIR        = Path(\"{DATA_DIR}\")   # directory containing source files\n",
    "TENANT          = \"{TENANT}\"           # Lakehouse tenant name\n",
    "DATASET         = \"{DATASET}\"          # dataset name (or None to use DATA_DIR.name)\n",
    "BUCKET          = \"cdm-lake\"\n",
    "MODE            = \"{MODE}\"             # \"overwrite\" or \"append\"\n",
    "CHUNK_TARGET_GB = 20                   # tables above this size are ingested in chunks\n",
    "CHUNKED_INGEST  = True                 # False = force single-batch ingest (risky for large tables)\n",
    "CONFIRMED       = False                # set True after reviewing the pre-flight plan\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "DATASET       = DATASET or DATA_DIR.name\n",
    "NAMESPACE     = f\"{TENANT}_{DATASET}\"\n",
    "BRONZE_PREFIX = f\"tenant-general-warehouse/{TENANT}/datasets/{DATASET}\"\n",
    "CONFIG_KEY    = f\"{BRONZE_PREFIX}/{DATASET}.json\"\n",
    "SILVER_BASE   = f\"s3a://{BUCKET}/tenant-sql-warehouse/{TENANT}/{NAMESPACE}.db\"\n",
    "PROGRESS_KEY  = f\"{BRONZE_PREFIX}/_ingest_progress.jsonl\"\n",
    "\n",
    "print(f\"Tenant    : {TENANT}\")\n",
    "print(f\"Dataset   : {DATASET}\")\n",
    "print(f\"Namespace : {NAMESPACE}\")\n",
    "print(f\"Mode      : {MODE}\")\n",
    "print(f\"Chunked   : {CHUNKED_INGEST}  (threshold: {CHUNK_TARGET_GB} GB)\")\n",
    "print(f\"Source    : {DATA_DIR.resolve()}\")\n",
    "print(f\"Bronze    : s3a://{BUCKET}/{BRONZE_PREFIX}/\")\n",
    "print(f\"Silver    : {SILVER_BASE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0000004-0000-0000-0000-000000000004",
   "metadata": {},
   "source": [
    "### Imports and `berdl_notebook_utils` stubs\n",
    "\n",
    "Replaces JupyterHub-only submodules with lightweight stubs before importing\n",
    "`data_lakehouse_ingest`, then wires in real implementations once clients are built."
   ]
  },
  {
   "cell_type": "code",
   "id": "b0000005-0000-0000-0000-000000000005",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "import csv\n",
    "import io\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import re\n",
    "import sqlite3\n",
    "import sys\n",
    "from datetime import datetime, timezone\n",
    "from types import ModuleType\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "_STUB_MODULES = [\n",
    "    \"berdl_notebook_utils\",\n",
    "    \"berdl_notebook_utils.berdl_settings\",\n",
    "    \"berdl_notebook_utils.clients\",\n",
    "    \"berdl_notebook_utils.setup_spark_session\",\n",
    "    \"berdl_notebook_utils.spark\",\n",
    "    \"berdl_notebook_utils.spark.database\",\n",
    "    \"berdl_notebook_utils.spark.cluster\",\n",
    "    \"berdl_notebook_utils.spark.dataframe\",\n",
    "    \"berdl_notebook_utils.minio_governance\",\n",
    "]\n",
    "for _name in _STUB_MODULES:\n",
    "    sys.modules[_name] = ModuleType(_name)\n",
    "\n",
    "def _create_namespace_if_not_exists(spark, namespace=None, append_target=True, tenant_name=None):\n",
    "    ns = f\"{tenant_name}_{namespace}\" if tenant_name else namespace\n",
    "    location = f\"s3a://cdm-lake/tenant-sql-warehouse/{tenant_name}/{ns}.db\"\n",
    "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS `{ns}` LOCATION '{location}'\")\n",
    "    print(f\"Namespace {ns} ready at {location}\")\n",
    "    return ns\n",
    "\n",
    "sys.modules[\"berdl_notebook_utils.spark.database\"].create_namespace_if_not_exists = (\n",
    "    _create_namespace_if_not_exists\n",
    ")\n",
    "sys.modules[\"berdl_notebook_utils.setup_spark_session\"].get_spark_session = None\n",
    "sys.modules[\"berdl_notebook_utils.clients\"].get_minio_client = None\n",
    "\n",
    "from minio import Minio\n",
    "from data_lakehouse_ingest import ingest\n",
    "from get_spark_session import get_spark_session\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "print(\"Imports OK.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0000006-0000-0000-0000-000000000006",
   "metadata": {},
   "source": [
    "### Initialize Spark and MinIO clients\n\nMinIO credentials are read from `~/.mc/config.json`. Spark connects via pproxy on port 8123."
   ]
  },
  {
   "cell_type": "code",
   "id": "b0000007-0000-0000-0000-000000000007",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "import urllib3\n",
    "\n",
    "_mc_cfg = json.loads(Path.home().joinpath(\".mc/config.json\").read_text())\n",
    "_berdl  = _mc_cfg[\"aliases\"][\"berdl-minio\"]\n",
    "\n",
    "minio_client = Minio(\n",
    "    endpoint=_berdl[\"url\"].replace(\"https://\", \"\").replace(\"http://\", \"\"),\n",
    "    access_key=_berdl[\"accessKey\"],\n",
    "    secret_key=_berdl[\"secretKey\"],\n",
    "    secure=_berdl[\"url\"].startswith(\"https\"),\n",
    "    http_client=urllib3.ProxyManager(\"http://127.0.0.1:8123\"),\n",
    ")\n",
    "\n",
    "spark = get_spark_session()\n",
    "\n",
    "sys.modules[\"berdl_notebook_utils.setup_spark_session\"].get_spark_session = lambda **kw: spark\n",
    "sys.modules[\"berdl_notebook_utils.clients\"].get_minio_client = lambda **kw: minio_client\n",
    "\n",
    "print(\"Spark and MinIO clients ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0000008-0000-0000-0000-000000000008",
   "metadata": {},
   "source": [
    "### Detect source format"
   ]
  },
  {
   "cell_type": "code",
   "id": "b0000009-0000-0000-0000-000000000009",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "if not DATA_DIR.exists():\n",
    "    raise FileNotFoundError(f\"DATA_DIR not found: {DATA_DIR}\")\n",
    "\n",
    "db_files  = sorted(DATA_DIR.glob(\"*.db\")) + sorted(DATA_DIR.glob(\"*.sqlite\")) + sorted(DATA_DIR.glob(\"*.sqlite3\"))\n",
    "sql_files = sorted(DATA_DIR.glob(\"*.sql\"))\n",
    "tsv_files = sorted(DATA_DIR.glob(\"*.tsv\"))\n",
    "csv_files = sorted(DATA_DIR.glob(\"*.csv\"))\n",
    "\n",
    "print(f\"SQLite databases : {[f.name for f in db_files]}\")\n",
    "print(f\"SQL schema files : {[f.name for f in sql_files]}\")\n",
    "print(f\"TSV files        : {[f.name for f in tsv_files]}\")\n",
    "print(f\"CSV files        : {[f.name for f in csv_files]}\")\n",
    "\n",
    "if db_files:\n",
    "    SOURCE_MODE = \"sqlite\"\n",
    "    SOURCE_DB   = db_files[0]\n",
    "    print(f\"\\nMode: SQLite -> TSV  (source: {SOURCE_DB.name})\")\n",
    "elif tsv_files:\n",
    "    SOURCE_MODE = \"tsv\"\n",
    "    print(f\"\\nMode: TSV files ({len(tsv_files)} found)\")\n",
    "elif csv_files:\n",
    "    SOURCE_MODE = \"csv\"\n",
    "    print(f\"\\nMode: CSV files ({len(csv_files)} found)\")\n",
    "else:\n",
    "    raise ValueError(f\"No recognised source files found in {DATA_DIR}\")\n",
    "\n",
    "SQL_SCHEMA = sql_files[0] if sql_files else None\n",
    "print(f\"Schema file      : {SQL_SCHEMA.name if SQL_SCHEMA else 'none -- all columns default to STRING'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0000010-0000-0000-0000-000000000010",
   "metadata": {},
   "source": [
    "### Parse schema from SQL file\n\nExtracts `CREATE TABLE` statements and maps SQL types to Spark SQL types.\nAny table or column without a match defaults to `STRING`."
   ]
  },
  {
   "cell_type": "code",
   "id": "b0000011-0000-0000-0000-000000000011",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "_TYPE_MAP = {\n",
    "    \"TEXT\": \"STRING\",    \"VARCHAR\": \"STRING\",  \"CHAR\": \"STRING\",   \"CLOB\": \"STRING\",\n",
    "    \"INTEGER\": \"INT\",    \"INT\": \"INT\",          \"SMALLINT\": \"INT\",  \"TINYINT\": \"INT\",\n",
    "    \"MEDIUMINT\": \"INT\",  \"BIGINT\": \"BIGINT\",\n",
    "    \"REAL\": \"DOUBLE\",    \"FLOAT\": \"DOUBLE\",     \"DOUBLE\": \"DOUBLE\",\n",
    "    \"NUMERIC\": \"DOUBLE\", \"DECIMAL\": \"DOUBLE\",   \"NUMBER\": \"DOUBLE\",\n",
    "    \"BLOB\": \"BINARY\",    \"BOOLEAN\": \"BOOLEAN\",  \"BOOL\": \"BOOLEAN\",\n",
    "}\n",
    "\n",
    "def parse_sql_schema(sql_path):\n",
    "    \"\"\"Return {table_name: spark_schema_ddl} parsed from CREATE TABLE statements.\"\"\"\n",
    "    text = sql_path.read_text(encoding=\"utf-8\", errors=\"replace\")\n",
    "    schemas = {}\n",
    "    pattern = re.compile(\n",
    "        r'CREATE\\s+TABLE\\s+(?:IF\\s+NOT\\s+EXISTS\\s+)?[`\"\\[]?(\\w+)[`\"\\]]?\\s*\\(([^;]+?)\\)\\s*;',\n",
    "        re.IGNORECASE | re.DOTALL,\n",
    "    )\n",
    "    for m in pattern.finditer(text):\n",
    "        table_name = m.group(1)\n",
    "        cols = []\n",
    "        for line in m.group(2).splitlines():\n",
    "            line = re.sub(r'/\\*.*?\\*/', '', line).strip()\n",
    "            line = re.sub(r'--.*$', '', line).strip()\n",
    "            line = line.rstrip(\",\")\n",
    "            if not line:\n",
    "                continue\n",
    "            if re.match(r'(PRIMARY\\s+KEY|UNIQUE|INDEX|FOREIGN\\s+KEY|CHECK|CONSTRAINT)\\b', line, re.I):\n",
    "                continue\n",
    "            tokens = re.split(r'\\s+', line, maxsplit=2)\n",
    "            if len(tokens) < 2:\n",
    "                continue\n",
    "            col_name   = re.sub(r'[`\"\\[\\]]', '', tokens[0])\n",
    "            raw_type   = re.sub(r'\\(.*', '', tokens[1]).upper()\n",
    "            spark_type = _TYPE_MAP.get(raw_type, \"STRING\")\n",
    "            cols.append(f\"{col_name} {spark_type}\")\n",
    "        if cols:\n",
    "            schemas[table_name] = \", \".join(cols)\n",
    "    return schemas\n",
    "\n",
    "if SQL_SCHEMA:\n",
    "    SCHEMAS = parse_sql_schema(SQL_SCHEMA)\n",
    "    print(f\"Parsed {len(SCHEMAS)} tables from {SQL_SCHEMA.name}:\")\n",
    "    for name, schema in SCHEMAS.items():\n",
    "        print(f\"  {name}: {schema[:100]}{'...' if len(schema) > 100 else ''}\")\n",
    "else:\n",
    "    SCHEMAS = {}\n",
    "    print(\"No .sql schema file -- all columns will default to STRING\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0000012-0000-0000-0000-000000000012",
   "metadata": {},
   "source": [
    "### Prepare data files and calculate chunk plan\n",
    "\n",
    "Counts lines in each source file to determine per-table chunk sizes.\n",
    "Chunk size is calculated proportionally: `chunk_lines = data_lines * (CHUNK_TARGET_GB / file_size_GB)`.\n",
    "Tables at or below `CHUNK_TARGET_GB` are ingested in a single shot."
   ]
  },
  {
   "cell_type": "code",
   "id": "b0000013-0000-0000-0000-000000000013",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "def _count_lines(filepath):\n",
    "    \"\"\"Count newlines in a file efficiently (result includes the header line).\"\"\"\n",
    "    n = 0\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        for block in iter(lambda: f.read(1 << 20), b\"\"):\n",
    "            n += block.count(b\"\\n\")\n",
    "    return n\n",
    "\n",
    "if SOURCE_MODE == \"sqlite\":\n",
    "    WORK_DIR  = Path(f\"/tmp/{DATASET}_tsv\")\n",
    "    FILE_EXT  = \".tsv\"\n",
    "    DELIMITER = \"\\t\"\n",
    "    WORK_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "    def _clean(v):\n",
    "        if v is None: return \"\"\n",
    "        if isinstance(v, str):\n",
    "            return v.replace(\"\\t\", \" \").replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "        return v\n",
    "\n",
    "    conn = sqlite3.connect(SOURCE_DB)\n",
    "    conn.text_factory = lambda b: b.decode(\"utf-8\", errors=\"replace\")\n",
    "    cur  = conn.cursor()\n",
    "    cur.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
    "    TABLES = [r[0] for r in cur.fetchall()]\n",
    "    for table in TABLES:\n",
    "        out = WORK_DIR / f\"{table}.tsv\"\n",
    "        cur.execute(f'SELECT * FROM \"{table}\"')\n",
    "        cols = [d[0] for d in cur.description]\n",
    "        if table not in SCHEMAS:\n",
    "            SCHEMAS[table] = \", \".join(f\"{c} STRING\" for c in cols)\n",
    "        with open(out, \"w\", newline=\"\", encoding=\"utf-8\") as fh:\n",
    "            w = csv.writer(fh, delimiter=\"\\t\", quoting=csv.QUOTE_MINIMAL)\n",
    "            w.writerow(cols)\n",
    "            for row in cur:\n",
    "                w.writerow([_clean(v) for v in row])\n",
    "        rows = cur.execute(f'SELECT count(*) FROM \"{table}\"').fetchone()[0]\n",
    "        print(f\"  {table:30s}: {rows:>9,} rows  {out.stat().st_size / 1e6:.1f} MB\")\n",
    "    conn.close()\n",
    "    source_files = sorted(WORK_DIR.glob(\"*.tsv\"))\n",
    "\n",
    "elif SOURCE_MODE in (\"tsv\", \"csv\"):\n",
    "    source_files = tsv_files if SOURCE_MODE == \"tsv\" else csv_files\n",
    "    WORK_DIR  = DATA_DIR\n",
    "    FILE_EXT  = \".tsv\" if SOURCE_MODE == \"tsv\" else \".csv\"\n",
    "    DELIMITER = \"\\t\"  if SOURCE_MODE == \"tsv\" else \",\"\n",
    "    TABLES    = [f.stem for f in source_files]\n",
    "    for f in source_files:\n",
    "        if f.stem not in SCHEMAS:\n",
    "            with open(f, newline=\"\") as fh:\n",
    "                cols = next(csv.reader(fh, delimiter=DELIMITER))\n",
    "            SCHEMAS[f.stem] = \", \".join(f\"{c} STRING\" for c in cols)\n",
    "\n",
    "# ── Calculate chunk plan ─────────────────────────────────────────────────────\n",
    "CHUNK_TARGET_BYTES = CHUNK_TARGET_GB * 1e9\n",
    "TABLE_STATS = {}\n",
    "\n",
    "print(f\"Analyzing {len(source_files)} table(s) -- counting lines...\")\n",
    "for f in source_files:\n",
    "    table      = f.stem\n",
    "    size_bytes = f.stat().st_size\n",
    "    print(f\"  {f.name}: {size_bytes / 1e9:.1f} GB\", end=\" \", flush=True)\n",
    "    total_lines = _count_lines(f)\n",
    "    data_lines  = max(total_lines - 1, 0)          # exclude header row\n",
    "\n",
    "    if CHUNKED_INGEST and size_bytes > CHUNK_TARGET_BYTES and data_lines > 0:\n",
    "        chunk_size = max(1, round(data_lines * CHUNK_TARGET_BYTES / size_bytes))\n",
    "        n_chunks   = math.ceil(data_lines / chunk_size)\n",
    "    else:\n",
    "        chunk_size = data_lines\n",
    "        n_chunks   = 1\n",
    "\n",
    "    TABLE_STATS[table] = {\n",
    "        \"path\":       f,\n",
    "        \"size_bytes\": size_bytes,\n",
    "        \"data_lines\": data_lines,\n",
    "        \"chunk_size\": chunk_size,\n",
    "        \"n_chunks\":   n_chunks,\n",
    "        \"chunked\":    n_chunks > 1,\n",
    "    }\n",
    "    note = (f\"{n_chunks} chunks x ~{chunk_size:,} lines\"\n",
    "            if n_chunks > 1 else \"single ingest\")\n",
    "    print(f\"-> {data_lines:,} data lines  [{note}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0000014-0000-0000-0000-000000000014",
   "metadata": {},
   "source": [
    "### Pre-flight plan\n",
    "\n",
    "Review the upload and ingest plan below.\n",
    "When satisfied, set `CONFIRMED = True` in the Configuration cell and re-run from here."
   ]
  },
  {
   "cell_type": "code",
   "id": "b0000015-0000-0000-0000-000000000015",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "W = 72\n",
    "print(\"=\" * W)\n",
    "print(\"PRE-FLIGHT PLAN\")\n",
    "print(\"=\" * W)\n",
    "\n",
    "total_gb = sum(s[\"size_bytes\"] for s in TABLE_STATS.values()) / 1e9\n",
    "print(\"\\nSTEP 1 -- MinIO Upload  (all tables uploaded in full before any ingest begins)\")\n",
    "for table, s in TABLE_STATS.items():\n",
    "    print(f\"  {table:<45s}  {s['size_bytes'] / 1e9:>7.1f} GB\")\n",
    "print(f\"  {'TOTAL':<45s}  {total_gb:>7.1f} GB\")\n",
    "\n",
    "print(f\"\\nSTEP 2 -- Spark Ingest into Delta  (namespace: {NAMESPACE})\")\n",
    "for table, s in TABLE_STATS.items():\n",
    "    if s[\"chunked\"]:\n",
    "        chunk_gb = s[\"size_bytes\"] / s[\"n_chunks\"] / 1e9\n",
    "        print(f\"  {table:<45s}  {s['n_chunks']} chunks x ~{s['chunk_size']:,} lines\"\n",
    "              f\"  (~{chunk_gb:.1f} GB each)  [CHUNKED]\")\n",
    "    else:\n",
    "        print(f\"  {table:<45s}  {s['data_lines']:,} lines  [single ingest]\")\n",
    "\n",
    "print(f\"\\nProgress log : s3a://{BUCKET}/{PROGRESS_KEY}\")\n",
    "print(f\"Ingest mode  : {MODE}\")\n",
    "print(\"=\" * W)\n",
    "\n",
    "if not CONFIRMED:\n",
    "    raise RuntimeError(\n",
    "        \"\\nReview the plan above.\\n\"\n",
    "        \"If it looks correct, set CONFIRMED = True in the Configuration cell \"\n",
    "        \"and re-run from this cell onward.\"\n",
    "    )\n",
    "print(\"CONFIRMED -- proceeding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0000016-0000-0000-0000-000000000016",
   "metadata": {},
   "source": [
    "### Step 1 -- Upload all files to MinIO bronze"
   ]
  },
  {
   "cell_type": "code",
   "id": "b0000017-0000-0000-0000-000000000017",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": "def _minio_object_size(key):\n    \"\"\"Return the size in bytes of a MinIO object, or -1 if it does not exist.\"\"\"\n    try:\n        return minio_client.stat_object(BUCKET, key).size\n    except Exception as e:\n        if any(tag in str(e) for tag in (\"NoSuchKey\", \"does not exist\", \"404\")):\n            return -1\n        raise\n\n# Upload master dataset config (always overwrite — it's tiny)\nconfig = {\n    \"tenant\": TENANT, \"dataset\": DATASET, \"is_tenant\": True,\n    \"paths\": {\n        \"data_plane\":  f\"s3a://{BUCKET}/tenant-general-warehouse/{TENANT}/\",\n        \"bronze_base\": f\"s3a://{BUCKET}/{BRONZE_PREFIX}/\",\n        \"silver_base\": SILVER_BASE,\n    },\n    \"defaults\": {\"csv\": {\"header\": True, \"delimiter\": DELIMITER, \"inferSchema\": False}},\n    \"tables\": [\n        {\n            \"name\": table, \"enabled\": True,\n            \"schema_sql\": SCHEMAS.get(table, \"\"),\n            \"partition_by\": None, \"mode\": MODE,\n            \"bronze_path\": f\"s3a://{BUCKET}/{BRONZE_PREFIX}/{table}{FILE_EXT}\",\n        }\n        for table in TABLES\n    ],\n}\nconfig_bytes = json.dumps(config, indent=2).encode(\"utf-8\")\nminio_client.put_object(BUCKET, CONFIG_KEY, io.BytesIO(config_bytes), len(config_bytes),\n                        content_type=\"application/json\")\nprint(f\"Config -> s3a://{BUCKET}/{CONFIG_KEY}\")\n\n# Upload data files — skip any file whose size already matches in MinIO\nprint(\"\\nChecking / uploading data files...\")\nfor table, s in TABLE_STATS.items():\n    key          = f\"{BRONZE_PREFIX}/{table}{FILE_EXT}\"\n    local_size   = s[\"size_bytes\"]\n    remote_size  = _minio_object_size(key)\n\n    if remote_size == local_size:\n        print(f\"  {table}: {local_size / 1e9:.1f} GB -- already in MinIO, skipping upload\")\n        continue\n\n    if remote_size != -1:\n        print(f\"  {table}: size mismatch (local {local_size:,} B vs MinIO {remote_size:,} B) -- re-uploading\")\n    else:\n        print(f\"  {table}: {local_size / 1e9:.1f} GB -- uploading...\", end=\" \", flush=True)\n\n    minio_client.fput_object(BUCKET, key, str(s[\"path\"]))\n    print(f\"done  -> s3a://{BUCKET}/{key}\")\n\nprint(\"\\nUpload step complete.\")"
  },
  {
   "cell_type": "markdown",
   "id": "b0000018-0000-0000-0000-000000000018",
   "metadata": {},
   "source": [
    "### Step 2 -- Initialize progress log\n\nLoads any existing progress from MinIO so an interrupted ingest can resume."
   ]
  },
  {
   "cell_type": "code",
   "id": "b0000019-0000-0000-0000-000000000019",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "def _load_progress_log():\n",
    "    \"\"\"Load existing JSONL progress log from MinIO. Returns list of entries.\"\"\"\n",
    "    try:\n",
    "        resp = minio_client.get_object(BUCKET, PROGRESS_KEY)\n",
    "        entries = []\n",
    "        for line in resp.read().decode().splitlines():\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                entries.append(json.loads(line))\n",
    "        return entries\n",
    "    except Exception as e:\n",
    "        if any(tag in str(e) for tag in (\"NoSuchKey\", \"does not exist\", \"404\")):\n",
    "            return []\n",
    "        raise\n",
    "\n",
    "def _append_progress(entry):\n",
    "    \"\"\"Append one entry to the MinIO progress log (read-modify-write for durability).\"\"\"\n",
    "    entries = _load_progress_log()\n",
    "    entries.append(entry)\n",
    "    data = (\"\\n\".join(json.dumps(e) for e in entries) + \"\\n\").encode()\n",
    "    minio_client.put_object(BUCKET, PROGRESS_KEY, io.BytesIO(data), len(data),\n",
    "                            content_type=\"application/x-ndjson\")\n",
    "\n",
    "PROGRESS_LOG = _load_progress_log()\n",
    "\n",
    "complete_tables = {e[\"table\"] for e in PROGRESS_LOG if e.get(\"status\") == \"complete\"}\n",
    "partial_chunks  = {}\n",
    "for e in PROGRESS_LOG:\n",
    "    if e.get(\"status\") == \"ingested\" and e[\"table\"] not in complete_tables:\n",
    "        partial_chunks.setdefault(e[\"table\"], []).append(e[\"chunk\"])\n",
    "\n",
    "if not PROGRESS_LOG:\n",
    "    print(\"No prior progress -- will ingest all tables from scratch.\")\n",
    "else:\n",
    "    print(f\"Loaded {len(PROGRESS_LOG)} progress log entries.\")\n",
    "    if complete_tables:\n",
    "        print(f\"  Already complete  : {', '.join(sorted(complete_tables))}\")\n",
    "    if partial_chunks:\n",
    "        print(\"  Partially ingested (will resume):\")\n",
    "        for t, chunks in sorted(partial_chunks.items()):\n",
    "            done_rows = sum(e[\"rows_written\"] for e in PROGRESS_LOG\n",
    "                           if e[\"table\"] == t and e.get(\"status\") == \"ingested\")\n",
    "            print(f\"    {t}: {len(chunks)} chunk(s) done, {done_rows:,} rows ingested so far\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0000020-0000-0000-0000-000000000020",
   "metadata": {},
   "source": [
    "### Step 2 -- Ingest tables into Delta namespace\n",
    "\n",
    "- Tables **<= CHUNK_TARGET_GB**: ingested in one shot via the `ingest()` pipeline.\n",
    "- Tables **> CHUNK_TARGET_GB**: local file streamed in chunks via `pandas.read_csv(chunksize=N)`;\n",
    "  each chunk is written to Delta with `append` mode via `spark.createDataFrame()`.\n",
    "- Progress logged to MinIO after every chunk. Restart the cell to resume an interrupted ingest."
   ]
  },
  {
   "cell_type": "code",
   "id": "b0000021-0000-0000-0000-000000000021",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pyspark.sql import types as T\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "_PY_TYPE = {\n",
    "    \"STRING\":  T.StringType(),  \"INT\":     T.IntegerType(),\n",
    "    \"BIGINT\":  T.LongType(),    \"DOUBLE\":  T.DoubleType(),\n",
    "    \"BOOLEAN\": T.BooleanType(), \"BINARY\":  T.BinaryType(),\n",
    "}\n",
    "\n",
    "def _spark_schema(schema_ddl):\n",
    "    \"\"\"Parse 'col TYPE, col TYPE, ...' into a PySpark StructType.\"\"\"\n",
    "    fields = []\n",
    "    for part in schema_ddl.split(\",\"):\n",
    "        tokens = part.strip().split()\n",
    "        if len(tokens) >= 2:\n",
    "            fields.append(T.StructField(tokens[0], _PY_TYPE.get(tokens[1], T.StringType()), True))\n",
    "    return T.StructType(fields)\n",
    "\n",
    "\n",
    "def _ingest_chunked(table, stats, schema_ddl):\n",
    "    \"\"\"Stream a large local file into Delta in line-count chunks.\"\"\"\n",
    "    delta_path   = f\"{SILVER_BASE}/{table}\"\n",
    "    target_schema = _spark_schema(schema_ddl) if schema_ddl else None\n",
    "    done_chunks  = {e[\"chunk\"] for e in PROGRESS_LOG\n",
    "                    if e[\"table\"] == table and e.get(\"status\") == \"ingested\"}\n",
    "    rows_done    = sum(e[\"rows_written\"] for e in PROGRESS_LOG\n",
    "                       if e[\"table\"] == table and e.get(\"status\") == \"ingested\")\n",
    "    table_registered = bool(done_chunks)\n",
    "\n",
    "    reader = pd.read_csv(\n",
    "        stats[\"path\"], sep=DELIMITER, chunksize=stats[\"chunk_size\"],\n",
    "        dtype=str, keep_default_na=False, na_values=[],\n",
    "    )\n",
    "    for chunk_num, chunk_df in enumerate(reader):\n",
    "        if chunk_num in done_chunks:\n",
    "            print(f\"  Chunk {chunk_num + 1}/{stats['n_chunks']}: already ingested -- skipping\")\n",
    "            continue\n",
    "\n",
    "        start_line = chunk_num * stats[\"chunk_size\"] + 1\n",
    "        end_line   = start_line + len(chunk_df) - 1\n",
    "\n",
    "        # Build Spark DataFrame: read all as string, then cast to target types\n",
    "        if target_schema:\n",
    "            str_schema = T.StructType([\n",
    "                T.StructField(f.name, T.StringType(), True) for f in target_schema.fields\n",
    "            ])\n",
    "            sdf = spark.createDataFrame(chunk_df, schema=str_schema)\n",
    "            for field in target_schema.fields:\n",
    "                if not isinstance(field.dataType, T.StringType):\n",
    "                    sdf = sdf.withColumn(field.name, F.col(field.name).cast(field.dataType))\n",
    "        else:\n",
    "            sdf = spark.createDataFrame(chunk_df)\n",
    "\n",
    "        # First chunk of a fresh ingest uses MODE; all subsequent chunks always append\n",
    "        write_mode = MODE if (chunk_num == 0 and not done_chunks) else \"append\"\n",
    "        sdf.write.format(\"delta\").mode(write_mode).save(delta_path)\n",
    "\n",
    "        # Register table in catalog on first successful write\n",
    "        if not table_registered:\n",
    "            spark.sql(f\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS `{NAMESPACE}`.`{table}`\n",
    "                USING DELTA LOCATION '{delta_path}'\n",
    "            \"\"\")\n",
    "            table_registered = True\n",
    "\n",
    "        rows_done += len(chunk_df)\n",
    "        _append_progress({\n",
    "            \"table\": table, \"chunk\": chunk_num,\n",
    "            \"start_line\": start_line, \"end_line\": end_line,\n",
    "            \"rows_written\": len(chunk_df), \"rows_cumulative\": rows_done,\n",
    "            \"status\": \"ingested\",\n",
    "            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n",
    "        })\n",
    "        print(f\"  Chunk {chunk_num + 1}/{stats['n_chunks']}: \"\n",
    "              f\"lines {start_line:,}-{end_line:,}  \"\n",
    "              f\"({len(chunk_df):,} rows written, {rows_done:,} cumulative)\")\n",
    "\n",
    "    return rows_done\n",
    "\n",
    "\n",
    "# ── Ensure namespace exists ───────────────────────────────────────────────────\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS `{NAMESPACE}` LOCATION '{SILVER_BASE}'\")\n",
    "print(f\"Namespace {NAMESPACE} ready.\\n\")\n",
    "\n",
    "# ── Ingest loop ───────────────────────────────────────────────────────────────\n",
    "for table, stats in TABLE_STATS.items():\n",
    "    if any(e[\"table\"] == table and e.get(\"status\") == \"complete\" for e in PROGRESS_LOG):\n",
    "        print(f\"{table}: already complete -- skipping\\n\")\n",
    "        continue\n",
    "\n",
    "    print(f\"{'=' * 60}\")\n",
    "    print(f\"Ingesting: {table}\")\n",
    "    print(f\"  {stats['data_lines']:,} rows | {stats['n_chunks']} chunk(s) | {stats['size_bytes'] / 1e9:.1f} GB\")\n",
    "\n",
    "    if not stats[\"chunked\"]:\n",
    "        # Small table: single ingest via pipeline\n",
    "        tbl_cfg = {\n",
    "            \"tenant\": TENANT, \"dataset\": DATASET, \"is_tenant\": True,\n",
    "            \"paths\": {\n",
    "                \"data_plane\":  f\"s3a://{BUCKET}/tenant-general-warehouse/{TENANT}/\",\n",
    "                \"bronze_base\": f\"s3a://{BUCKET}/{BRONZE_PREFIX}/\",\n",
    "                \"silver_base\": SILVER_BASE,\n",
    "            },\n",
    "            \"defaults\": {\"csv\": {\"header\": True, \"delimiter\": DELIMITER, \"inferSchema\": False}},\n",
    "            \"tables\": [{\n",
    "                \"name\": table, \"enabled\": True,\n",
    "                \"schema_sql\": SCHEMAS.get(table, \"\"),\n",
    "                \"partition_by\": None, \"mode\": MODE,\n",
    "                \"bronze_path\": f\"s3a://{BUCKET}/{BRONZE_PREFIX}/{table}{FILE_EXT}\",\n",
    "            }],\n",
    "        }\n",
    "        tbl_cfg_key   = f\"{BRONZE_PREFIX}/{table}_config.json\"\n",
    "        tbl_cfg_bytes = json.dumps(tbl_cfg, indent=2).encode()\n",
    "        minio_client.put_object(BUCKET, tbl_cfg_key, io.BytesIO(tbl_cfg_bytes),\n",
    "                                len(tbl_cfg_bytes), content_type=\"application/json\")\n",
    "        ingest(f\"s3a://{BUCKET}/{tbl_cfg_key}\", spark=spark, minio_client=minio_client)\n",
    "        rows_done = stats[\"data_lines\"]\n",
    "        _append_progress({\n",
    "            \"table\": table, \"chunk\": 0,\n",
    "            \"start_line\": 1, \"end_line\": rows_done,\n",
    "            \"rows_written\": rows_done, \"rows_cumulative\": rows_done,\n",
    "            \"status\": \"ingested\",\n",
    "            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n",
    "        })\n",
    "    else:\n",
    "        # Large table: chunked streaming ingest\n",
    "        rows_done = _ingest_chunked(table, stats, SCHEMAS.get(table, \"\"))\n",
    "\n",
    "    _append_progress({\n",
    "        \"table\": table, \"status\": \"complete\",\n",
    "        \"total_rows\": rows_done, \"total_chunks\": stats[\"n_chunks\"],\n",
    "        \"timestamp\": datetime.now(timezone.utc).isoformat(),\n",
    "    })\n",
    "    print(f\"  {table}: {rows_done:,} rows -- COMPLETE\\n\")\n",
    "\n",
    "print(\"All tables ingested.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0000022-0000-0000-0000-000000000022",
   "metadata": {},
   "source": [
    "### Verify row counts"
   ]
  },
  {
   "cell_type": "code",
   "id": "b0000023-0000-0000-0000-000000000023",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "spark.sql(f\"SHOW TABLES IN `{NAMESPACE}`\").show()\n",
    "\n",
    "PROGRESS_LOG = _load_progress_log()\n",
    "all_match = True\n",
    "print(\"Row counts (Delta vs expected):\")\n",
    "for table, stats in TABLE_STATS.items():\n",
    "    count    = spark.sql(f\"SELECT COUNT(*) FROM `{NAMESPACE}`.`{table}`\").collect()[0][0]\n",
    "    expected = stats[\"data_lines\"]\n",
    "    match    = \"OK\" if count == expected else \"MISMATCH\"\n",
    "    if count != expected:\n",
    "        all_match = False\n",
    "    print(f\"  {table:<45s}  {count:>12,}  expected {expected:>12,}  [{match}]\")\n",
    "\n",
    "print(\"\\nProgress log summary:\")\n",
    "for e in PROGRESS_LOG:\n",
    "    if e.get(\"status\") == \"complete\":\n",
    "        print(f\"  {e['table']}: {e['total_rows']:,} rows, {e['total_chunks']} chunk(s) -- COMPLETE\")\n",
    "\n",
    "print()\n",
    "if all_match:\n",
    "    print(\"All row counts match. Ingest successful.\")\n",
    "    print(f\"Namespace : {NAMESPACE}\")\n",
    "    print(f\"Bronze    : s3a://{BUCKET}/{BRONZE_PREFIX}/\")\n",
    "    print(f\"Silver    : {SILVER_BASE}\")\n",
    "else:\n",
    "    print(\"Row count mismatch detected.\")\n",
    "    print(\"Check the progress log for the last completed line range per chunk:\")\n",
    "    print(f\"  s3a://{BUCKET}/{PROGRESS_KEY}\")\n",
    "    print(\"Or check the quarantine path for rejected rows:\")\n",
    "    print(f\"  {SILVER_BASE}/quarantine/\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "b0000024-0000-0000-0000-000000000024",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "spark.stop()"
   ]
  }
 ]
}