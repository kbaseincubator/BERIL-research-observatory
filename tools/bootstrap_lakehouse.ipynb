{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Bootstrap Lakehouse: microbialdiscoveryforge_observatory\n\nOne-time setup notebook. Creates the `microbialdiscoveryforge_observatory` database and\nregistry tables in the BERDL lakehouse under the `microbialdiscoveryforge` tenant.\n\n**Run this on BERDL JupyterHub before using the upload notebook.**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_spark_session import get_spark_session\n",
    "spark = get_spark_session()\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "DATABASE = \"microbialdiscoveryforge_observatory\"\n\nspark.sql(f\"CREATE DATABASE IF NOT EXISTS {DATABASE}\")\nprint(f\"Database '{DATABASE}' created.\")\n\n# Verify it appears in the database list\ndbs = [row.namespace for row in spark.sql(\"SHOW DATABASES\").collect()]\nassert DATABASE in dbs, f\"{DATABASE} not found in database list!\"\nprint(f\"Verified: {DATABASE} exists in SHOW DATABASES.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Registry Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project registry: one row per uploaded project\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {DATABASE}.project_registry (\n",
    "        project_id STRING,\n",
    "        title STRING,\n",
    "        status STRING,\n",
    "        authors STRING,\n",
    "        git_repo STRING,\n",
    "        git_branch STRING,\n",
    "        git_commit STRING,\n",
    "        upload_date TIMESTAMP,\n",
    "        file_manifest STRING\n",
    "    )\n",
    "    USING DELTA\n",
    "\"\"\")\n",
    "print(f\"Table '{DATABASE}.project_registry' created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project files: non-tabular files (notebooks, figures, markdown, etc.)\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {DATABASE}.project_files (\n",
    "        project_id STRING,\n",
    "        file_path STRING,\n",
    "        file_type STRING,\n",
    "        content STRING,\n",
    "        size_bytes LONG,\n",
    "        modified_date TIMESTAMP,\n",
    "        upload_date TIMESTAMP\n",
    "    )\n",
    "    USING DELTA\n",
    "\"\"\")\n",
    "print(f\"Table '{DATABASE}.project_files' created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Validate Write Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from datetime import datetime\n",
    "\n",
    "# Test write to project_registry\n",
    "test_row = Row(\n",
    "    project_id=\"_bootstrap_test\",\n",
    "    title=\"Bootstrap Test\",\n",
    "    status=\"test\",\n",
    "    authors=\"bootstrap\",\n",
    "    git_repo=\"test\",\n",
    "    git_branch=\"test\",\n",
    "    git_commit=\"test\",\n",
    "    upload_date=datetime.now(),\n",
    "    file_manifest=\"[]\",\n",
    ")\n",
    "df = spark.createDataFrame([test_row])\n",
    "df.write.format(\"delta\").mode(\"append\").insertInto(f\"{DATABASE}.project_registry\")\n",
    "print(\"Write to project_registry: OK\")\n",
    "\n",
    "# Verify we can read it back\n",
    "result = spark.sql(f\"\"\"\n",
    "    SELECT * FROM {DATABASE}.project_registry\n",
    "    WHERE project_id = '_bootstrap_test'\n",
    "\"\"\").collect()\n",
    "assert len(result) == 1, \"Failed to read back test row!\"\n",
    "print(\"Read from project_registry: OK\")\n",
    "\n",
    "# Clean up test row\n",
    "spark.sql(f\"\"\"\n",
    "    DELETE FROM {DATABASE}.project_registry\n",
    "    WHERE project_id = '_bootstrap_test'\n",
    "\"\"\")\n",
    "print(\"Delete from project_registry: OK\")\n",
    "print()\n",
    "print(\"All write access tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = spark.sql(f\"SHOW TABLES IN {DATABASE}\").collect()\n",
    "print(f\"Database: {DATABASE}\")\n",
    "print(f\"Tables: {len(tables)}\")\n",
    "for t in tables:\n",
    "    print(f\"  - {t['tableName']}\")\n",
    "print()\n",
    "print(\"Bootstrap complete. You can now run upload_to_lakehouse.ipynb.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}