{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Upload Projects to Lakehouse\n\nUpload BERIL Observatory project data and files to the `microbialdiscoveryforge_observatory`\nlakehouse collection.\n\n**Prerequisites**: Run `bootstrap_lakehouse.ipynb` first to create the database.\n\n## Parameters\n\nSet `PROJECT_ID` below:\n- A specific project name (e.g., `\"metal_fitness_atlas\"`) to upload one project\n- `\"all\"` to upload all projects (bulk backfill)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PARAMETERS ---\n",
    "PROJECT_ID = \"metal_fitness_atlas\"  # Change this, or set to \"all\" for bulk upload\n",
    "OVERWRITE = True                     # Overwrite existing tables if they exist\n",
    "BASE_PATH = \".\"                      # Path to BERIL-research-observatory root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add tools/ to path so we can import lakehouse_upload\n",
    "base = Path(BASE_PATH).resolve()\n",
    "tools_dir = base / \"tools\"\n",
    "if str(tools_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(tools_dir))\n",
    "\n",
    "import lakehouse_upload\n",
    "print(f\"Base path: {base}\")\n",
    "print(f\"Database: {lakehouse_upload.DATABASE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_spark_session import get_spark_session\n",
    "spark = get_spark_session()\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview: Files to Upload\n",
    "\n",
    "Scan the project directory and show what will be uploaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "projects_dir = base / \"projects\"\n",
    "\n",
    "if PROJECT_ID == \"all\":\n",
    "    project_ids = sorted([\n",
    "        d.name for d in projects_dir.iterdir()\n",
    "        if d.is_dir() and not d.name.startswith(\".\")\n",
    "    ])\n",
    "else:\n",
    "    project_ids = [PROJECT_ID]\n",
    "\n",
    "print(f\"Projects to upload: {len(project_ids)}\\n\")\n",
    "\n",
    "for pid in project_ids:\n",
    "    project_path = projects_dir / pid\n",
    "    if not project_path.exists():\n",
    "        print(f\"  WARNING: {pid} not found\")\n",
    "        continue\n",
    "    manifest = lakehouse_upload.get_upload_manifest(project_path)\n",
    "    tabular = sum(1 for f in manifest if f['classification'] == 'tabular' and f['in_data_dir'])\n",
    "    other = len(manifest) - tabular\n",
    "    total_mb = sum(f['size_bytes'] for f in manifest) / 1024 / 1024\n",
    "    print(f\"  {pid}: {tabular} data tables, {other} files, {total_mb:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"all\":\n",
    "    results = lakehouse_upload.upload_all_projects(spark, str(base), overwrite=OVERWRITE)\n",
    "else:\n",
    "    result = lakehouse_upload.upload_project(spark, PROJECT_ID, str(base), overwrite=OVERWRITE)\n",
    "    results = [result] if result else []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the project registry\n",
    "print(\"=== Project Registry ===\")\n",
    "lakehouse_upload.list_uploaded_projects(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show tables for each uploaded project\n",
    "for r in results:\n",
    "    if r:\n",
    "        print()\n",
    "        lakehouse_upload.list_project_tables(spark, r['project_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sanity check: query the first table\n",
    "if results and results[0] and results[0]['tables']:\n",
    "    first_table = results[0]['tables'][0]['table']\n",
    "    print(f\"Sample from {first_table}:\")\n",
    "    spark.sql(f\"SELECT * FROM {first_table} LIMIT 5\").show(truncate=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show non-tabular files uploaded\n",
    "if results:\n",
    "    pid = results[0]['project_id']\n",
    "    print(f\"Files uploaded for {pid}:\")\n",
    "    spark.sql(f\"\"\"\n",
    "        SELECT file_path, file_type, size_bytes\n",
    "        FROM {lakehouse_upload.DATABASE}.project_files\n",
    "        WHERE project_id = '{pid}'\n",
    "        ORDER BY file_path\n",
    "    \"\"\").show(truncate=60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}