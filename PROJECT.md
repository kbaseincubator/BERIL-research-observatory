# Microbial Discovery Forge

## Purpose

Use the **BERDL Data Lakehouse** and the AI co-scientist to pursue scientific questions across microbial genomics, ecology, metabolic modeling, and multi-omics analysis, while building shared documentation and reusable skills that accelerate future work.

BERDL hosts **35 databases across 9 tenants** including pangenome data for 293K microbial genomes, mutant fitness data for 48 organisms, ModelSEED biochemistry, multi-omics from NMDC, and more. See [docs/collections.md](docs/collections.md) for the full inventory.

## Dual Goals

1. **Science**: Answer research questions in `projects/` subdirectories
2. **Knowledge**: Capture learnings in `docs/` to reduce re-discovery overhead

## Documentation Workflow

When working on any science project, update `docs/` when you discover:

| Discovery Type | Add To |
|----------------|--------|
| Query pitfall or gotcha | `docs/pitfalls.md` |
| Performance issue or strategy | `docs/performance.md` |
| Data limitation or coverage gap | `docs/pitfalls.md` |
| Useful insight about data structure | `docs/schemas/{collection}.md` |
| Any other learning worth sharing | `docs/discoveries.md` |
| Research idea or future direction | `docs/research_ideas.md` |

**Tag each addition** with the project that uncovered it:
```markdown
### [ecotype_analysis] AlphaEarth coverage is only 28%
Discovered that only 83K/293K genomes have embeddings...
```

## Documentation Files

| File | Purpose |
|------|---------|
| `docs/collections.md` | Overview of all BERDL databases and tenants |
| `docs/schemas/` | Per-collection schema documentation |
| `docs/overview.md` | Project goals, data workflow, scientific context |
| `docs/pitfalls.md` | SQL gotchas, data sparsity, common errors |
| `docs/performance.md` | Query strategies for large tables |
| `docs/discoveries.md` | Running log of insights (low-friction capture) |
| `docs/research_ideas.md` | Future research directions, project ideas |

## Project Structure

Each science project in `projects/` should have:
- `README.md`: Project overview, reproduction instructions, authors
- `RESEARCH_PLAN.md`: Hypothesis, approach, query strategy, revision history
- `REPORT.md`: Key findings, interpretation, supporting evidence (created by `/synthesize`)
- `REVIEW.md`: Automated review (generated by `/submit`)
- `notebooks/`: Analysis notebooks **with saved outputs** (see Reproducibility below)
- `data/`: Agent-derived data from queries and analysis (gitignore large files)
- `user_data/`: User-provided input data — gene lists, phenotype tables, external databases (gitignore large files)
- `figures/`: Key visualizations saved as PNG files
- `requirements.txt`: Python dependencies
- `src/`: Reusable scripts (if applicable)

## Reproducibility Standards

Projects must be **followable by both humans and agents** without re-running anything. A reader cloning the repo should be able to understand the full analysis — data, methods, results, and conclusions — from the committed files alone.

### Notebook Outputs

**Notebooks must be committed with saved outputs.** This is a hard requirement, not a nice-to-have. Notebooks serve three critical roles in the observatory:

1. **Provenance record**: The saved outputs prove what actually happened — what data was returned, what the statistics were, what the figures looked like. Without outputs, a notebook is just a script with extra formatting.
2. **Human review**: In the UI, notebooks are rendered as a methods+results section. Reviewers (human and automated) read the outputs to verify the analysis. Empty output cells make review impossible.
3. **Machine readability**: The `/synthesize` and `/submit` skills read notebook output cells to extract results. No outputs = no synthesis.

**A notebook with only source code and no outputs should never be committed.** If analysis was prototyped as a script, the notebook must still be executed to capture outputs before committing.

```bash
# Execute notebook and save outputs in place
jupyter nbconvert --to notebook --execute --inplace notebooks/my_analysis.ipynb
```

This embeds text output, tables, and inline figures directly in the `.ipynb` file.

**Spark-dependent notebooks** (NB01-02 pattern) that require JupyterHub cannot be re-executed locally. For these:
- Add a note in the header: "Requires BERDL JupyterHub"
- Document what outputs they produce
- Ensure downstream notebooks can run locally from cached data

### Figures

**Every notebook that produces visual results should save figures to `figures/`.** Inline notebook figures are good for the notebook reader, but standalone PNGs are needed for:
- README documentation
- Review assessment
- Quick project browsing without opening notebooks

Recommended figures per analysis stage:
- **Data exploration**: distributions, coverage summaries
- **Method output**: result size distributions, parameter selection plots
- **Validation**: comparison charts, enrichment plots
- **Summary**: key findings visualization

### Dependencies

Include a `requirements.txt` listing Python packages needed to run the notebooks locally. This allows reproduction without guessing dependencies.

### README Reproduction Section

Include a `## Reproduction` section in README.md that explains:
- Prerequisites (Python version, packages, BERDL access if needed)
- Step-by-step instructions to run the pipeline
- Which notebooks need Spark vs run locally
- Expected runtime for compute-intensive steps

Current projects:
- `projects/ecotype_analysis/` - Environment vs phylogeny effects on gene content
- `projects/pangenome_openness/` - Open vs closed pangenome patterns
- `projects/cog_analysis/` - COG functional category distributions across core/aux/novel genes
- `projects/pangenome_pathway_geography/` - Pangenome openness, metabolic pathways, and biogeography
- `projects/resistance_hotspots/` - Antibiotic resistance hotspot analysis
- `projects/conservation_vs_fitness/` - Gene conservation vs fitness browser data
- `projects/fitness_modules/` - Pan-bacterial fitness modules via ICA decomposition

## Data Organization

| Location | What Goes There | Examples |
|----------|-----------------|----------|
| `data/` | Shared extracts reusable across projects | Pangenome stats for all species, genome metadata, species lists |
| `projects/*/data/` | Agent-derived data from queries and analysis | Distance matrices, fitness scores, DIAMOND hits |
| `projects/*/user_data/` | User-provided input data | RNASeq gene lists, phenotype CSVs, SQLite databases |
| `microbialdiscoveryforge` (lakehouse) | Archived project data and files | All project outputs uploaded via `/submit` |

**Rule of thumb**: If another project might need it, put it in top-level `data/`. If it's clearly for one question, keep it in the project. User-provided data always goes in `user_data/` — never mix it with agent-derived outputs in `data/`.

### Cross-Project Data Reuse

When building on data from another project:

**DO NOT copy files from other projects into your project directory.** This creates duplication and obscures attribution.

**DO reference data directly from the lakehouse:**
```python
# CORRECT: Reference another project's data from MinIO
# In your notebook or script
lakehouse_path = "s3a://cdm-lake/tenant-general-warehouse/microbialdiscoveryforge/projects/essential_genome/data/essential_families.tsv"
df = spark.read.csv(lakehouse_path, header=True, sep="\t")

# Or download temporarily (not committed to git)
# mc cp berdl-minio/.../essential_genome/data/essential_families.tsv /tmp/
```

**DO save modified versions in your project:**
```python
# If you transform or filter the data, save YOUR version
filtered_df = df[df['essentiality_class'] == 'universally_essential']
filtered_df.to_csv('projects/your_project/data/filtered_essential_families.tsv')
```

**Attribution:**
- In your `README.md` Data Sources section, cite the original project
- In notebooks, add a comment when loading external project data
- Example: `# Essential gene families from projects/essential_genome (Dehal, 2026)`

**Benefits:**
- Avoids data duplication (original project already archived to lakehouse)
- Clear provenance (data stays with the original creator)
- Attribution (original author gets credit)
- Single source of truth (updates to original data propagate)

### Lakehouse Archival

Project data files are gitignored (too large for git) but are archived to the `microbialdiscoveryforge` tenant on BERDL MinIO object storage. This ensures data products are:
- **Preserved** — not lost if a local machine fails
- **Shared** — accessible to all BERDL users via `mc cp` or Spark
- **Downloadable** — any user can pull project data with `mc cp --recursive berdl-minio/cdm-lake/tenant-general-warehouse/microbialdiscoveryforge/projects/<project>/data/ ./`

**Upload workflow**: When a project passes `/submit` review, run `python tools/lakehouse_upload.py <project_id>`. See [docs/collections.md](docs/collections.md) for the full collection details.

## Database Access

- **Databases**: 35 databases across BERDL (see [docs/collections.md](docs/collections.md))
- **Auth**: Token in `.env` file (KBASE_AUTH_TOKEN)
- **API**: `https://hub.berdl.kbase.us/apis/mcp/`
- **Direct Spark**: Use JupyterHub for complex queries

Use `/berdl` skill for BERDL queries. Read `docs/pitfalls.md` before your first query.

### Local Spark Connect

Run queries locally while computation happens on the remote BERDL cluster. This avoids the JupyterHub web UI for interactive work.

**One-time setup:**
```bash
bash scripts/bootstrap_client.sh   # creates .venv-berdl with required packages
```

**Running queries:**
```bash
source .venv-berdl/bin/activate
python scripts/run_sql.py --berdl-proxy --query "SHOW DATABASES"
```

**Prerequisites:**
- `KBASE_AUTH_TOKEN` set in `.env`
- A JupyterHub session active (log in and open a notebook so your Spark Connect service is running)
- BERDL proxy chain running (SSH SOCKS tunnels on ports 1337/1338 + pproxy on port 8123). BERDL services are not directly reachable from external networks. See `.claude/skills/berdl-query/references/proxy-setup.md` for setup instructions.

See the `/berdl-query` and `/berdl-minio` skills for full workflow details.

### Spark Notebooks

**All analysis notebooks should use direct Spark access** on the BERDL JupyterHub for best performance.

**Required initialization** at the top of every Spark notebook:
```python
# On BERDL JupyterHub — no import needed (injected into kernel)
spark = get_spark_session()

# On local machine — requires .venv-berdl + proxy chain
from get_spark_session import get_spark_session
spark = get_spark_session()
```

See `docs/pitfalls.md` ("Use the Right Import for Your Environment") for the full breakdown including CLI scripts on JupyterHub.

Then query any database — **keep data as Spark DataFrames** and use PySpark operations:
```python
# Preferred: work with Spark DataFrames
df = spark.sql("SELECT ... FROM database_name.table")
df_filtered = df.filter(df.no_genomes >= 50).groupBy("phylum").count()

# Only convert to pandas for final small results (plotting, export)
plot_df = df_filtered.toPandas()
```

**Benefits vs REST API**:
- No timeouts on complex queries
- Better performance on large joins
- Full Spark SQL functionality
- Can handle species with >500 genomes

**Important**: Avoid calling `.toPandas()` on large intermediate results. It pulls all data to the driver node and can be very slow or cause out-of-memory errors. Do filtering, joins, and aggregations in Spark first.

### JupyterHub Workflow

**BERDL runs on Kubernetes/Rancher** - compute nodes are ephemeral pods, not persistent VMs. This means:
- No direct SSH to compute nodes
- No remote script execution
- JupyterHub web UI is the designed interface

**Typical workflow:**

1. **Develop locally**
   - Write/edit notebooks on your local machine
   - Test logic with small datasets if possible
   - Commit to git when ready

2. **Upload to JupyterHub**
   - Navigate to: `https://hub.berdl.kbase.us`
   - Authenticate with MFA
   - Upload notebook via Upload button (drag & drop)
   - Place in appropriate directory

3. **Run analysis**
   - Open notebook in JupyterHub
   - Verify Spark session initializes: `spark = get_spark_session()`
   - Kernel -> Restart & Run All (or run cells interactively)
   - Monitor progress (typical runtime: 5-30 minutes for multi-species analyses)

4. **Download results**
   - Select output files in JupyterHub file browser (notebooks, CSVs, PNGs)
   - Right-click -> Download (or use Download button)
   - Place in local `projects/*/data/` directory
   - Commit visualizations and small data files to git

**Current limitations:**
- No programmatic notebook execution (must use web UI)
- No completion notifications (must monitor manually)
- File transfer is manual (acceptable for files <1GB)

## Key Reminders

1. Use exact equality for species IDs (e.g., `WHERE id = 's__Species--RS_GCF_123'`). The `--` inside quotes is fine.
2. Large tables (gene, genome_ani) need filters. Never full-scan.
3. AlphaEarth embeddings only cover 28% of genomes.
4. Gene clusters are species-specific. Can't compare across species.
5. Update docs when you learn something worth sharing!
6. Check [docs/collections.md](docs/collections.md) for the full database inventory.
